---
title: "LECTURE 2: introduction to linear models"
subtitle: "FANR 6750 (Experimental design)"
author: "<br/><br/><br/>Fall 2025"
output:
  xaringan::moon_reader:
    css: ["default", "FANR6750.css", "FANR6750-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', warning=FALSE, message=FALSE, fig.retina = 2)
source(here::here("R/zzz.R"))
# library(emo)
# library(gganimate)
```

class: inverse

# outline

<br/>
#### 1) Basic structure of a linear model

<br/>  
--

#### 2) Parameter interpretation

<br/> 
--

#### 3) Assumptions

<br/> 

---
class: inverse, middle

### Statistics = Information + Uncertainty

#### In the last lecture, we learned that statistics is what allows us to make inferences about a population in the face of uncertainty

--

#### Statistical inference requires **models**

---
# what is a model?

![](https://media.giphy.com/media/12npFVlmZoXN4Y/giphy.gif)

---
# what is a model?

> an abstraction of reality used to describe the relationship between two or more variables

--

#### Many types (conceptual, graphical, mathematical)

--

#### In this class, we will deal with *statistical* models

--

- Mathematical representation of our hypothesis

--

- By necessity, models will be simplifications of reality ("all models are wrong...") 

--

- Do not have to be complex

---
# but i don't want to be a modeler!

```{r echo = FALSE, out.height="65%", out.width="46%"}
knitr::include_graphics("puffy_shirt.png")
```

--
- Inference **requires** models  

--
- Models link **observations** to **processes**  

--
- Models are tools that allow us understand processes that we **cannot directly observe** based on quantities that we **can** observe  

---
# a simple example

#### Suppose we are interested in the hypothesis that water availability limits acorn production by oak trees

- **Prediction**: Acorn production will increase following years with higher rainfall

--

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=6}
x <- seq(from = 0, to = 200, by = 0.1)
x2 <- runif(n = 50, 0, 200)
line_df <- data.frame(x = x,
                      y = 50 + 2 * x)
samp_df <- data.frame(x = x2, y = rnorm(n = 25, 50 + 2 * x2, 50))
ggplot(line_df, aes(x = x, y = y)) + 
  geom_point(data = samp_df, aes(x = x, y = y), color = "#D47500", alpha = 0.75) +
  scale_x_continuous("Rainfall (mm)") +
  scale_y_continuous("Number of acorns")
```

---
# a simple model

--

</br>
</br>

$$\Huge y = a + bx$$

--

```{r, echo=FALSE, fig.align='center', fig.height=2.5, fig.width=5}
ggplot(line_df, aes(x = x, y = y)) + 
  geom_path(color = "#446E9B")
```

--
It may not be obvious, but this is essentially the only model we will use this semester<sup>1</sup>

.footnote[[1] With some minor variations, mainly in $x$]

---
# a simple model

</br>
</br>

$$\Huge y = a + bx$$


```{r, echo=FALSE, fig.align='center', fig.height=2.5, fig.width=5}
ggplot(line_df, aes(x = x, y = y)) + 
  geom_path(color = "#446E9B")
```

If we want to use this as a *statistical* model, what's missing?

---
# a simple model

</br>
</br>

$$\Huge y = a + bx$$


```{r, echo=FALSE, fig.align='center', fig.height=2.5, fig.width=5}

ggplot(line_df, aes(x = x, y = y)) + 
  geom_path(color = "#446E9B")
```

If we want to use this as a statistical model, what's missing?

#### **Stochasticity!**

---
# a simple model

</br>
</br>

$$\Huge y = a + bx$$


```{r, echo=FALSE, fig.align='center', fig.height=2.5, fig.width=5}
ggplot(line_df, aes(x = x, y = y)) + 
  geom_path(color = "#446E9B") +
  geom_point(data = samp_df, aes(x = x, y = y), color = "#D47500", alpha = 0.75)+
  scale_x_continuous("Rainfall (mm)") +
  scale_y_continuous("Acorns")
```

If we want to use this as a statistical model, what's missing?

#### **Stochasticity!**

---
class:inverse, middle, center

# the linear model

---
# statistics cookbook

```{r out.width="50%"}
knitr::include_graphics("stats_flow_chart.png")
```

---
# the linear model

<br/>
<br/>
$$\Large response = deterministic\; part+stochastic\; part$$ 
<br/>
<br/>

--
$$\underbrace{\LARGE E[y_i] = \beta_0 + \beta_1 \times x_i}_{Deterministic}$$

<br/>
<br/>

--
$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma)}_{Stochastic}$$  

???

Note that the deterministic portion of the model has the same form as the equation for a line: $y = a + b \times x$, which is why we call these linear models

---
# the linear model

#### Sometimes you will see linear models written as:

$$\LARGE y_i = \beta_0 + \beta_1  x_i + \epsilon_i$$

$$\LARGE \epsilon_i \sim normal(0, \sigma)$$  

#### with the $\large \epsilon_i$ terms referred to as *residuals* or *residual error*

- Mathematically, the two formulations are identical 

- We will use both formulations 

---
class:inverse

# a note on notation

#### Throughout the semester, we will try to be consistent with mathematical notation, e.g.

- $y$ = response/dependent variable

- $x$ = predictor/independent variable

- $\mu$ = population mean

- $\sigma$ = population standard deviation

- $\beta$ = model parameter (i.e., intercept/slope)

--

#### To the extent possible, these follow conventions used in many textbooks/papers (but there is a lot of variation between authors)

--

#### Remember that these are just *symbols* - you could replace them with other symbols (e.g., emoji) and it would not change their interpretation!

---
# parameter interpretation

$\large \beta_0$ = intercept

- The *expected* value of the response variable, $\large E[y_i]$, when all predictors $=0$
    
$\large \beta_n$ = slope

- The expected change in the response variable for a one unit change in the associated predictor variable $\large x_n$
    
--

Mathematically, the interpretation of the intercept and slope(s) is always the same

--

Ecologically, the interpretation will depend on:

- the structure of the predictor variables (continuous vs. categorical)

- the units of the predictor variables (e.g., scaled vs unscaled)

- the inclusion of interaction terms

We will discuss each of these scenarios in detail as the semester progresses



---
# the linear model

#### A "simple" example

$$\underbrace{\LARGE E[y_i] = -2 + 0.5 \times x_i}_{Deterministic}$$

--

$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma=0.25)}_{Stochastic}$$

--
```{r fig.height=4, fig.width=6}
x <- rnorm(100)
y <- -2 + 0.5 * x + rnorm(100, 0, 0.25)
mu <- -2 + 0.5 * x
resid <- y - mu
df <- data.frame(x = x, y = y, mu = mu, Residuals = resid)

ggplot(df, aes(x, y)) + 
  geom_abline(slope = 0.5, intercept = -2, color = "#446E9B", size = 1.5) +
  scale_y_continuous("y") +
  geom_point(color = "#D47500", alpha = 0.75)
```

---
# the linear model

#### A more complex model

$$\large y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ... + \beta_px_{ip} + \epsilon_i$$

--

- Each $\beta$ coefficient is the effect of a specific predictor variables $x$

- Predictor variables may be continuous, binary, factors, or a combination

- We will cover more complex models (and interpretation) later

---
# is this a linear model?

$$\Large y = 20 + 0.5x - 0.3x^2$$

```{r, fig.height=3.5, fig.width=5.5}
x <- seq(from = 0, to = 10, 0.1)
lm2 <- data.frame(x = x,
                 y = 20 + 0.5 * x - 0.3 * x^2)

ggplot(lm2, aes(x = x, y = y)) +
  geom_line() 
```

---
# residuals

#### One concept we will talk about a lot is *residuals*, i.e., $\Large \epsilon_i$

--

- Residuals are the difference between the observed values $\large y_i$ and the predicted values $\large E[y_i]$ - how much variation in $\large y$ is explained by $\large x$?

```{r fig.height=5, fig.width=7}
ggplot(df, aes(x, y)) + 
  geom_segment(aes(x = x, xend = x, y = mu, yend = y), color = "#3CB521", size = 1) +
  geom_abline(slope = 0.5, intercept = -2, color = "#446E9B", size = 1.5) +
  scale_y_continuous("y") +
  geom_point(color = "#D47500", alpha = 0.9)
```

---
# residuals

#### One concept we will talk about a lot is *residuals*, i.e., $\Large \epsilon_i$

- Residuals are the difference between the observed values $\large y_i$ and the predicted values $\large  E[y_i]$ - how much variation in $\large y$ is explained by $\large x$?

```{r fig.height=3.5, fig.width=5.5}
ggplot(df, aes(x, Residuals)) + 
  geom_abline(slope = 0, intercept = 0, color = "grey40", linetype = "dashed", size = 1.5) +
  geom_point(color = "#D47500", alpha = 0.75)
```

--

- Parameters are generally estimated by finding values that minimize residual error (e.g., ordinary least squares)

--

- Useful for assessing whether data violate model assumptions



---
class:inverse, center, middle

# assumptions

---
# assumptions

#### **EVERY** model has assumptions

--

- Assumptions are necessary to simplify real world to workable model

--

- If your data violate the assumptions of your model, inferences *may* be invalid

--

- **Always** know (and test) the assumptions of your model<sup>1</sup> 

.footnote[[1] You know what happens when you assume...]

---
# linear model assumptions

</br>

$$\Large y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

$$\Large \epsilon_i \sim normal(0, \sigma)$$


</br>

--

1) **Linearity**: The relationship between $x$ and $y$ is linear

--

2) **Normality**: The residuals are normally distributed<sup>2</sup>

.footnote[[2] Note that these assumptions apply to the residuals, not the data!]

--

3) **Homogeneity**: The residuals have a constant variance at every level of $x$

--

4) **Independence**: The residuals are independent (i.e., uncorrelated with each other)

???

Because virtually every model we will use this semester is a linear model, these assumptions apply to everything we will discuss from here out

---
# assumptions

$$\large y_i = \beta_0 + \beta_1x_i + \epsilon_i$$
$$\large \epsilon_i \sim normal(0, \sigma)$$

```{r fig.height=6, fig.width=8}
  x <- runif(100, 0, 60)
  ey <- 10 + 1*x 
  eps <- rnorm(100, 0, 3)
  y <- ey + eps
  df <- data.frame(x, y)
  lm_fit <- lm(y ~ x, data = df)
  
  k <- 2.5
  sigma <- sigma(lm_fit)
  ab <- coef(lm_fit); a <- ab[1]; b <- ab[2]
  
  x <- seq(-k*sigma, k*sigma, length.out = 50)
  y <- dnorm(x, 0, sigma)/dnorm(0, 0, sigma) * 3
  
  x0 <- 5
  y0 <- a+b*x0
  path1 <- data.frame(x = y + x0, y = x + y0)
  segment1 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)
  x0 <- 20
  y0 <- a+b*x0 
  path2 <- data.frame(x = y + x0, y = x + y0)
  segment2 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)
  x0 <- 35
  y0 <- a+b*x0 
  path3 <- data.frame(x = y + x0, y = x + y0)
  segment3 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)
  x0 <- 50
  y0 <- a+b*x0 
  path4 <- data.frame(x = y + x0, y = x + y0)
  segment4 <- data.frame(x = x0, y = y0 - k*sigma, xend = x0, yend = y0 + k*sigma)
  
  ggplot(df, mapping = aes(x=x, y=y)) + 
    geom_point(color="#446E9B", alpha = 0.5) 
```

---
# assumptions - linearity

$$\large y_i = \color{#D47500}{\beta_0 + \beta_1x_i} + \epsilon_i$$
$$\large \epsilon_i \sim normal(0, \sigma)$$

```{r fig.height=6, fig.width=8}
  ggplot(df, mapping = aes(x=x, y=y)) + 
    geom_point(color="#446E9B", alpha = 0.5) + 
    geom_smooth(method='lm', se=FALSE, color="#D47500", alpha = 0.5) 
```

---
# assumptions - normality

$$\large y_i = \color{#D47500}{\beta_0 + \beta_1x_i} + \epsilon_i$$
$$\large \epsilon_i \sim \color{#3CB521}{normal}(0, \sigma)$$

```{r fig.height=6, fig.width=8}
  ggplot(df, mapping = aes(x=x, y=y)) + 
    geom_point(color="#446E9B", alpha = 0.5) + 
    geom_smooth(method='lm', se=FALSE, color="#D47500", alpha = 0.5) + 
    geom_path(aes(x,y), data = path1, linewidth = 1, color = "#3CB521") + 
    geom_path(aes(x,y), data = path2, linewidth = 1, color = "#3CB521") + 
    geom_path(aes(x,y), data = path3, linewidth = 1, color = "#3CB521") +
    geom_path(aes(x,y), data = path4, linewidth = 1, color = "#3CB521")
```

---
# assumptions - homogeniety

$$\large y_i = \color{#D47500}{\beta_0 + \beta_1x_i} + \epsilon_i$$
$$\large \epsilon_i \sim \color{#3CB521}{normal}(0, \color{#CD0200}\sigma)$$

```{r fig.height=6, fig.width=8}
  ggplot(df, mapping = aes(x=x, y=y)) + 
    geom_point(color="#446E9B", alpha = 0.5) + 
    geom_smooth(method='lm', se=FALSE, color="#D47500", alpha = 0.5) + 
    geom_path(aes(x,y), data = path1, linewidth = 1, color = "#3CB521") + 
    geom_path(aes(x,y), data = path2, linewidth = 1, color = "#3CB521") + 
    geom_path(aes(x,y), data = path3, linewidth = 1, color = "#3CB521") +
    geom_path(aes(x,y), data = path4, linewidth = 1, color = "#3CB521") +
    geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment1, linewidth = 0.6, 
                 color = "#CD0200") +
    geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment2, linewidth = 0.6,
                 color = "#CD0200") +
    geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment3, linewidth = 0.6,
                 color = "#CD0200") +
    geom_segment(aes(x=x,y=y,xend=xend,yend=yend), data = segment4, linewidth = 0.6,
                 color = "#CD0200") 
```


---
# assumptions - normality

Remember that the normality assumption applies to the *residuals* not the data. 

--

In the data below, the histogram of the response variable *y* shows that the data is clearly not normally distributed

```{r fig.width=9, fig.height = 4 }
set.seed(97342)
x <- c(rnorm(60, 7, 3), runif(40, 15, 60))
ey <- -3 + 1.2 * x

eps <- rnorm(length(x), 0, 4)
y <- ey + eps
fm1 <- lm(y ~ x)

df <- data.frame(y = y,
                 ey = ey,
                 x = x, 
                 r = resid(fm1))

y <- ggplot(df, aes(x = y)) +
  geom_histogram() +
  scale_y_continuous("") +
  scale_x_continuous("y")
xy1 <- ggplot(df, aes(x = x, y = y)) +
  stat_smooth(method = "lm", color = "red", se = FALSE) +
  geom_point(alpha = 0.5, size = 1)
xy2 <- ggplot(df, aes(x = x, y = y)) +
  stat_smooth(method = "lm", color = "red", se = FALSE) +
  geom_segment(aes(x = x, xend = x, y = ey, yend = y), color = "#3CB521", size = 1) +
  geom_point(alpha = 0.5, size = 1)
blank <- ggplot() + geom_blank()
r <- ggplot(df, aes(x = r)) +
  geom_histogram() +
  scale_y_continuous("") +
  scale_x_continuous("residuals")
cowplot::plot_grid(xy1, y, nrow = 1, ncol = 3)

```

---
# assumptions - normality

Remember that the normality assumption applies to the *residuals* not the data. 

In the data below, the histogram of the response variable *y* shows that the data is clearly not normally distributed

But a histogram of the residuals (green lines) shows that they are normal (or at least close)

```{r fig.width=9, fig.height = 4 }

cowplot::plot_grid(xy2, y, r, nrow = 1, ncol = 3)

```

---
# linear models

#### Very flexible

--

- Predictor(s) can take different forms (binary, continuous, factor)

--

- Can contain many predictors

--

- Can model non-linear relationships

--

#### Link different "tests" (e.g., t-tests, ANOVA, ANCOVA, linear regression)

--

#### Can be used for different statistical goals

- Estimating unknown parameters  

- Testing hypotheses  

- Describing stochastic systems  

- Making predictions that account for uncertainty  

---
# statistical inference

#### Linear models allow us to quantify relationships between variables

--

#### Generally, models are fit to **samples**

- The intercept and slope values correspond to the *observed* response and predictor values

- If we repeated our study, the sampled values would change and so would the intercept/slope values

--

#### We're generally interested in the **population**, not the sample

- The intercept and slope values fitted to a particular sample are considered *estimates* of the true population values

--

#### Inference about populations requires quantifying how well our estimated values represent the true population values (which we can never know)

- This will be the topic of the next lecture (and a theme we will revisit throughout the semester)

---
# looking ahead

<br/>

### **Next time**: Princples of statistical inference

<br/>

### **Reading**: [Feiberg chp. 1.6-1.8](https://statistics4ecologists-v3.netlify.app/01-linearregression#sec-sampdist)

