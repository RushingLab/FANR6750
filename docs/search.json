[{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"what-is-markdown","dir":"Articles","previous_headings":"","what":"What is Markdown?","title":"Introduction to R Markdown","text":"discuss R Markdown , need discuss Markdown . Markdown? Let’s start ’s . Many probably created report paper using word processor like Microsoft Word Google Docs. Word processors referred “see get” (wysiwyg) text editors. means highlight text click boldface icon Word, text appears bold screen. sorts formatting options, including making headers, inserting figures, adding page numbers, etc., possible clicking buttons. code behind scenes creates changes users don’t see code, formatting output. makes wysiwyg editors relatively easy use beginners. advanced users, can actually problematic. ever Word act ways don’t fully understand? course! . ever tried opening .docx file using older version Word, find doesn’t look way thought ? ever inserted figure jump another page get ‘anchored’ bottom page? just problems occur document bunch hidden formatting code see understand. Markdown different. Markdown files plain text files, meaning can created edited using text editors (like NotePad Windows TextEdit Mac). biggest difference Markdown files Word documents formatting Markdown documents occurs document rather behind scenes. make something boldface tell Markdown putting two **asterisks** either side word phrase. Italics done putting one *asterisk* around text. Hyperlinks written like : [Hyperlinks](https://en.wikipedia.org/wiki/Markdown). just many formatting options can include Markdown document. ’ll learn options like headers, lists, mathematical symbols equations, figures later tutorial throughout semester. ’re writing, text won’t look bold italic whatever (‘see get’, ’s ‘see type’). formatting shows render Markdown file create another type document (pdf, html, even Word). nice thing Markdown uses standard ways express specific formatting options, can convert documents different output formats easily.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"what-is-r-markdown","dir":"Articles","previous_headings":"","what":"What is R Markdown?","title":"Introduction to R Markdown","text":"course, use specific ‘flavor’ Markdown called ‘R Markdown’. R Markdown gives us formatting options available Markdown plus ability embed, display, run R code documents. mixing R code plain text, can create dynamic reports replicate analytical processes, show code underlying processes, create output analysis (figures, summary statistics, etc.), provide necessary text explanations go along code output.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"why-use-r-markdown","dir":"Articles","previous_headings":"","what":"Why use R Markdown","title":"Introduction to R Markdown","text":"R Markdown many advantages compared creating reports Word GoogleDocs. advantages include: Versatility- Want convert Word document pdf? ’s hard. pdf Word? ’s pain. PDF HTML? Maybe know don’t. R Markdown, can change formats single click (single line code). can even convert pretty nice slide shows. Embed code text - running analysis, get results Word? Type hand? Copy--paste? pain error prone. Rerun analysis using new data? Oops, now copy paste new results figures. R Markdown, embed code directly text results figures get added reports automatically. means copying pasting updating reports new results come . Annotate code - Using # great adding small annotations R scripts definitely get habitat . sometimes need add lot details help users (future self) make sense complex code. R Markdown allows create documents much text formatting need, along code. Version control - Tired saving manuscript_v1.doc, manuscript_v2.doc, manuscript_final.doc, manuscript_final_v2.doc? version control . won’t go specifics R Markdown allows seamlessly use version control systems like git Github document changes reports. Edit text files - R Markdown files easily created edited within RStudio don’t way. can opened edited base R even using text editors. means can create edit platform (Windows, Mac, Linux) using free software already installed computer Stability - many us Word crash ’re working paper? save working? Hope . R Markdown files smaller lightweight, tend cause computer crash ’re working . Focus text, formatting - spend lot time tweaking formatting Word document rather writing? R Markdown allows separate writing process formatting process, allows focus former without worrying later (theory least). Plus lots templates can use ensure formatting taken care without anything special!","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"why-not-use-r-markdown","dir":"Articles","previous_headings":"","what":"Why not use R Markdown?","title":"Introduction to R Markdown","text":"disadvantages R Markdown. adviser doesn’t use - Try sending .Rmd file adviser get feedback. ’ll wait… Like , folks still use word processors, adopt R Markdown still create edit Word documents collaborators stuck ways track changes - Even ’re lucky adviser review .Rmd file, won’t get nice track changes like Word. alternative (version control helps) none quite easy track changes. Fewer formatting options - better worse, limited set formatting options R Markdown. can constraining (often ’s actually freeing!) ’s learning curve - already know use Word. R Markdown new. make something bold? insert equations? get figures go end document? first, almost certainly google almost every thing need R Markdown (number 1 problem). pretty simple still means going can slow first.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"creating-a-new-r-markdown-file","dir":"Articles","previous_headings":"","what":"Creating a new R Markdown file","title":"Introduction to R Markdown","text":"Click File -> New File -> R Markdown... Choose title format (HTML, pdf, Word) document Click Ok Save newly created document Pretty easy","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"the-yaml-header","dir":"Articles","previous_headings":"Basic formatting","what":"The YAML header","title":"Introduction to R Markdown","text":"top .Rmd file, see several line three blue dashes: called “YAML header” ’s can control lot major formatting options documents. example, change output pdf, just switch html_document pdf_document (note, may need install Latex distribution knit pdf. get error message step, see suggestions ) click Knit button Pretty cool, right? YAML header allows control many “high level” options document. example, change font size, type following directly output: pdf_document argument: Check see font size changed clicking Knit. Changing font type little trickier. Behind scenes, R Markdown turns document Latex code, converted pdf. don’t need know much Latex (though little knowledge helpful) conversion mean formatting options passed Latex converter specific ways. tell Latex want use Arial font, modify output: argument follows: Make sure include spaces indent pdf_document: latex_engine: xelatex. indent first line paragraph, add following header: many possible options header (see additional examples). ’ll learn options later semester.","code":"--- title: \"Test document\" author: \"Clark Rushing\" output: html_document --- fontsize: 12pt title: \"FANR6750\" subtitle: \"Homework 1\" author: \"YOUR NAME HERE\" date: \"2025-08-13\" output:    pdf_document:     latex_engine: xelatex  mainfont: Arial indent: true"},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"content-headers","dir":"Articles","previous_headings":"Basic formatting","what":"Content headers","title":"Introduction to R Markdown","text":"Using headers natural way break document report smaller sections. can include headers putting one # signs front text. One # main header, ## secondary header, etc.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"paragraph-and-line-breaks","dir":"Articles","previous_headings":"Header 1","what":"Paragraph and line breaks","title":"Introduction to R Markdown","text":"writing chunks text R Markdown (e.g., report manuscript), can create new paragraphs leaving empty line paragraph: want force line break, include two spaces end line want break:","code":"This is one paragraph.  This is the next paragraph This is one line   This is the next line"},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"bold-italics","dir":"Articles","previous_headings":"Header 1","what":"Bold, Italics","title":"Introduction to R Markdown","text":"mentioned earlier, create boldface surrounding text two asterisks (**bold**) use single asterisks italics (*italics*)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"code-type","dir":"Articles","previous_headings":"Header 1","what":"Code type","title":"Introduction to R Markdown","text":"highlight code (note, actually insert functioning code, just formats text show code rather plain text), surround text back ticks: mean() can include multiple lines code including three back ticks line code three back ticks line code:","code":"Multiple lines of code look like  this"},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"bulleted-lists","dir":"Articles","previous_headings":"Header 1","what":"Bulleted lists","title":"Introduction to R Markdown","text":"Bulleted lists can included starting line asterisk can also start lines single dash - sub-sub-bullets, indent twice (press tab two times) start -","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"numbered-lists","dir":"Articles","previous_headings":"Header 1","what":"Numbered lists","title":"Introduction to R Markdown","text":"Numbered lists look like can also include sub-levels number lists can lower case roman numerals lowercase letters B. uppercase letters","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"quotations","dir":"Articles","previous_headings":"Header 1","what":"Quotations","title":"Introduction to R Markdown","text":"highlight quotations starting line >, produces: models wrong useful (George E.P. Box)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"hyperlinks","dir":"Articles","previous_headings":"Header 1","what":"Hyperlinks","title":"Introduction to R Markdown","text":"Insert hyperlinks putting text want displayed square brackets followed link parentheses: [RStudio cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"equations","dir":"Articles","previous_headings":"Header 1","what":"Equations","title":"Introduction to R Markdown","text":"Inserting equations R Markdown knowing Latex really comes handy equations written using Latex code. part, difficult need insert complex equations probably need look code symbols. many good resources including feeling particularly ambitious .","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"inline-vs--block-equations","dir":"Articles","previous_headings":"Header 1 > Equations","what":"Inline vs. block equations","title":"Introduction to R Markdown","text":"can include equations either inline (e=mc2e = mc^2) stand-alone block: e=mc2e=mc^2 Inline equations added putting single dollar sign $ either side equation ($e=mc^2$). Equation blocks create starting ending new line double dollar signs $$e=mc^2$$","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"greek-letters","dir":"Articles","previous_headings":"Header 1","what":"Greek letters","title":"Introduction to R Markdown","text":"Statistical models include lot Greek letters (α,β,γ\\alpha, \\beta, \\gamma, etc.). can add Greek letters equation typing backslash \\ followed name letter \\alpha. Uppercase lower case letters possible capitalizing name (Δ\\Delta = $\\Delta$) (δ\\delta = $\\delta$).","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"subscripts-and-superscripts","dir":"Articles","previous_headings":"Header 1","what":"Subscripts and superscripts","title":"Introduction to R Markdown","text":"can add superscripts using ^ (πr2\\pi r^2=$\\pi r^2$) symbol subscripts using underscore _ (NtN_t = $N_t$). superscript subscript includes one character, put entire script within curly brackets {}: Nt−1≠Nt−1N_t-1 \\neq N_{t-1} $N_t-1 \\neq N_{t-1}$","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"brackets-and-parentheses","dir":"Articles","previous_headings":"Header 1","what":"Brackets and parentheses","title":"Introduction to R Markdown","text":"can add normal sized brackets parenthesis just typing equation: (x+y)(x + y) = (x + y) need bigger sizes, using $\\big($, $\\bigg($, $\\Bigg($ produces (\\big(, (\\bigg(, (\\Bigg( (switch opening parenthesis closing parenthesis square bracket needed)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"fractions","dir":"Articles","previous_headings":"Header 1","what":"Fractions","title":"Introduction to R Markdown","text":"Fractions can either inline (1/n1/n = $1/n$) stacked (1n\\frac{1}{n} = $\\frac{1}{n}$). stacked equations, terms first curly brackets numerator terms second curly brackets denominator.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"operators","dir":"Articles","previous_headings":"Header 1","what":"Operators","title":"Introduction to R Markdown","text":"Pretty much every operator need can written latex. common ones include ×\\times ($\\times$), <\\lt ($\\lt$), >\\gt ($\\gt$), ≤\\leq ($\\leq$), ≥\\geq ($\\geq$), ≠\\neq ($\\neq$), ∑\\sum ($\\sum$), ∏\\prod ($\\prod$), ∞\\infty ($\\infty$), ∝\\propto ($\\propto$). See documents Equations section list operators.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"adding-code","dir":"Articles","previous_headings":"","what":"Adding code","title":"Introduction to R Markdown","text":"ability format create pdf html documents great real strength R Markdown ability include run code within document. Code can included inline chunks","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"inline-code","dir":"Articles","previous_headings":"Adding code","what":"Inline code","title":"Introduction to R Markdown","text":"Inline code useful including (simple) R output directly text. Inline code can added enclosing R code `r `. example, typing `r mean(c(3,7,4,7,9))` compute print mean given vector. , print 6 instead code . can useful including summary statistics reports. example, vector indicating number individuals captured occasion mark-recapture study (e.g., n <- c(155, 132, 147, 163)) want include number occasions report, instead typing 4, can type `r length(n)`. prevent typos, extremely useful length(n) might change future. Instead manually changing number occasions, just re-render document new number occasions printed automatically.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"code-chunks","dir":"Articles","previous_headings":"Adding code","what":"Code chunks","title":"Introduction to R Markdown","text":"complicated code, generally useful use chunks inline code. Chunks start separate line ```{r} end ``` line (instead manually, can click Insert button top right script window, click R). two lines, can include many lines code want. example, ```{r}n1 <- 44     # Number individuals captured first occasionn2 <- 32     # Number individuals captured second occasionm2 <- 15     # Number previously marked individuals captured second occasionN <- n1 * n2 / m2     # Lincoln-Peterson estimate abundance```","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"chunk-options","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Chunk options","title":"Introduction to R Markdown","text":"Code chunks can take lot options control code run displayed documents. options go {r closing } (see options put cursor {r, hit space bar, hit tab). example: echo = FALSE shows output code code include = FALSE runs code display code output (useful chunks read format data) eval = FALSE shows code run (useful showing code) warning = FALSE message = FALSE can include ensure error messages warnings printed, can useful cleaning appearance documents cache = TRUE save results R code doesn’t rerun chunk unless code changed (useful chunks take long time run) .height .width control size figures pdf document inches centimeters (e.g., `.height = “3in”, notice quotation marks) See main R Markdown page complete list possible options.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"setting-defaults-for-all-chunks","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Setting defaults for all chunks","title":"Introduction to R Markdown","text":"Often useful set default behavior chunks rather including, example, warning = FALSE beginning one. , can include chunk beginning document: ```{r include = FALSE}opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)``` options can included chuck set default behaviors. can -ride defaults within chunks needed. can also load common packages chunk streamline chunks later document.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"tables","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Tables","title":"Introduction to R Markdown","text":"nicely print matrices data frames R Markdown document, use kable() function: Table 1: Automobile data 1974 Motor Trends US. kableExtra package provides even advanced options creating nice looking tables. See overview options provided package.","code":"library(knitr) kable(head(mtcars), caption= 'Table 1: Automobile data from 1974 *Motor Trends US*.',       align = 'l')"},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"figures","dir":"Articles","previous_headings":"Adding code > Code chunks","what":"Figures","title":"Introduction to R Markdown","text":"can also easily produce figures directly RMarkdown file. , simple demonstration produce many better looking figures throughout semester. Figure 1: Automobile mpg function horsepower.","code":"plot(mtcars$hp, mtcars$mpg)"},{"path":"http://rushinglab.github.io/FANR6750/articles/RMarkdown.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Introduction to R Markdown","text":"RStudio tool bar, click Help -> Cheatsheets select R Markdown cheat sheet (lots good cheat sheets well) RStudio’s R Markdown tutorial Tom Edward’s R Markdown tutorial Coding Club’s Getting Started R Markdown Cosma Shalizi’s Using R Markdown Class Reports","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/bonus_assignment.html","id":"bonus-assignment","dir":"Articles","previous_headings":"","what":"Bonus Assignment","title":"bonus","text":"Georgia Department Natural Resources (GADNR) interested understanding relationship several demographic variables likelihood deer hunter reach annual harvest limit (10 2 bucks). data collected GADNR (can found harvestdata dataset) contains: Limit: Whether hunter harvested limit (1- ; 0- )Age: Age hunterFam_veh: number vehicles owned householdDrive_dist: average drive distance hunter’s home hunting spotSick12: number available sick days accumulated hunter past 12 monthsSick24: number available sick days accumulated hunter past 24 monthsRange_hrs: number hours spent hunter shooting range prior start seasonMuzzleloader: Whether hunter participated muzzleloader season (1- ; 0- )Archery: Whether hunter participated archery season (1- ; 0- )Utilized: percent available days hunting season hunter utilizedPast_max: Whether hunter harvested limit past season (1- ; 0- )COUNTY`: hunter’s county residence tasked constructing parsimonious model can accurately predict whether hunter harvest limit. addition constructing model, GADNR like able tell model accuracy (.e. percentage time model correctly assign 0 1). several ways may approach research question, rough outline : Check missing values typos Consider variables considered numerical vs factors Check correlation among predictors consider removing necessary (justify reasons removed predictors) Consider using functions sample() setdiff() may use quadratic effects interaction terms required . point, can get predictions observations test dataset (using predict() function) already actual observations set Next can convert predictions (values probability) ones zeros factors. Consider using ifelse() function . Now can test see many values predictions dataframe match corresponding value test set. Include report least one figure shows relationship “Limit” one continuous predictor variables. Make sure include model predictions 95% confidence intervals can set variables either reference level (categorical) mean (continuous) Make sure figure displays multiple levels one categorical variable","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/graphics.html","id":"attributes-of-good-figures","dir":"Articles","previous_headings":"","what":"Attributes of good figures","title":"Checklist and tips for publication-quality graphics in R","text":"Effective data visualizations : Tell story. figure manuscript report communicate one main findings want readers know. casual reader able understand primary findings looking figures. intuitive. Strive figures readers can understand without read caption text. Readers least grasp main gist figure just looking . Err towards simplicity. Avoid clutter. Avoid gratuitous use visual attributes (color, size, shape) show attributes data. Avoid background colors. Use grid lines sparingly make interpretation data easier. readable. default sizes axis labels text rarely big enough. almost always need make bigger. Maximize data-ink ratio, within reason. Related point 3, “ink” figure show data rather non-data (axis ticks/labels, titles, legends, grid lines, etc.). Maximize ink shows data relative non-data, within reason. visual hierarchy. important information plot visible. Use layering, color, brightness, size, transparency, etc. make important attributes (generally data, sometimes even specific portions data) jump less important attributes (axis ticks/lines, grid lines, less important data points) fade background.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/graphics.html","id":"what-type-of-figure-should-i-use","dir":"Articles","previous_headings":"","what":"What type of figure should I use?","title":"Checklist and tips for publication-quality graphics in R","text":"Trends time series - line chart Amounts/comparisons discrete groups - bar chart, dot plot error bars Frequencies/distributions - histogram Associations 2 continuous variables - scatterplot Comparing distributions discrete groups - box--whisker plot, violin plt Spatial relationships - map","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/graphics.html","id":"visualization-checklist","dir":"Articles","previous_headings":"","what":"Visualization checklist","title":"Checklist and tips for publication-quality graphics in R","text":"list considerations making high-quality figures publications. complete list considered suggestions. figure include caption clearly explains elements needed interpret visualization? Remember caption figure understandable without referencing main text. figure accurately show variability data uncertainty estimates? axes start stop reasonable values? axis titles clearly indicate variables shown? include units? axis labels, axis titles, text annotations appropriately sized? figure includes different colors, shapes, sizes, attributes communicate properties data? figure includes different colors, shapes, sizes, legend show values represented attributes? color palettes colorblind-friendly? overlapping elements (points, bars, etc.), can clearly distinguished? figure includes multiple panels, axis limits consistent? figure includes multiple panels, panel labeled? grid lines? , show relevant information needed interpret figure? grid lines? , appropriately sized colored distract data? use 3D pie chart? 😠","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/graphics.html","id":"saving-high-resolution-figures-for-publication","dir":"Articles","previous_headings":"","what":"Saving high-resolution figures for publication","title":"Checklist and tips for publication-quality graphics in R","text":"many cases, need save figures rather render directly .rmd file. ggsave() function (opinion) easiest way save high-resolution figures. default, ggsave() save last plot created R using size graphics device. tell function save figure name file. , example, following code save ggsave() also uses whatever file type name figure file guess format save figure (case, png). Options include: “eps”, “ps”, “tex”, “pdf”, “jpeg”, “tiff”, “png”, “bmp”, “svg”. can also change defaults , example, save specific figure object, change figure size, change figure resolution. example, Easy!","code":"libary(ggplot2)  df <- data.frame(x = seq(1:10),                   y = seq(1:10))  ggplot(df, aes(x = x, y = y)) +   geom_point()  ggsave(\"figs/scatterplot.png\") libary(ggplot2)  df <- data.frame(x = seq(1:10),                   y = seq(1:10))  p <- ggplot(df, aes(x = x, y = y)) +       geom_point()  ggsave(filename = \"figs/scatterplot.png\",        plot = p,        width = 5,         height = 8,        units = \"in\",        dpi = 600)"},{"path":"http://rushinglab.github.io/FANR6750/articles/graphics.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Checklist and tips for publication-quality graphics in R","text":"Fundamentals Data Visualization, Claus Wilke. comprehensive book data visualization, lots R code.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/homework.html","id":"step-1-create-a-new-directory-to-store-your-homework-files","dir":"Articles","previous_headings":"Homework steps","what":"Step 1: Create a new directory to store your homework files","title":"Homework instructions","text":"Create new directory (ideally subdirectoy FANR6750\\homework directory) call LastNameFirstName-assignment#, replacing LastNameFirstName last first names # appropriate assignment number","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/homework.html","id":"step-2-create-a-new-r-markdown-file","dir":"Articles","previous_headings":"Homework steps","what":"Step 2: Create a new R Markdown file","title":"Homework instructions","text":"2a) Click File -> New File -> R Markdown... 2b) Title: window type Assignment #, substituting correct assignment number 2c) Author window, type name 2d) Click Ok 2e) Save R Markdown file LastnameFirstname-assignment# directory created step 1, replacing LastNameFirstName last first names # appropriate assignment number","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/homework.html","id":"step-3-complete-the-assignment","dir":"Articles","previous_headings":"Homework steps","what":"Step 3: Complete the assignment","title":"Homework instructions","text":"Complete assignment, following instructions lab document go","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/homework.html","id":"step-4-submit-your-assignment","dir":"Articles","previous_headings":"Homework steps","what":"Step 4: Submit your assignment","title":"Homework instructions","text":"4a) submitting assignment, always click “Knit” button sure .Rmd file can rendered HTML page. problems rendering file, please contact TA prior submission deadline. 4b) know file can rendered, upload LastnameFirstname-assignment#.Rmd LastnameFirstname-assignment#.html files eLC correct assignment folder Assignments fail follow instructions graded","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"what-is-r","dir":"Articles","previous_headings":"","what":"What is R?","title":"Lab 1: Introduction to R","text":"R free, open-source programming language software environment statistical computing, bioinformatics, visualization general computing. based ever-expanding set analytical packages perform specific analytical, plotting, programming tasks.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"why-r","dir":"Articles","previous_headings":"","what":"Why R?","title":"Lab 1: Introduction to R","text":"R free(!), runs pretty much every operating system, huge user base. R far programming language working data, widely used language fields ecology, evolution, wildlife sciences. plan pursue career fields, proficiency Ris quickly becoming prerequisite many jobs. Even don’t pursue career one fields, ability manipulate, analyze, visualize data (otherwise known data science) extremely marketable skill many professions right now.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"additional-resources-and-where-to-get-help","dir":"Articles","previous_headings":"","what":"Additional resources and where to get help","title":"Lab 1: Introduction to R","text":"go basics using R lab sessions many good online resources learning R getting help. favorites (material developed) include: Tom Edward’s online Learning R course John Fieberg’s online Statistics Ecologists book Data Analysis Visualization R Ecologists course, encounter error messages don’t understand need help figuring accomplish something R, google best friend (even experienced R users use google daily basis). key finding answers google asking right questions. spend much time topic lab, please refer links advice formulating R-related questions: ask R help Seeking help Data Analysis Visualization R Ecologists","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"the-rstudio-interface-and-panes","dir":"Articles","previous_headings":"Using R- the very basics","what":"The RStudio interface and panes","title":"Lab 1: Introduction to R","text":"Although users can work directly R, choose use RStudio IDE (Integrated Development Environment) R. use RStudio, must first R installed. opening RStudio, see 3 panes1. Console: console appear left side screen. can type code directly console (also known command line) executed immediately. console also output shown tasks executed R. Environment pane: environment pane appear top right screen. , can see objects created R well values objects R interprets (later). environment pane also includes tabs require use class. Plot pane: plot pane appear bottom right screen. might imagine, graphics displayed created R. pane also includes several useful tabs including Files tab (allows navigate manage files), Packages tab (can install manage additional R packages), Help tab can search R documentation pages.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"using-r-as-a-calculator","dir":"Articles","previous_headings":"Using R- the very basics","what":"Using R as a calculator","title":"Lab 1: Introduction to R","text":"statistical programming tool, one thing R good math. starting point, let’s treat R like fancy calculator. interact calculator typing numbers operators (+, -, *, /) Console window. Let’s try - bottom left window (Console), write Rcode required add two plus two press enter: run code, see answer printed window. Play code bit - try changing number operators run code .","code":"2+2"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"creating-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Creating objects","title":"Lab 1: Introduction to R","text":"can run R like calculator typing equations directly console printing answer. usually don’t want just calculation see answer. Instead, assign values objects. object saved R’s memory allows us use object later analysis. might seem bit confusing new programming let’s try . following code creates object called x assigns value 3: operator <-2 3 assignments R. Whatever left <- object’s name whatever right value. see later, objects can much complex simply number now, ’ll keep simple. try - change code create object called new.x. Instead assigning new.x number, give calculation, example 25/5. think value new.x ?","code":"x <- 3"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"naming-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Naming objects","title":"Lab 1: Introduction to R","text":"’s good idea give objects names tell something object represents. Names can long want spaces (also remember long names require typing brevity good rule thumb). Names can contain numbers letters begin number. R also case-sensitive , example, Apple apple. creating object names, also good idea avoid words show R functions. R generally smart enough distinguish attempting create object vs use function, avoiding practice save headache interpreting code (especially code looked ).","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"working-with-objects","dir":"Articles","previous_headings":"Using R- the very basics","what":"Working with objects","title":"Lab 1: Introduction to R","text":"exercise , may noticed running code, R print anything. simply told R create object (top right window, click Environment tab, see x new.x). Now stored R’s memory, can lot things . one, can print see value. , simply type name object run code4: can also use objects create new objects. think following code ? running , print new object y see value. right?","code":"new.x <- 25/5 new.x #> [1] 5 x <- 3 y <- x*4"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"r-scripts","dir":"Articles","previous_headings":"","what":"R scripts","title":"Lab 1: Introduction to R","text":"console useful simple tasks analyses become complicated, console efficient. need go back change line code? want show code someone else get help? Instead using console, work done using scripts (source editor pane). Scripts special files allow us write, save, run many lines code. Scripts can saved can work later send collaborators. create script, click File -> New File -> R Script. new file show new window.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"commenting-your-code","dir":"Articles","previous_headings":"R scripts","what":"Commenting your code","title":"Lab 1: Introduction to R","text":"R ignore code follows #. useful making code readable others. Use comments remind newly created object , explain line code , leave reminder later, etc. example, previous code, might good idea use comments define object represents: Notice run code, R ignores comments.","code":"n1 <- 44     # Number of individuals captured on first occasion  n2 <- 32     # Number of individuals captured on second occasion    m2 <- 15     # Number of previously marked individuals captured on second occasion"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"the-working-directory","dir":"Articles","previous_headings":"R scripts","what":"The working directory","title":"Lab 1: Introduction to R","text":"Now created new R script, need able save file somewhere computer. , can set working directory. addition providing place save script, setting working directory also tells R like put files come data management analyses (e.g. spreadsheets graphics) well find source data plan use particular project. two methods exist set working directory within R. can choose set working directory clicking Session –> Set working directory –> Choose directory navigating folder like store files. opened R script unsure current working directory located, can run getwd() see current working directory. can set working directory directly R script using setwd() function. example, set working directory folder called Lab_1 desktop, run following line code: C:/Users/mab46065/Desktop/Lab_1. Notice although computer probably create folder pathway using backslash (\\), R require forward slashes (/) instead. Also, using Mac, omit c: directory name.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"r-data-object-types","dir":"Articles","previous_headings":"","what":"R data object types","title":"Lab 1: Introduction to R","text":"point, briefly talked creating objects R. , discuss different object types R. important know types objects (e.g. vectors, lists, matrices, factors, data frames, arrays) working R interpret differently different object types required perform certain tasks. learn data structures encounter lab exercises.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"integer-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Integer class","title":"Lab 1: Introduction to R","text":"far, working objects store single number. However, often convenient store string numbers single object. R, strings called vectors usually created enclosing string c( ): can also create sequences consecutive numbers different ways: seq() function flexible useful familiar , sure look help page better understand use . Another useful function creating vectors rep(), repeats values vector: : sure notice difference using times argument vs argument! function class() indicates class (type element) object:","code":"x <- c(3,5,2,5) x #> [1] 3 5 2 5 x <- 1:10 x #>  [1]  1  2  3  4  5  6  7  8  9 10  x2 <- seq(from = 1, to = 10, by = 1) x2 #>  [1]  1  2  3  4  5  6  7  8  9 10 rep(x2, times = 2) #>  [1]  1  2  3  4  5  6  7  8  9 10  1  2  3  4  5  6  7  8  9 10 rep(x2, each = 2) #>  [1]  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 class(x) #> [1] \"integer\""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"character-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Character class","title":"Lab 1: Introduction to R","text":"vector can also contain characters (though mix numbers characters vector!): quotes around “Occasion1”, “Occasion2”, “Occasion3” critical. Without quotes, R assume objects called Occasion1, Occasion2 Occasion3. objects don’t exist R’s memory, error message. Vectors can length (including 1. fact, numeric objects ’ve working just vectors length 1). function length() tells long vector : class vector numeric characters entries? Hint: can also use c() function add elements vector:","code":"occasions <- c(\"Occasion1\", \"Occasion2\", \"Occasion3\") occasions #> [1] \"Occasion1\" \"Occasion2\" \"Occasion3\" class(occasions) #> [1] \"character\" length(x) #> [1] 10 mixed <- c(1, 2, \"3\", \"4\") y <- c(x, 4,8,3)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"factor-class","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Factor class","title":"Lab 1: Introduction to R","text":"Another class vectors referred factors. Factors similar character vectors R interpreting text strings perform math . difference, however, R sees factors grouping variables. category within factor referred ‘level’5.","code":"Species <- as.factor(c(1,3,4,2,3,3,4,1,1,1)) Species #>  [1] 1 3 4 2 3 3 4 1 1 1 #> Levels: 1 2 3 4 class(Species) #> [1] \"factor\""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"vectorized-arithmetic","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Vectorized arithmetic","title":"Lab 1: Introduction to R","text":"One useful properties vectors R can use simplify basic arithmetic operations need done multiple observations. example, consider following data wing chord (measure wing length) body mass Swainson’s thrushes (Catharus ustulatus): Swainson’s Thrush. Image courtesy VJAnderson via Wikicommons Perhaps want derive body condition individual based measures. One common metric body condition used ornithologists masssize\\frac{mass}{size}, wing chord used proxy body size. calculate body condition individual: time consuming error prone. Luckily, R vectorize basic arithmetic: can see, divide one vector another, R divides first element first vector first element second vector, etc. returns vector. Vectorized arithmetic works well vectors using length. happen though perform arithmetic vectors different lengths? Try running following code seeing R vectors. Notice way R recycles vector depends longer.","code":"cond1 <- 36.2/95.1 # Body condition of the first individual  cond2 <- 34.6/88.4 # Body condition of the second individual mass <- c(36.2, 34.6, 31.0, 31.8, 29.4, 32.0) wing <- c(95.1, 88.4, 97.9, 96.8, 92.3, 90.6)  cond <- mass/wing cond #> [1] 0.3807 0.3914 0.3166 0.3285 0.3185 0.3532 a <- c(1,10,100,1000) b <- c(1,2,3,4,5) c <- a/b c #> [1]   1.00   5.00  33.33 250.00   0.20  x <- c(1,10,100,1000, 10000) y <- c(1,2,3,4) z <- x/y z #> [1]     1.00     5.00    33.33   250.00 10000.00"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"indexing-vectors","dir":"Articles","previous_headings":"R data object types > Vectors","what":"Indexing vectors","title":"Lab 1: Introduction to R","text":"Often need work just subset vector. example, maybe vector plant biomass measured along transects need first third observations. Notice index certain elements vector y, use square brackets. Inside brackets, provided integer vector, integer refers position elements first vector. indexing vector can length (including 1). can also index vectors using logical vector. logical vector special type object contains values TRUE FALSE. using logical vector indexing, logical vector indicates elements keep (TRUE) remove (FALSE) original vector. reason, indexing vector must length focal vector; .e., length() == length(v) can also use indexing remove elements vector: rearrange order vector","code":"y <- c(2, 4, 8, 4, 25) y[c(1,3)] #> [1] 2 8 # Logical vector (which elements of y are greater than 4?) y > 4 #> [1] FALSE FALSE  TRUE FALSE  TRUE # Indexing using a logical vector (keep elements 3 and 5) y[y > 4] #> [1]  8 25 # Remove the second element y[-2] #> [1]  2  8  4 25 y[c(5,4,3,2,1)] #> [1] 25  4  8  4  2"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"functions","dir":"Articles","previous_headings":"R data object types","what":"Functions","title":"Lab 1: Introduction to R","text":"power R apparent large number built-functions available users. Functions small bits code perform specific task. functions accept one inputs called arguments return value new object. Let’s say following data number ticks recorded 5 dogs: total number ticks recorded study? , can use built-sum() function: mean number ticks per dog? variance?","code":"ticks <- c(4,7,2,3,150)  sum(ticks) #> [1] 166 mean(ticks) #> [1] 33.2 var(ticks) #> [1] 4267"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"arguments","dir":"Articles","previous_headings":"R data object types > Functions","what":"Arguments","title":"Lab 1: Introduction to R","text":"Every function takes different set arguments many cases need look arguments . best way get help specific function type question mark followed function name, bring help page bottom right panel. example, round function rounds number specified number decimal places. useful function don’t want print really large number digits: see round takes argument called x, number want round, number digits want round . provide arguments exact order defined don’t name . example, : name arguments, can switch order: Although don’t name arguments, ’s good idea get habit naming . make code easier read, help avoid mistakes can occur don’t put arguments correct order, makes easier trouble shoot code doesn’t expect .","code":"?round y <- mean(ticks) y #> [1] 33.2  round(y, 0) #> [1] 33 round(digits = 0, x = y) #> [1] 33"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"matrices","dir":"Articles","previous_headings":"R data object types","what":"Matrices","title":"Lab 1: Introduction to R","text":"Matrices similar vectors two dimensions. first dimension shows number rows matrix second shows number columns. , combined multiple vectors create matrix. Notice vectors need length. Notice matrices can contain one data class, numeric vectors coerced characters. matrices many uses R, one drawback lead us directly next object type.","code":"Site <- c(1,2,3,4,5) Species <- c('Alasmidonta varicosa',              'Alasmidonta varicosa',              'Alasmidonta varicosa',               'Lasmigona decorata',               'Lasmigona decorata') Year <- c(rep(2023,5)) mymatrix <- cbind(Site, Species, Year) mymatrix #>      Site Species                Year   #> [1,] \"1\"  \"Alasmidonta varicosa\" \"2023\" #> [2,] \"2\"  \"Alasmidonta varicosa\" \"2023\" #> [3,] \"3\"  \"Alasmidonta varicosa\" \"2023\" #> [4,] \"4\"  \"Lasmigona decorata\"   \"2023\" #> [5,] \"5\"  \"Lasmigona decorata\"   \"2023\""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"data-frames","dir":"Articles","previous_headings":"R data object types","what":"Data frames","title":"Lab 1: Introduction to R","text":"Although useful many applications, vectors matrices limited ability store multiple types data (numeric character). data frames become useful. Perhaps common type data object use R data frame. Data frames tabular objects (rows columns) similar structure spreadsheets (think Excel GoogleSheets). effect, data frames store multiple vectors - column data frame vector. advantage matrices column can different class (numeric, character, etc.) values within column must class. Just first row Excel spreadsheet can list column names, column data frame name (hopefully) provides information values column represent. see data frames work, let’s load data frame called jayData comes FANR6750 package.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"an-aside-about-packages","dir":"Articles","previous_headings":"R data object types > Data frames","what":"An aside about packages","title":"Lab 1: Introduction to R","text":"One R’s primary strengths large number packages available users. Packages units shareable code data created R users. already seen built-functions R comes . Packages allow users share lots lots functions serve specific purposes. Packages also allow users share data sets. packages cleaning data, visualizing data, making maps, fitting specialized models, basically anything else can think . Accessing code package first requires installing package. needs done per computer usually done using install.packages() function: Note name package (case devtools) must quotation marks. Packages installed using install.packages() stored centralized repository called CRAN (Comprehensive R Archive Network). devtools (package) installed computer, need re-run install.packages() function unless re-install/update R need update package newer version. Installing package automatically make functions package available given R session. tell R functions come , must load package using library() function6: Unlike install.packages(), library() must re-run time open R. people include calls library() beginning script packages needed run code loaded beginning script. Occasionally, packages stored places (e.g., github). packages can installed using different functions. example, created package course contains small data sets use labs throughout semester. package stored github can installed running: Note install_github() function devtools package need run library(devtools) install package. Make sure install FANR6750 package now access data sets.","code":"install.packages(\"devtools\") library(devtools) install_github(\"RushingLab/FANR6750\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"back-to-dataframes","dir":"Articles","previous_headings":"R data object types > Data frames","what":"Back to dataframes","title":"Lab 1: Introduction to R","text":"Note - discussed , want access function data sets come packages, first need load package current working environment. , use library() function, unquoted package name argument. loaded, package’s functions available use. Alternatively, can access functions given package without loading package using package.name::function.name(). example, want use filter() function dplyr package, type dplyr::filter(). Although less commonly used, method advantages: Sometimes different packages functions names. R default using function package loaded last. example, raster package also function called filter() load dplyr first (using library()raster, R default using raster’s filter() function, cause problems. share code others, :: method makes clear packages use functions. additional clarity often helpful reason often use :: course. get quick idea information data frame contains, can use head() tail() functions, print first last 6 rows data frame: can see jaydata contains eight columns: x, y, elevation, forest, chaparral, habitat, seeds, jays. ’ll learn columns represents later semester, though just like functions, many data sets help pages also can access help pages using ?jaydata. Several useful functions investigating structure data frames str() summary() str() tells us structure data frame, example x y numeric columns habitat contains character strings. summary() provides simple summary statistics variable. Another useful function nrow(), tells us now many rows data frame (similar length() vectors):","code":"library(FANR6750) data(\"jaydata\") # the data() function loads data sets the come with packages  head(jaydata) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 4 239437 3763524      1508   0.02      0.03    Pine   Med   43 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39  tail(jaydata) #>          x       y elevation forest chaparral habitat seeds jays #> 95  258937 3767124       804   0.19      0.68     Oak   Med   40 #> 96  259837 3768024       210   0.00      0.00     Oak   Low   33 #> 97  249337 3769524       467   0.70      0.09    Pine   Med   36 #> 98  262237 3767424      1318   0.02      0.23     Oak   Med   44 #> 99  261937 3770124       354   0.00      0.05    Bare   Low   33 #> 100 247837 3769524       686   0.10      0.32     Oak   Med   40 str(jaydata) #> 'data.frame':    100 obs. of  8 variables: #>  $ x        : num  258637 261937 246337 239437 239437 ... #>  $ y        : num  3764124 3769224 3764124 3763524 3767724 ... #>  $ elevation: int  423 506 859 1508 483 830 457 304 834 164 ... #>  $ forest   : num  0 0.1 0 0.02 0.26 0 0.02 0 0.54 0 ... #>  $ chaparral: num  0.02 0.45 0.26 0.03 0.37 0.01 0.22 0.09 0.21 0.11 ... #>  $ habitat  : chr  \"Oak\" \"Oak\" \"Oak\" \"Pine\" ... #>  $ seeds    : chr  \"Med\" \"Med\" \"High\" \"Med\" ... #>  $ jays     : int  34 38 40 43 36 39 38 35 41 33 ...  summary(jaydata) #>        x                y             elevation        forest       #>  Min.   :230737   Min.   :3761424   Min.   :  12   Min.   :0.0000   #>  1st Qu.:238762   1st Qu.:3765324   1st Qu.: 365   1st Qu.:0.0000   #>  Median :245587   Median :3766824   Median : 548   Median :0.0000   #>  Mean   :246949   Mean   :3767130   Mean   : 659   Mean   :0.0553   #>  3rd Qu.:254662   3rd Qu.:3768699   3rd Qu.: 929   3rd Qu.:0.0300   #>  Max.   :266137   Max.   :3773724   Max.   :1537   Max.   :0.7000   #>    chaparral       habitat             seeds                jays      #>  Min.   :0.000   Length:100         Length:100         Min.   :30.0   #>  1st Qu.:0.080   Class :character   Class :character   1st Qu.:36.0   #>  Median :0.210   Mode  :character   Mode  :character   Median :38.0   #>  Mean   :0.241                                         Mean   :38.6   #>  3rd Qu.:0.370                                         3rd Qu.:41.0   #>  Max.   :0.850                                         Max.   :48.0 nrow(jaydata) #> [1] 100"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"subsetting-data-frames","dir":"Articles","previous_headings":"R data object types > Data frames","what":"Subsetting data frames","title":"Lab 1: Introduction to R","text":"see shortly, one common tasks working data frames creating new objects parts full data frame. task involves subsetting data frame - selecting specific rows columns. many ways subsetting data frames R, many discuss learn .","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"selecting-columns","dir":"Articles","previous_headings":"R data object types > Data frames > Subsetting data frames","what":"Selecting columns","title":"Lab 1: Introduction to R","text":"First, may want select subset columns big data frame. Data frames essentially tables, means can reference rows columns number: data.frame[row#, column#]. row column numbers put inside square brackets following name data frame object. row number always comes first column number second. want select rows specific column, just leave row# blank. example, wanted vector containing number jays survey location: can also select columns using data.frame$column (data.frame name data frame object column name column). example, Notice hit tab type $, RStudio bring columns can use buttons find one want. Sometimes may want select one column. One way indexing using column names7: can also use select remove columns:","code":"jaydata[,8] #>   [1] 34 38 40 43 36 39 38 35 41 33 34 37 37 38 42 43 39 37 38 40 37 35 37 44 45 #>  [26] 37 36 34 48 43 39 41 45 38 35 38 39 38 41 38 36 43 38 36 33 41 38 30 39 36 #>  [51] 39 36 34 30 38 37 44 36 36 40 44 48 37 41 42 30 41 39 43 30 42 42 41 38 36 #>  [76] 37 33 44 38 35 45 41 35 38 37 45 33 42 34 45 40 42 40 44 40 33 36 44 33 40 jaydata$jays #>   [1] 34 38 40 43 36 39 38 35 41 33 34 37 37 38 42 43 39 37 38 40 37 35 37 44 45 #>  [26] 37 36 34 48 43 39 41 45 38 35 38 39 38 41 38 36 43 38 36 33 41 38 30 39 36 #>  [51] 39 36 34 30 38 37 44 36 36 40 44 48 37 41 42 30 41 39 43 30 42 42 41 38 36 #>  [76] 37 33 44 38 35 45 41 35 38 37 45 33 42 34 45 40 42 40 44 40 33 36 44 33 40 head(jaydata[, c('x', 'y', 'jays')]) #>        x       y jays #> 1 258637 3764124   34 #> 2 261937 3769224   38 #> 3 246337 3764124   40 #> 4 239437 3763524   43 #> 5 239437 3767724   36 #> 6 236437 3769524   39 head(subset(jaydata, select= -c(seeds))) #>        x       y elevation forest chaparral habitat jays #> 1 258637 3764124       423   0.00      0.02     Oak   34 #> 2 261937 3769224       506   0.10      0.45     Oak   38 #> 3 246337 3764124       859   0.00      0.26     Oak   40 #> 4 239437 3763524      1508   0.02      0.03    Pine   43 #> 5 239437 3767724       483   0.26      0.37     Oak   36 #> 6 236437 3769524       830   0.00      0.01     Oak   39"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab01_Intro_to_R.html","id":"filtering-rows","dir":"Articles","previous_headings":"R data object types > Data frames > Subsetting data frames","what":"Filtering rows","title":"Lab 1: Introduction to R","text":"select specific rows, can use row# method learned , time leaving columns blank: want one row, just put vector rows want: Note can use square brackets also subset vectors, case don’t need comma long tell R column want first: Sometimes, may know specific row number(s) want know value one columns want keep. can R indexing using logical subsetting. example, want just surveys conducted oak habitat, use: Notice need two equals signs (==) telling R want row habitat equals Oak. also select multiple rows using operators like greater , less , etc. slightly complicated example:","code":"jaydata[1,] #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423      0      0.02     Oak   Med   34 jaydata[1:2,] #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423    0.0      0.02     Oak   Med   34 #> 2 261937 3769224       506    0.1      0.45     Oak   Med   38  jaydata[c(1,30),] #>         x       y elevation forest chaparral habitat seeds jays #> 1  258637 3764124       423      0      0.02     Oak   Med   34 #> 30 259537 3765924      1419      0      0.07    Pine   Med   43 jaydata$jays[1] #> [1] 34 head(jaydata[jaydata$habitat == \"Oak\",]) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39 #> 7 263737 3766524       457   0.02      0.22     Oak   Med   38 head(jaydata[jaydata$elevation > 1000,]) #>         x       y elevation forest chaparral habitat seeds jays #> 4  239437 3763524      1508   0.02      0.03    Pine   Med   43 #> 24 261637 3768324      1276   0.02      0.36     Oak  High   44 #> 25 248737 3766524      1024   0.03      0.41    Pine   Low   45 #> 29 255937 3765024      1400   0.02      0.45     Oak  High   48 #> 30 259537 3765924      1419   0.00      0.07    Pine   Med   43 #> 32 245737 3762924      1004   0.02      0.32     Oak   Low   41 head(jaydata[jaydata$elevation < 1000 & jaydata$habitat == \"Oak\",]) #>        x       y elevation forest chaparral habitat seeds jays #> 1 258637 3764124       423   0.00      0.02     Oak   Med   34 #> 2 261937 3769224       506   0.10      0.45     Oak   Med   38 #> 3 246337 3764124       859   0.00      0.26     Oak  High   40 #> 5 239437 3767724       483   0.26      0.37     Oak   Med   36 #> 6 236437 3769524       830   0.00      0.01     Oak   Low   39 #> 7 263737 3766524       457   0.02      0.22     Oak   Med   38"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab02_Intro_project_and_Rmd.html","id":"introduction-to-r-markdown","dir":"Articles","previous_headings":"","what":"Introduction to R Markdown","title":"Lab 2: RMarkdown and RStudio projects","text":"Click ","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab02_Intro_project_and_Rmd.html","id":"projects-and-directories","dir":"Articles","previous_headings":"","what":"Projects and directories","title":"Lab 2: RMarkdown and RStudio projects","text":"Click ","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab02_Intro_project_and_Rmd.html","id":"homeworks","dir":"Articles","previous_headings":"","what":"Homeworks","title":"Lab 2: RMarkdown and RStudio projects","text":"Click ","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab02_Intro_project_and_Rmd.html","id":"assignment-not-for-a-grade","dir":"Articles","previous_headings":"","what":"Assignment (not for a grade)","title":"Lab 2: RMarkdown and RStudio projects","text":"Create R Markdown file following: following: 1a) Change author field YAML header name; 1b) Click Knit check can create html document .Rmd file; 1c) Save .Rmd file LastnameFirstname-homework0.Rmd directory named LastNameFirstName-homework0 YAML header allows control many “high level” options document. example, many HTML themes can use change font, colors, etc. document. , change output line YAML header : 2a) Click link look various themes available. Choose theme like change YAML header accordingly. Knit document using new theme Create level 1 header call “goals semester”. Add several bullet points objectives semester (can related FANR6750 ) Create level 2 header titled “favorite animals”. create numbered list, ranking top three favorite animals lecture 3, learned basic linear model. Create level 1 header titled “basic linear model” header write two block equations linear model discussed Create code chunk set chunk options ensure code runs shown html document. Inside chunk, copy following line code: date <- Sys.Date() code chunk created , type “completed assignment ” include inline code prints date object created chunk Bonus: code inside chunk ? inline code ? happen re-knit document tomorrow? things remember creating assignment: sure output type set : output: html_document Title document: title: \"Homework 0\" sure include first last name author section sure set echo = TRUE R chunks can see code output Regularly knit document work check errors See R Markdown reference sheet help creating R chunks, equations, tables, etc.","code":"output:    html_document:     theme: \"cosmo\""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"lab-2","dir":"Articles","previous_headings":"","what":"Lab 2","title":"Lab 3: Using R to explore sampling error","text":"Introduction RMarkdown Introduction RProjects directories Submitting HWs","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 3: Using R to explore sampling error","text":"Introduction Data visualization using ggplot2 Using loops explore sampling confidence intervals Assignment","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Lab 3: Using R to explore sampling error","text":"Today dive deeper several key topics lecture related statistical inference. particular, use R explore concepts related sampling, sampling error, sample size. , also learn (reinforce) new skills R, including: Data visualizationg using ggplot2 Using R’s built functions randomly generate samples probability distributions Using loops repeat code many times","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"graphics","dir":"Articles","previous_headings":"","what":"Graphics","title":"Lab 3: Using R to explore sampling error","text":"R powerful graphing capabilities make possible create data visualizations reports publications. get started today’s material, introduce ways visualize data. tasks R, many ways create graphs find people strong feelings best approach. debate graphics R usually boils using built-graphing functions (“base graphics”) vs ggplot2 package. advantages approaches, part newer generations R users prefer ggplot2. Therefore sample code provided lab reflect preference. However, don’t care make plots long effectively display information trying convey. prefer base graphics, means use base graphics.1 exercise, use small data set comes FANR6750 package: data frame contains two columns: calls average number calls made birds brief sampling period wind indicates whether high low wind sample.","code":"library(FANR6750) library(ggplot2) data(\"thrushdata\") str(thrushdata) #> 'data.frame':    100 obs. of  2 variables: #>  $ calls: num  9.1 5.7 7.7 14.9 12.3 16.7 17.8 11.6 7.5 14.3 ... #>  $ wind : chr  \"high\" \"high\" \"high\" \"high\" ..."},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"brief-introduction-to-ggplot2","dir":"Articles","previous_headings":"Graphics","what":"Brief introduction to ggplot2","title":"Lab 3: Using R to explore sampling error","text":"code provide use ggplot2, worth briefly learning/reviewing package approaches data visualization. power flexibility ggplot2 come ’s consistent structure. Although bit confusing first, get hang , structure actually makes quite easy create highly customized visualizations. plots created using ggplot2 use underlying structure: ggplot⏟initiateplot(data=df⏟dataframe,aes(x=,y=)⏟plotattributes)+geom_line()⏟geometry\\underbrace{ggplot}_{initiate\\; plot} ( \\underbrace{data = df}_{data\\;frame},\\;  \\underbrace{aes(x =\\; , y = \\;)}_{plot\\; attributes} ) + \\underbrace{geom\\_line()}_{geometry} ggplot() function initiates new plot. function, tell ggplot2 data frame using plot (ggplot accepts data frames input) tell map attributes data visual properties figures. Attributes mapped inside aes() argument. Attributes usually include location (x-axis y-axis placement), color, size, shape, line type, many others. general, attribute mapped one column data frame. ggplot() function simply initiates graph - run just portion code get blank graph. can see creating plot showing relationship wind (x-axis plot) calls (y-axis):  can see ggplot created figure correct axes labels. data. ’s didn’t tell ggplot type geometry use represent data. add geometry, can see data:  case, boxplot might make sense:  ’s also possible use one geometry:  reasonable figure showing call frequencies function wind. ggplot2 makes easy tweak way data visualized (maybe easy, can spend lot time tweaking minor details). example, maybe want color points based wind. want map attribute (color) variable (wind), make change inside aes:  ’s exactly wanted. boxplot points now colored function wind. make just points function wind, specify color = wind inside geom_point() function (anything ggplot() function apply geoms):  can also things like change size geometries. case, mapping variable attribute (size function data values). changes happen outside aes() argument:  One last example. many points overlap, can hard tell many individual points group. One way deal overplotting like make point slightly transparent. can alpha parameter:  , aren’t mapping alpha value data, include outside aes().","code":"ggplot(data = thrushdata, aes(x = wind, y = calls)) ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_point() ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_boxplot() ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_boxplot() +   geom_point() ggplot(data = thrushdata, aes(x = wind, y = calls, color = wind)) +    geom_boxplot() +   geom_point() ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_boxplot() +   geom_point(aes(color = wind)) ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_boxplot() +   geom_point(aes(color = wind), size = 5) ggplot(data = thrushdata, aes(x = wind, y = calls)) +    geom_boxplot() +   geom_point(aes(color = wind), size = 5, alpha = 0.5)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"exercise-1","dir":"Articles","previous_headings":"Graphics","what":"Exercise 1","title":"Lab 3: Using R to explore sampling error","text":"graph fine quick visualization data wouldn’t appropriate including publications reports. , Improve axis labels. include: title case, units, font size, etc. Run ?scale_y_continuous ?scale_x_discrete need help (note difference two functions!). ?theme may also useful Manually change color points (?scale_color_manual) Instead displaying data using boxplot, create histograms showing distribution call densities level wind (?geom_histogram) learn graphics functions, whether base ggplot2, probably need look help certain things. Google usually best bet good references: fantastic Fundamentals Data Visualization book Claus Wilke ggplot2 package website , yes, even base R graph gallery","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"using-r-to-generate-samples","dir":"Articles","previous_headings":"","what":"Using R to generate samples","title":"Lab 3: Using R to explore sampling error","text":"lecture, discussed concept sampling - , using randomly-selected subset larger population learn characteristics population. explore idea lab, taking advantage several built-R functions make easy simulate samples. start, let’s assume interested quantifying body mass gray squirrels (Sciurus carolinensis) campus. ’re simulating data, first need define true distribution body mass (.e., population). exercise, assume mean mass campus squirrels 500g. Furthermore, assume body mass population normally distributed (reasonable assumption given central limit theorem) standard deviation 30. can visualize population using code below2:  Sampling population simple using built rnorm() function. R number built-functions randomly generate samples range probability distributions (’s r stands function name). addition rnorm(), rpois() (Poisson distribution), rbinom() (binomial distribution), runif() (uniform distribution), etc. function, can run ?rnorm() get information arguments required run function. Briefly, need define x: number random numbers generate, .e. sample size) mean: mean normal distribution (case, population mean 500) sd: standard deviation normal distribution (case 30) Let’s start sample size 15:","code":"x <- seq(from = 380, to = 620, length.out = 1000) df <- data.frame(x = x,                  y = dnorm(x, 500, 30))  # Function to shade area under normal distribution dnorm_one_sd <- function(x){   norm_one_sd <- dnorm(x, 500, 30)   norm_one_sd[x <= 470 | x >= 530] <- NA   return(norm_one_sd) }  p <- ggplot() +   stat_function(fun = dnorm_one_sd, geom = \"area\", fill = \"#446E9B\", alpha = 0.3) +   geom_segment(aes(x = 500, xend = 500, y = -Inf, yend = max(df$y)), color = \"grey30\") +   geom_path(data = df, aes(x = x, y = y), color = \"#446E9B\") +   guides(color = \"none\") +   scale_x_continuous(\"Mass (g)\") +   scale_y_continuous(\"\") +   theme(axis.text.y = element_blank(), axis.ticks = element_blank()) +   annotate(\"text\", label = \"mu\", parse = TRUE, x = 500, y = max(df$y) + 0.0013) p #>  [1] 530.0 464.3 492.4 526.0 500.0 560.2 502.7 460.2 489.0 529.9 472.1 532.7 #> [13] 534.9 525.1 476.6 samp <- rnorm(15, 500, 30) samp"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"exercise-2","dir":"Articles","previous_headings":"Using R to generate samples","what":"Exercise 2","title":"Lab 3: Using R to explore sampling error","text":"Question: expected values mean standard deviation sample? Use R calculate mean standard deviation sample. close expected values? can also visualize sample compares population adding previous plot3:  Remember random sample, sample slightly different one shown .","code":"samp_df <- data.frame(x = samp) p2 <- p +   geom_rug(data = samp_df, aes(x = x)) +   geom_segment(aes(x = mean(samp), xend = mean(samp), y = -Inf, yend = dnorm(mean(samp), 500, 30)), color = \"grey30\", linetype = \"longdash\") +   annotate(\"text\", label = \"bar(y)\", parse = TRUE, x = mean(samp), y = dnorm(mean(samp), 500, 30) + 0.001)  p2"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"exploring-variability-among-samples","dir":"Articles","previous_headings":"","what":"Exploring variability among samples","title":"Lab 3: Using R to explore sampling error","text":"One foundational topics everything learn semester sampling error. Sampling error stems directly fact sampling inherently random process therefore two samples exactly (sample exactly resemble population drawn). Although straightforward, simple fact form basis nearly every topic discuss semester. fact, reason need statistics first place! explore concept sampling error detail, build intuition sampling error influences conclusions samples, expand previous exercise generating large number samples population. real world, course, generally single sample. R makes easy explore see repeat sampling process many times. essentially entire concept frequentist statistics repeat sampling process implemented , just copy paste samp <- rnorm(15, 500, 30) code , time naming sample object something different: obviously inefficient just 2-3 samples. Anytime find needing repeat code many times, consider using loop. R, loops allow us efficiently4 repeat code many times want. run loop, first specify many times want run loop also create empty matrix store samples created time run loop. syntax loop always start function (). Inside parentheses, first thing add counter, essentially just symbol use keep track iteration loop . convention, use counter, though make pretty much anything want (e.g., j, k, etc5). defining counter, add sequence values counter take time runs loop. want run loop 1000 times generate sequence 1 1000. defining counter sequence, put code want run inside curly brackets: Note time loop runs, place see counter inside curly brackets, R replace values specified sequence 1:nsamp (first loop, = 1, second loop = 2, etc). code inside loop uses indexing rules learned several weeks ago. sure understand indexing works, particularly relation counter . counter symbol sequence flexible, following code thing: samp_mat object just created contains 1000 samples hypothetical population squirrels, containing 15 observations. Although expect mean sample 500, know variation among 1000 sample means (sampling error!). explore much variation among sample means, first need calculate mean sample. ways. First, use loop6: works perfectly well, though loops can slow, many advanced users try avoid possible. R another built-function called apply() can useful avoiding loops need apply function row column matrix: first argument X matrix argument FUN function want apply row column matrix. MARGIN argument dimension want apply function (1 = row, 2 = column). case, telling R take mean column samp_mat. Apply return vector mean column. Let’s visualize distribution 1000 sample means:  notice distribution sampling means? can also calculate summary statistics sample means: notice mean sample means? interpretation standard deviation sample means? may figured , just used R approximate sampling distribution, .e., means large number samples population. Thanks central limit theorem, mean sample means population mean (, case, close population mean since 1000 large infinite). importantly, standard deviation sample means tells us average far sample mean population mean, .e., standard error. can prove calculating standard error first sample took comparing standard deviation sample means: , values close equal. won’t equal? factors influence close ?","code":"samp1 <- rnorm(15, 500, 30) samp2 <- rnorm(15, 500, 30) samp3 <- rnorm(15, 500, 30) . . . sampN <- rnorm(15, 500, 30) n <- 15 # sample size nsamp <- 1000 # number of samples samp_mat <- matrix(NA, nrow = n, ncol = nsamp) for(i in 1:nsamp){   samp_mat[,i] <- rnorm(n, 500, 30) } for(j in 1:nsamp){   samp_mat[,j] <- rnorm(15, 500, 30) }  for(samp in 1:nsamp){   samp_mat[,samp] <- rnorm(15, 500, 30) }  samps <- 1:nsamp for(i in seq_along(samps)){   samp_mat[,i] <- rnorm(15, 500, 30) } samp_means <- numeric() # object to store sample means for(i in 1:nsamp){   samp_means[i] <- mean(samp_mat[,i]) } samp_means <- apply(X = samp_mat, MARGIN = 2, FUN =  mean) p3 <- p +   geom_histogram(data = data.frame(x = samp_means), aes(x = x, ..density..),                   fill = \"grey50\", alpha = 0.5, binwidth = 10) p3 mean(samp_means) #> [1] 500.4 sd(samp_means) #> [1] 7.746 sd(samp)/sqrt(length(samp)) #> [1] 7.798 sd(samp_means) #> [1] 7.746"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"exercise-3","dir":"Articles","previous_headings":"Exploring variability among samples","what":"Exercise 3","title":"Lab 3: Using R to explore sampling error","text":", change sample size rerun previous code. Note need change n <- 15 another value re-run code. Run smaller sample sizes (e.g., n <- 5) larger sample sizes (e.g., n <- 30). happens sampling distribution standard error?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"confidence-intervals","dir":"Articles","previous_headings":"","what":"Confidence intervals","title":"Lab 3: Using R to explore sampling error","text":"can also use concepts just learned better understand confidence intervals calculated represent. Let’s quickly review properties normal distributions. First, ~68% samples normal distribution fall within 1 SD mean. squirrel population, means ~68% squirrel body masses fall within range 470g-530g. dark shaded area :  also true 95% samples fall within ~1.96 x SD mean. squirrel population, means ~95% squirrel body masses fall within range 441.2g-558.8g. dark shaded area . graph, visualizing distribution body masses individual squirrels way learn properties normal distributions. sample, (usually) trying learn individual squirrels, instead population mean. relate confidence intervals? Remember sampling distribution normal distribution. denote mean sampling distribution μx‾\\mu_{\\bar{x}} standard deviation σx‾\\sigma_{\\bar{x}}, means ~95% sample means fall within range μx‾−1.96×σx‾\\mu_{\\bar{x}} - 1.96 \\times \\sigma_{\\bar{x}} - μx‾+1.96×σx‾\\mu_{\\bar{x}} + 1.96 \\times \\sigma_{\\bar{x}}. Stare sentence minutes sinks . range samples? 95% sample means fall within range 484.8 - 515.2. can use quantile function check whether case: Pretty close! real world, never actually know μ̂x‾\\hat{\\mu}_{\\bar{x}} σ̂x‾\\hat{\\sigma}_{\\bar{x}} can’t know exact range values sample means fall within. can estimate sample! Remember : μ̂x‾=y‾\\hat{\\mu}_{\\bar{x}} = \\bar{y} σ̂x‾=sn=SE\\hat{\\sigma}_{\\bar{x}} = \\frac{s}{\\sqrt{n}} = SE. given sample, expect 95% sample means within interval y‾−1.96×SE\\bar{y} - 1.96 \\times SE - y‾+1.96×SE\\bar{y} + 1.96 \\times SE. initial sample: interpret range? Remember, saying anything probability population mean values. fact, case can see makes sense think probability . know whether confidence interval just calculated includes population mean . probability. can see visualizing estimated sampling distribution 95% confidence interval relative true population mean:  case, know population mean certain whether within confidence interval. important point. ’s important collect sample, population mean either within confidence interval isn’t. Even don’t know population mean, know ’s either within interval ’s . collect sample, however, don’t know exactly bounds confidence interval be7. know , given properties normally-distributed sampling distribution, 95% probability confidence interval include population mean 5% probability won’t. Put another way, repeated sampling many times, 95% confidence intervals contain population mean 5% won’t. don’t take word ! can actually prove , , hopefully clarify confidence intervals . already large number samples, let’s calculate 95% confidence interval one. , ’ll use loop: aid visualization, let’s compile data frame: can also create another column indicate whether confidence interval contains true population mean: Now can visualize samples. Note look first 200 samples just make things easier see (can change ’d like). Also note using additional ggplot2 functions arguments improve aesthetics graph:  1000 total intervals, proportion think black? course, sample forms basis thesis, way knowing whether black points one red points. :)","code":"500 - 1.96 * sd(samp_means) #> [1] 484.8 500 + 1.96 * sd(samp_means) #> [1] 515.2 quantile(samp_means, probs = c(0.025, 0.975)) #>  2.5% 97.5%  #> 485.1 515.3 y_bar <- mean(samp) se <- sd(samp)/sqrt(length(samp))  y_bar - 1.96 * se #> [1] 491.1 y_bar + 1.96 * se #> [1] 521.7 # empty objects to store lower and upper CI bounds for each sample lci <- uci <- numeric()  for(i in 1:nsamp){   # first, calculate SE of sample    # note that we use the object n for sample size. If you changed n above, be sure it is correct here   se <- sd(samp_mat[,i])/sqrt(n)      # next, calculate bounds of CI   # again, we use the samp_means object here to index the correct sample mean. Make sure it is correct   lci[i] <- samp_means[i] - 1.96 * se   uci[i] <- samp_means[i] + 1.96 * se } ci_df <- data.frame(sample_id = 1:nsamp,                      mean = samp_means,                     lci = lci,                      uci = uci) head(ci_df) #>   sample_id  mean   lci   uci #> 1         1 513.0 497.5 528.6 #> 2         2 494.2 477.2 511.2 #> 3         3 511.3 495.5 527.0 #> 4         4 501.6 487.5 515.7 #> 5         5 510.0 492.5 527.6 #> 6         6 487.9 469.5 506.4 library(dplyr) ci_df <- mutate(ci_df, Coverage = ifelse(lci < 500 & uci > 500, \"Yes\", \"No\")) head(ci_df) #>   sample_id  mean   lci   uci Coverage #> 1         1 513.0 497.5 528.6      Yes #> 2         2 494.2 477.2 511.2      Yes #> 3         3 511.3 495.5 527.0      Yes #> 4         4 501.6 487.5 515.7      Yes #> 5         5 510.0 492.5 527.6      Yes #> 6         6 487.9 469.5 506.4      Yes # plot the mean of each sample and color based on whether CI contains population mean ggplot(ci_df[1:200,], aes(x = sample_id, y = mean, color = Coverage)) +   # add horizontal line, corresponding to population mean   geom_hline(yintercept = 500, linetype = \"dashed\", color = \"grey50\") +   # add CI for each sample; make each bar semi-transparent to make them easier to see   geom_errorbar(aes(x = sample_id, ymin = lci, ymax = uci), alpha = 0.6, width = 0) +   # add sample means; make each point semi-transparent to make them easier to see   geom_point(alpha = 0.75) +   # change the y-axis title to be more informative   scale_y_continuous(\"Sample mean\") +   # set the colors manually so that No = red, Yes = black   scale_color_manual(values = c(\"red\", \"black\")) +   # turn off x-axis text, tick marks, and title since it is cluttered and not relevant   theme(axis.text.x = element_blank(),         axis.ticks.x = element_blank(),         axis.title.x = element_blank()) coverage <- ifelse(ci_df$Coverage == \"Yes\", 1, 0) mean(coverage) #> [1] 0.933"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"exercise-4","dir":"Articles","previous_headings":"Confidence intervals","what":"Exercise 4","title":"Lab 3: Using R to explore sampling error","text":"convention, people calculate report 95% confidence intervals. nothing magical 95% can calculate confidence intervals percentage think relevant inference. example, 80% normal distribution falls within ~1.281 standard deviations mean. Based information: Calculate 80% confidence intervals 1000 samples population Add 80% confidence intervals graph created . Use slightly thicker line 80% intervals 95% intervals can see limits intervals (hint - geom_errorbar() size argument). may help limit graph smaller number samples (e.g., 100) 80% confidence intervals compare 95% confidence intervals? reasons might prefer intervals 95% reporting results making decisions based confidence intervals sample?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab03_inference.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 3: Using R to explore sampling error","text":"assignment, build concepts code lab explore sampling error detail. Create R Markdown file titled “Assignment 1”. Within document, following: Create header called “Quantifying magnitude sampling error” header create R chunk. Within R chunk, create object called r contains difference 1000 sample means population mean (r thus vector 1000 numeric values). Next, use ggplot2 create histogram values. chunk, explain values histogram represent. answer, indicate expected value r positive vs negative values indicate sample. Create header called “Standardizing magnitude sampling error” header create another R chunk. Within chunk, calculate standard deviation r. Next, create new object (call t) dividing element r standard deviation r. Note using single line code hard code standard deviation (words, use sd() function rather typing actual value calculated previous step). Next, use ggplot2 create histogram values. chunk, explain values histogram represent, particular positive negative values represent. Also, explain variation values comes . words, samples value? friend another university also happens studying gray squirrels (chances!) claims squirrels bigger squirrels. friend history exaggeration certainly don’t want think squirrels better squirrels, skeptical. ask friend capture weigh 15 squirrels campus send data, included : Create new header called “squirrels really bigger squirrels?” header create another R chunk. Copy new code chunk. Next, perform following calculations: mean new data (call mu_samp) difference mean population mean UGA gray squirrels (call r_samp) divide r_samp standard deviation r calculated (call t_samp) Next, create new figure includes histogram created previous step plus dashed, vertical bar corresponding t_samp. Finally, think friend right? squirrels campus bigger? Discuss information figure informs conclusions. Remember assignment focused sampling error, sure answer relates back concept. sampling error histogram friend’s data? may use AI help inform answer, though , must include full prompt(s) used ’s full answer(s) separate section end document. Create new header called “Reflection”. header, answer following questions: 1-10 scale, rate confidence topics covered lab? lingering questions/confusion topics still ? used AI help aspects lab, well ? help understand material better? struggle specific topics? things remember creating document: sure include name author field header sure output type set : output: html_document sure set echo = TRUE R chunks code output visible knitted document Regularly knit document work check errors See R Markdown reference sheet help creating R chunks, equations, tables, etc. use AI part assignment, must include full prompt(s) used ’s full answer(s) separate section titled “AI assistance” end document.","code":"new_data <- c(575.5, 513.1, 509.4, 527.1, 485.6, 473.9, 467.3, 535.9, 527.5,                533.4, 535.9, 516.9, 572.9, 530.3, 498.3)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"lab-3","dir":"Articles","previous_headings":"","what":"Lab 3","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Graphics Sampling error Confidence intervals","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Introduction Linear models one categorical predictor: t-tests One sample Samples vs tails Two sample Paired","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Today discuss range scenarios broadly fall category t-tests. include: sample mean compared particular value (one-sample t-test) two sample means compared observations samples independent (two-sample t-test) two sample means compared observations samples paired (paired t-test)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"scenario-1-one-sample-t-tests","dir":"Articles","previous_headings":"","what":"Scenario 1: One sample t-tests","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"First, talk scenario one sample data interested mean. Question: average height students UGA equal 65 inches? Problem: don’t know true population mean (μ1\\mu_1). sample mean (y‾1\\bar{y}_1). relevant hypotheses : H0:μ1=65H_0 : \\mu_1 = 65 HA:μ1≠65H_A : \\mu_1 \\neq 65 Suppose collected random sample 100 students. plot distribution heights. case sample mean 61 inches. plot , difficult conclude one way another whether population mean equal 65.    can think problem different ways. see approach perspective linear model well perspective test. Remember linear model interested estimating β0\\beta_0 simple case just represents population mean. yi=β0+ϵiwhereϵi∼N(0,σ) y_i = \\beta_0 + \\epsilon_i \\ \\ \\ \\ \\ \\text{} \\ \\ \\ \\ \\ \\epsilon_i \\sim \\text{N}(0,\\sigma)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"key-points","dir":"Articles","previous_headings":"Scenario 1: One sample t-tests","what":"Key points","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"sample mean (y‾1\\bar{y}_1) different proposed population mean standard error difference small, tt-statistic far zero tt-statistic extreme critical values, reject null hypothesis (H0H_0)","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-linear-model","dir":"Articles","previous_headings":"Scenario 1: One sample t-tests > Exercise 1","what":"Formulation as a linear model","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Open FANR6750 RStudio project (one) Create new R script save directory store lab activities. Name something like lab04-t_tests.R Load FANR6750 package studentsdata object Create object students vector studentsdata dataframe. Fit linear model dataset. things think : include - 65 model statement? ~ 1 represent? intercept value mean? conclude model p-value?","code":"library(FANR6750) data(\"studentsdata\") students <- studentsdata$students mod1 <- lm(students - 65 ~ 1) summary(mod1) #>  #> Call: #> lm(formula = students - 65 ~ 1) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -10.406  -2.181  -0.156   1.819   7.794  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   -3.594      0.321   -11.2   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 3.21 on 99 degrees of freedom"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-by-hand","dir":"Articles","previous_headings":"Scenario 1: One sample t-tests > Exercise 1","what":"Formulation as a t-test by hand","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Now seen R can create linear model help us answer question population mean, dive deeper exactly computer . exactly decide reject null hypothesis? calculate p-value? get numbers? know lecture formula get test statistic one sample test following: t=y‾−μ0SEMwhereSEM=sn=1n−1∑=1n(yi−y‾)2n t = \\frac{\\bar{y} - \\mu_0}{SEM} \\ \\ \\ \\ \\ \\text{} \\ \\ \\ \\ \\ SEM = \\frac{s}{\\sqrt{n}} = \\frac{\\sqrt{\\frac{1}{n-1}\\sum^n_{=1}(y_i - \\bar{y})^2}}{\\sqrt{n}}  looks like lot, can break pieces lines code R. Create object sample mean. Create object standard error Put together calculate test statistic Now test statistic, need calculate critical value comparison. Calculate critical values Notice results used lm() function.","code":"y_bar <- mean(students) se_y <- sd(students)/sqrt(length(students)) t_stat <- (y_bar - 65)/se_y alpha <- 0.05 t_crit <- qt(c(alpha/2, 1-alpha/2), df= length(students)-1)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-using-the-built-in-r-function","dir":"Articles","previous_headings":"Scenario 1: One sample t-tests > Exercise 1","what":"Formulation as a t-test using the built in R function","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Use built R function t.test().","code":"t.test(students, mu= 65, alternative= 'two.sided', conf.level= 0.95) #>  #>  One Sample t-test #>  #> data:  students #> t = -11, df = 99, p-value <2e-16 #> alternative hypothesis: true mean is not equal to 65 #> 95 percent confidence interval: #>  60.77 62.04 #> sample estimates: #> mean of x  #>     61.41"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"samples-vs-tails","dir":"Articles","previous_headings":"","what":"Samples vs Tails","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"go , lets address issue many students find confusing t-tests. issue samples vs tails. mean talk ‘one sample t-test vs two sample t-test’? mean say t-test ‘one tail two tails’? Samples: number samples (one two) data using approach research question. comparing one sample specific value (e.g. μ1=6\\mu_1 = 6) call one sample test. instead, comparing two sample means eachother, call two sample test Tails: number tails (one two) related specific research question interested asking directly informed null alternative hypotheses. two tailed test hypotheses like : H0:μ1=x H_0: \\mu_1 = x HA:μ1≠x H_A: \\mu_1 \\ne x Notice == ≠\\ne hypotheses. μ1\\mu_1 may compared specific value compared another mean (e.g. μ2\\mu_2) hypotheses always set == vs ≠\\ne. constrast, one tailed test hypotheses like : H0:μ1≤x H_0: \\mu_1 \\le x HA:μ1>x H_A: \\mu_1 > x way around: H0:μ1≥x H_0: \\mu_1 \\ge x HA:μ1<x H_A: \\mu_1 < x Notice one tailed test, alternative interested one direction null includes everything else. types situations might want set t-test one tailed? two tailed? think statistical power detect effect?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"scenario-2-two-sample-t-tests","dir":"Articles","previous_headings":"","what":"Scenario 2: Two sample t-tests","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"dataset, average number calls 10 minutes point count surveys Song Thrushes (Turdus philomelos), species song bird eastern Europe. researcher interested understanding wind may affecting frequency bird calls. Specifically, like know whether high wind conditions results fewer average calls 10 minutes low wind conditions. Question: samples come population, come populations different means? Problem: don’t know true population means (μH\\mu_H, μL\\mu_L) assumption variances two populations equal, relevant hypotheses : H0:μH≥μLH_0 : \\mu_H \\ge \\mu_L HA:μH<μLH_A : \\mu_H < \\mu_L many tails test? decide ?  see problem can approached perspective linear model well considered test. linear model two sample t-test. look similar last one used, now added complexity two categorical levels (.e. high wind vs low wind): yi=β0+β1xi+ϵiwhereϵi∼N(0,σ) y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\ \\ \\ \\ \\ \\text{} \\ \\ \\ \\ \\ \\epsilon_i \\sim \\text{N}(0,\\sigma) interpret β0\\beta_0 β1\\beta_1?","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-linear-model-1","dir":"Articles","previous_headings":"Scenario 2: Two sample t-tests > Exercise 2","what":"Formulation as a linear model","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Load FANR6750 package thrushdata object. Lets look structure summary dataset well Fit linear model data estimates call frequency function wind conditions. interpret results? difference call frequency function wind conditions? numbers represent β0\\beta_0 β1\\beta_1 linear model ?","code":"library(FANR6750) data(\"thrushdata\") str(thrushdata) #> 'data.frame':    100 obs. of  2 variables: #>  $ calls: num  9.1 5.7 7.7 14.9 12.3 16.7 17.8 11.6 7.5 14.3 ... #>  $ wind : chr  \"high\" \"high\" \"high\" \"high\" ... summary(thrushdata) #>      calls          wind           #>  Min.   : 5.7   Length:100         #>  1st Qu.:11.3   Class :character   #>  Median :14.1   Mode  :character   #>  Mean   :14.1                      #>  3rd Qu.:16.7                      #>  Max.   :23.4  # Notice from the str() and summary() functions that R is interpreting the 'wind' variable  # as a character. Because we would like to treat 'wind' as a grouping variable, we can  # convert it to a factor in R.  thrushdata$wind <- as.factor(thrushdata$wind) mod2 <- lm(calls~ wind, data= thrushdata) summary(mod2) #>  #> Call: #> lm(formula = calls ~ wind, data = thrushdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #>  -6.02  -2.42  -0.12   2.17   6.95  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   11.720      0.414   28.32  < 2e-16 *** #> windlow        4.730      0.585    8.08  1.7e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 2.93 on 98 degrees of freedom #> Multiple R-squared:   0.4,   Adjusted R-squared:  0.394  #> F-statistic: 65.3 on 1 and 98 DF,  p-value: 1.69e-12"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-by-hand-1","dir":"Articles","previous_headings":"Scenario 2: Two sample t-tests > Exercise 2","what":"Formulation as a t-test by hand","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Similar Exercise 1, calculate test statistic hand. formulas creating test statistic well pooled variance shown : $$\\large t = \\frac{(\\bar{y}_H − \\bar{y}_L) − (\\mu_H − \\mu_L)}{\\sqrt{s^2_p/n_H + s^2_p/n_L}}$$ sp2s^2_p pooled variance $$\\large s^2_p = \\frac{(n_H − 1)s^2_H + (n_L − 1)s^2_L}{n_H + n_L − 2}$$ leave exercise perform. general steps follows: Calculate test statistic defining necessary terms R Calculate appropriate critical value/values Compare test statistic critical value/values reach conclusion hypotheses question","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-using-the-built-in-r-function-1","dir":"Articles","previous_headings":"Scenario 2: Two sample t-tests","what":"Formulation as a t-test using the built in R function","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Create two objects represent calls vectors high low wind conditions Use t.test() function perform two sample t-test Make sure set var.equal=TRUE. Otherwise, R assume variances two populations unequal. can test assumption using code . conclude? appropriate us assume equality variances? done assumption met? performed t-test dataset nice plot data. section provides brief introduction plotting R.","code":"yL <- thrushdata$calls[thrushdata$wind== 'low'] yH <- thrushdata$calls[thrushdata$wind== 'high'] t.test(yH, yL, var.equal = TRUE, paired = FALSE, alternative = \"less\") #>  #>  Two Sample t-test #>  #> data:  yH and yL #> t = -8.1, df = 98, p-value = 8e-13 #> alternative hypothesis: true difference in means is less than 0 #> 95 percent confidence interval: #>    -Inf -3.758 #> sample estimates: #> mean of x mean of y  #>     11.72     16.45 var.test(yH, yL) #>  #>  F test to compare two variances #>  #> data:  yH and yL #> F = 1.2, num df = 49, denom df = 49, p-value = 0.5 #> alternative hypothesis: true ratio of variances is not equal to 1 #> 95 percent confidence interval: #>  0.6874 2.1345 #> sample estimates: #> ratio of variances  #>              1.211"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"scenario-3-paired-t-test","dir":"Articles","previous_headings":"","what":"Scenario 3: Paired t-test","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"dataset, researcher interested studying effects pesticide caterpillar populations. Twelve bushes examined number caterpillars bush recorded. pesticide applied 3 days number caterpillars bush recorded . Question: pesticide negative effect caterpillar population? Note: Paired t-tests can thought one sample t-test differences. hypotheses test following: H0:μT≥μU H_0: \\mu_T \\ge \\mu_U HA:μT<μU H_A: \\mu_T < \\mu_U Another way think say μU−μT=μD\\mu_U - \\mu_T = \\mu_D. , can formulate hypotheses : H0:μD≤0 H_0: \\mu_D \\le 0 HA:μD>0 H_A: \\mu_D > 0","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-linear-model-2","dir":"Articles","previous_headings":"Scenario 3: Paired t-test > Exercise 4","what":"Formulation as a linear model","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"case, already know paired t-test can thought one sample t-test differences. result, linear model can set way Scenario 1 . left exercise","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-by-hand-2","dir":"Articles","previous_headings":"Scenario 3: Paired t-test > Exercise 4","what":"Formulation as a t-test by hand","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"left exercise general steps follows: Load caterpillar dataset Calculate difference untreated treated values mean different zero?  Calculate standard deviation differences $$\\large s_d = \\sqrt{\\frac{1}{n-1}\\sum_{=1}^n(y_i - \\bar{y})^2}$$ Calculate test statistic $$\\large t = \\frac{\\bar{y}-0}{s_d/\\sqrt{n}}$$ Compare appropriate critical value draw conclusion","code":"data(\"caterpillardata\") caterpillardata$diff <- caterpillardata$untreated - caterpillardata$treated  mean(caterpillardata$diff) #> [1] 1.5 ggplot(data = caterpillardata, aes(y = diff)) +    geom_boxplot() +   geom_hline(yintercept = 0, linetype = \"dashed\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"formulation-as-a-t-test-using-the-built-in-r-function-2","dir":"Articles","previous_headings":"Scenario 3: Paired t-test > Exercise 4","what":"Formulation as a t-test using the built in R function","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"Use t.test() function R perform test","code":"t.test(caterpillardata$diff, mu= 0, alternative= 'greater') #>  #>  One Sample t-test #>  #> data:  caterpillardata$diff #> t = 2.3, df = 11, p-value = 0.02 #> alternative hypothesis: true mean is greater than 0 #> 95 percent confidence interval: #>  0.3408    Inf #> sample estimates: #> mean of x  #>       1.5"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab04_t-tests.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 4: Linear models with one categorical predictor-- t-tests","text":"dataset comes fisheries landings data Yellowfin tuna (Thunnus albacares). weight pounds recorded fish well presence larval form Lacistorhynchidae tapeworm (Dasyrhynchus talismani). researcher believes presence tapeworm likely results less healthy therefore lighter fish. Create R Markdown file titled “Assignment 2”. Within document, following: Create R chunk load tuna data using: Make figure show weights infected healthy tuna. spend time considering type figure best display data Create header called “Hypotheses” header, plain English indicate null alternative hypotheses t-test. Also use R Markdown’s equation features write hypotheses using mathematical notation. order , need consider type test appropriate data many tails test Create header called “t-test hand”. header, add R chunk includes following code. Change ? inside set.seed() function number choosing, run code. get either 0 1 code returned 0, following: t-test tuna data without using t.test() function. Use functions mean, sd, possibly length. sure annotate code (either within R chunk using # plain text within R Markdown file) state decision (reject fail reject null) based results code returned 1, following: Ask AI perform t-test without using t.test() function. code may use functions mean, sd, possibly length. final code must annotated must state decision test. Provide prompts output used get final, working version code meets criteria . Regardless option completed, keep track long took complete question include time taken end answer. Create header called “t-test R”. header, t-test , time using t.test function (assume variances equal). get answer previous question. Explain think t-test failed reject null hypothesis even though sample means ~60 pounds different (almost 20% difference weight). data may caused result? Create new header called “Reflection”. header, answer following questions: 1-10 scale, rate confidence topics covered lab? lingering questions/confusion topics still ? used AI answer question 4, think helped? save time? use AI, think helped? things remember creating document: sure include name author field header sure output type set : output: html_document sure set echo = TRUE R chunks code output visible knitted document Regularly knit document work check errors See R Markdown reference sheet help creating R chunks, equations, tables, etc. use AI part assignment, must include full prompt(s) used ’s full answer(s) separate section titled “AI assistance” end document.","code":"library(FANR6750) data(\"tunadata\") set.seed(?) rbinom(1, 1, 0.5)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"lab-4","dir":"Articles","previous_headings":"","what":"Lab 4","title":"Lab 5: Completely randomized ANOVA","text":"t-tests One sample t-tests Two sample t-tests Samples vs tails Paired t-tests","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 5: Completely randomized ANOVA","text":"One way ANOVA linear model Using lm() Using aov() Performing ANOVA hand Plotting group means CI’s Multiple comparisons","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"scenario","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Scenario","title":"Lab 5: Completely randomized ANOVA","text":"independent samples >2a > 2 groups assume common variance","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"questions","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Questions","title":"Lab 5: Completely randomized ANOVA","text":"means differ much (effect sizes?) means differ ?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"null-hypothesis","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Null hypothesis","title":"Lab 5: Completely randomized ANOVA","text":"H0:μ1=μ2=μ3=...=μaH_0 : \\mu_1 = \\mu_2 = \\mu_3 = . . . = \\mu_a H0:α1=α2=α3=...=αa=0H_0 : \\alpha_1 = \\alpha_2 = \\alpha_3 = . . . = \\alpha_a = 0","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"alternative-hypothesis","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Alternative hypothesis","title":"Lab 5: Completely randomized ANOVA","text":"$H_A: \\mu_i \\ne \\mu_j \\space\\text{, j (1, . . ., )}$","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"formulation-as-a-linear-model","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Formulation as a linear model","title":"Lab 5: Completely randomized ANOVA","text":"Just seen last lab, many analyses perform semester can thought multiple ways. one-way ANOVA can formulated linear model: yij=μ+αi+ϵijwhereϵij∼N(0,σ) y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij} \\ \\ \\ \\ \\ \\text{} \\ \\ \\ \\ \\ \\epsilon_{ij} \\sim \\text{N}(0,\\sigma) Can define term context data? need add additional sub-scripting observation residual terms?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"exercise-1","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Exercise 1","title":"Lab 5: Completely randomized ANOVA","text":"dataset comes pilot study implemented US Forest Service. service interested purchasing number chainsaws use various projects fieldwork. Due safety concerns, service hoping select chainsaw brand (, B, C, D) lowest average kickback angle. Kickback, refers sudden upward motion saw’s guide bar saw strikes hard object, one leading causes injuries chainsaw work. service like prioritize safety, secondary objective find reasonably priced brand. brands range price (D,B,,C) Brand D expensive. Load sawdata dataset examine data may notice something seems different dataset previous ones looked last weeks. data called ‘wide format’. means level grouping variable (case chainsaw brand) gotten column dataframe. wide format often convenient reports allows easily see trends table, long format useful succinctly storing data use analyses. Convert dataframe wide format long format1 Visualize data","code":"library(FANR6750) data(\"sawdata\")  head(sawdata) #>   Chainsaw  A  B  C  D #> 1        1 42 38 52 29 #> 2        2 36 50 54 35 #> 3        3 33 44 48 31 #> 4        4 39 37 49 36 #> 5        5 43 47 54 32 library(tidyr) sawdata <- sawdata %>%  pivot_longer(cols= c('A', 'B', 'C', 'D'),                          names_to= 'Brand',                          values_to= 'angle') sawdata$Brand <- as.factor(sawdata$Brand) ggplot(sawdata, aes(x = Brand, y = angle)) +   geom_boxplot() +   scale_y_continuous(\"Kickback angle\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"two-anova-functions-in-r-aov-and-lm","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Two ANOVA functions: in R: aov() and lm()","title":"Lab 5: Completely randomized ANOVA","text":"R 2 common functions ANOVA: aov() lm() use class","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"using-lm","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Using lm()","title":"Lab 5: Completely randomized ANOVA","text":"Perform analysis using lm() function Can interpret estimate? last three brands shown (happened Brand )? p-value represent hypothesis testing?","code":"mod1 <- lm(angle~ Brand, data= sawdata)  summary(mod1) #>  #> Call: #> lm(formula = angle ~ Brand, data = sawdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #>   -6.2   -2.8    0.5    2.8    6.8  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)    38.60       1.80   21.39  3.4e-13 *** #> BrandB          4.60       2.55    1.80  0.09027 .   #> BrandC         12.80       2.55    5.02  0.00013 *** #> BrandD         -6.00       2.55   -2.35  0.03184 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.03 on 16 degrees of freedom #> Multiple R-squared:  0.784,  Adjusted R-squared:  0.743  #> F-statistic: 19.3 on 3 and 16 DF,  p-value: 1.45e-05  model.matrix(mod1) #>    (Intercept) BrandB BrandC BrandD #> 1            1      0      0      0 #> 2            1      1      0      0 #> 3            1      0      1      0 #> 4            1      0      0      1 #> 5            1      0      0      0 #> 6            1      1      0      0 #> 7            1      0      1      0 #> 8            1      0      0      1 #> 9            1      0      0      0 #> 10           1      1      0      0 #> 11           1      0      1      0 #> 12           1      0      0      1 #> 13           1      0      0      0 #> 14           1      1      0      0 #> 15           1      0      1      0 #> 16           1      0      0      1 #> 17           1      0      0      0 #> 18           1      1      0      0 #> 19           1      0      1      0 #> 20           1      0      0      1 #> attr(,\"assign\") #> [1] 0 1 1 1 #> attr(,\"contrasts\") #> attr(,\"contrasts\")$Brand #> [1] \"contr.treatment\""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"using-aov","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Using aov()","title":"Lab 5: Completely randomized ANOVA","text":"Perform analysis using aov() function View ANOVA table can conclude ANOVA table? appear less information (just different) provided used lm() function? default output summary fine viewing console want include ANOVA table report paper? manually create table copy/paste values, ’s pain. Luckily, handy package called broom turns output many model functions cleaned-data frames2: R Markdown, can even include data frame nicely formatted table directly knitted document:","code":"aov1 <- aov(angle ~ Brand, data = sawdata) summary(aov1) #>             Df Sum Sq Mean Sq F value  Pr(>F)     #> Brand        3    943   314.2    19.3 1.4e-05 *** #> Residuals   16    260    16.3                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 broom::tidy(aov1) #> # A tibble: 2 × 6 #>   term         df sumsq meansq statistic    p.value #>   <chr>     <dbl> <dbl>  <dbl>     <dbl>      <dbl> #> 1 Brand         3  943.  314.       19.3  0.0000145 #> 2 Residuals    16  260.   16.3      NA   NA aov_df <- broom::tidy(aov1)  options(knitr.kable.NA = '') # don't print NA's in table knitr::kable(aov_df,        col.names = c(\"Source\", \"df\", \"SS\", \"MS\", \"F\", \"p\"),       align = 'c', format = \"html\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"getting-estimates-of-group-means-mus-and-ses","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Using aov()","what":"Getting estimates of group means ( μ\\mu’s) and SE’s","title":"Lab 5: Completely randomized ANOVA","text":"","code":"model.tables(aov1, type = \"means\", se = TRUE) #> Tables of means #> Grand mean #>        #> 41.45  #>  #>  Brand  #> Brand #>    A    B    C    D  #> 38.6 43.2 51.4 32.6  #>  #> Standard errors for differences of means #>         Brand #>         2.551 #> replic.     5"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"getting-estimates-of-effect-sizes-alphas-and-ses","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Using aov()","what":"Getting estimates of effect sizes ( α\\alpha’s) and SE’s","title":"Lab 5: Completely randomized ANOVA","text":"","code":"model.tables(aov1, type = \"effects\", se = TRUE) #> Tables of effects #>  #>  Brand  #> Brand #>     A     B     C     D  #> -2.85  1.75  9.95 -8.85  #>  #> Standard errors of effects #>         Brand #>         1.804 #> replic.     5"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"constructing-an-anova-table-by-hand","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Constructing an ANOVA table by hand","title":"Lab 5: Completely randomized ANOVA","text":"Just last lab, see R actually ANOVA ’ll get practice constructing ANOVA table hand.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"grand-mean","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"Grand mean","title":"Lab 5: Completely randomized ANOVA","text":"","code":"(ybar. <- mean(sawdata$angle)) #> [1] 41.45"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"group-means","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"Group means","title":"Lab 5: Completely randomized ANOVA","text":"things R, several ways calculate group means. First, find group means, hard way: Better yet, use tapply find group means (base R way): Finally, find group means, tidyverse way: Although methods want - return mean kickback angle group - note return type object. Use class() function see type object one returns. might output method useful?","code":"A <- sawdata$angle[sawdata$Brand == \"A\"] B <- sawdata$angle[sawdata$Brand == \"B\"] C <- sawdata$angle[sawdata$Brand == \"C\"] D <- sawdata$angle[sawdata$Brand == \"D\"]  (ybar.i <- c(mean(A), mean(B), mean(C), mean(D))) #> [1] 38.6 43.2 51.4 32.6 (ybar.i <- tapply(sawdata$angle, INDEX = sawdata$Brand, FUN = mean)) #>    A    B    C    D  #> 38.6 43.2 51.4 32.6 library(dplyr) (ybar.i <- sawdata %>%              group_by(Brand) %>%               summarise(mu = mean(angle))) #> # A tibble: 4 × 2 #>   Brand    mu #>   <fct> <dbl> #> 1 A      38.6 #> 2 B      43.2 #> 3 C      51.4 #> 4 D      32.6"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"sum-of-squares","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"Sum of squares","title":"Lab 5: Completely randomized ANOVA","text":"Sums squares among Sums squares within3","code":"# Number of saw brands a <- length(unique(sawdata$Brand))  # Number of measurements of each brand (note, this assumes balanced design) n <- nrow(sawdata)/a  SSa <- n * sum((ybar.i$mu - ybar.)^2) SSa #> [1] 942.5 # individual saw data y.ij <- sawdata$angle  SSw <- sum((y.ij - rep(ybar.i$mu, times = n)) ^ 2) SSw #> [1] 260.4"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"means-squares","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"Means squares","title":"Lab 5: Completely randomized ANOVA","text":"Mean squares among Mean squares within","code":"df1 <- a - 1 (MSa <- SSa / df1) #> [1] 314.2 df2 <- a * (n - 1) (MSw <- SSw / df2) #> [1] 16.27"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"f-statistic","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"F-statistic","title":"Lab 5: Completely randomized ANOVA","text":"","code":"(F.stat <- MSa / MSw) #> [1] 19.3"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"critical-values-and-p-values","dir":"Articles","previous_headings":"One-way ANOVA as a linear model > Constructing an ANOVA table by hand","what":"Critical values and p-values","title":"Lab 5: Completely randomized ANOVA","text":"Recall ANOVA, consider upper tail F-distribution. Critical value: p-value","code":"alpha <- 0.05 (F.crit <- qf(1 - alpha, df1, df2)) #> [1] 3.239 (p.value <- 1 - pf(F.stat, df1, df2)) #> [1] 1.446e-05"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"plotting-group-means-and-confidence-intervals","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Plotting group means and confidence intervals","title":"Lab 5: Completely randomized ANOVA","text":"Although performed ANOVA multiple ways can conclude least one difference means chainsaw brands, still tasks perform. first plot data meaningful way. type data least two ways display results. example one ; ’ll practice second one homework assignment. ANOVA context, confidence intervals can constructed using equation: CI=Pointestimate±tα/2,(n−1)×SE CI = Point\\: estimate \\pm t_{\\alpha/2,(n-1)}\\times SE Figure 1: Mean kickback angle four chainsaw brands 95% CI","code":"# Extract the SE mean.SE <- 2.551 # hard coded from model.tables()  # OR  table <- model.tables(aov1, type = \"means\", se = TRUE) mean.SE <- as.numeric(table$se) # more explicit mean.SE #> [1] 2.551  # Compute confidence intervals tc <- qt(0.975, a*(n-1)) lowerCI <- ybar.i$mu - tc*mean.SE upperCI <- ybar.i$mu + tc*mean.SE ggplot(ybar.i, aes(x = Brand, y = mu, fill = Brand)) +   geom_col() +   geom_errorbar(aes(ymin = lowerCI, ymax = upperCI), width = 0) +   scale_y_continuous(\"Chainsaw kickback angle\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"multiple-comparisons","dir":"Articles","previous_headings":"One-way ANOVA as a linear model","what":"Multiple comparisons","title":"Lab 5: Completely randomized ANOVA","text":"brands significantly different one another? answer original question start exercise?","code":"TukeyHSD(aov1) #>   Tukey multiple comparisons of means #>     95% family-wise confidence level #>  #> Fit: aov(formula = angle ~ Brand, data = sawdata) #>  #> $Brand #>      diff      lwr   upr  p adj #> B-A   4.6  -2.6998  11.9 0.3079 #> C-A  12.8   5.5002  20.1 0.0007 #> D-A  -6.0 -13.2998   1.3 0.1276 #> C-B   8.2   0.9002  15.5 0.0251 #> D-B -10.6 -17.8998  -3.3 0.0037 #> D-C -18.8 -26.0998 -11.5 0.0000 plot(TukeyHSD(aov1))"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_anova.html","id":"optional-exercise-not-graded","dir":"Articles","previous_headings":"","what":"Optional exercise (not graded)","title":"Lab 5: Completely randomized ANOVA","text":"biologist wants compare growth four different tree species considering use reforestation efforts. Thirty two seedlings species planted time large plot. Heights meters recorded several years. Within RMarkdown document, following: Create R chunk load data using: Create header called “Hypotheses” header, indicate, plain English, null alternative hypotheses . Also use R Markdown’s equation features write hypotheses using mathematical notation Create header called “ANOVA hand”. header, perform ANOVA analysis (degrees freedom, sums--squares, mean-squares, F-value) without using aov(). Compute either critical value F, p-value. sure annotate code Create header called “ANOVA R”. header, perform ANOVA analysis data using lm() function using aov() function subheader called “ANOVA results” indicate whether null hypothesis can rejected α=0.05\\alpha = 0.05 level include well-formatted ANOVA table using broom::tidy() function create barplot showing effect sizes 95% confidence intervals around Create header called “means different?”. Use Tukey’s HSD test determine pairs means differ α=0.05\\alpha = 0.05 level. Include sentences indicating pairs different. inconsistency initial ANOVA results post-hoc comparisons, explain might case things remember finishing exercise: sure output type set : output: html_document sure include name author field header sure set echo = TRUE R chunks can see code output See R Markdown reference sheet help creating R chunks, equations, tables, etc. use AI part assignment, must include full prompt(s) used ’s full answer(s) separate section titled “AI assistance” end document.","code":"library(FANR6750) data(\"treedata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"lab-4","dir":"Articles","previous_headings":"","what":"Lab 4","title":"Lab 5: Multiple Regression","text":"ANOVA Performing ANOVA lm() aov() Performing ANOVA hand Plotting confidence intervals Multiple comparisons","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 5: Multiple Regression","text":"Multiple Regression ANCOVA Blocking Centering predictors","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"scenarios","dir":"Articles","previous_headings":"Today’s topics","what":"Scenarios","title":"Lab 5: Multiple Regression","text":"like perform ANOVA additional continuous predictor likely contributing variation response. like perform ANOVA another grouping variable treatment likely contributing variation response. like perform linear regression one continuous predictor (HW assignment).","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"scenario-1-ancova","dir":"Articles","previous_headings":"","what":"Scenario 1: ANCOVA","title":"Lab 5: Multiple Regression","text":"Remember lecture ANCOVA (Analysis Covariance) used interested performing ANOVA first accounting continuous variable. variable may may particular interest us, wise least take account. ANCOVA sometimes thought hybrid ANOVA regression, remember just different forms linear model. write ANCOVA linear model: $$ \\Large y_{ij} = \\mu + \\alpha_i + \\beta(x_{ij} − \\bar{x}) + \\epsilon_{ij} $$ interpret parameter context data? hypotheses related model?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"data-example","dir":"Articles","previous_headings":"Scenario 1: ANCOVA","what":"Data example","title":"Lab 5: Multiple Regression","text":"dataset contains information Blue Tilapia (Oreochromis aureus), species fish sometimes used aquaponics systems. case, researcher interested understanding various diet regimens affect tilapia weight (g) period 3 months. performing study, like take account length (cm) fish start study. Given longer fish tend weigh , suspects ignoring variable appropriate. Load data check structure:","code":"library(FANR6750) data(\"dietdata\") str(dietdata) #> 'data.frame':    60 obs. of  5 variables: #>  $ weight: num  185 217 232 238 238 ... #>  $ diet  : Factor w/ 4 levels \"Control\",\"Low\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ length: num  11 14.8 11.9 25.6 23.2 14.1 15.9 27.1 32.1 35.5 ... #>  $ sex   : chr  \"F\" \"F\" \"M\" \"F\" ... #>  $ gravid: chr  \"Y\" NA NA \"N\" ..."},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"an-aside-about-factors","dir":"Articles","previous_headings":"Scenario 1: ANCOVA > Data example","what":"An aside about factors","title":"Lab 5: Multiple Regression","text":"Factors look like character strings behave quite differently, understanding way R handles factors key working type data. key difference factors character strings factors used represent categorical data fixed number possible values, called levels1. Factors can created using factor() function: didn’t explicitly tell R levels factor, assume levels adult juvenile. Furthermore, default R assigns levels alphabetical order, adult first level juvenile second level: Sometimes alphabetical order might make sense: order R assign treatment levels?  want order factor different way, can use optional levels argument: Sometimes may also need remove factor levels. example, let’s see happens typo one factors:  Oops, one field techs accidentally capitalized “Male”. Let’s change lowercase:  Hmm, ’s still bar Male. can see ’s happening looking levels: Removing/editing factors doesn’t change levels! R still thinks three levels, just zero Male entries. remove mis-specified level, need drop :","code":"age <- factor(c(\"adult\", \"juvenile\", \"adult\", \"juvenile\")) levels(age) #> [1] \"adult\"    \"juvenile\" treatment <- factor(c(\"medium\", \"low\", \"high\")) treatment <- factor(c(\"low\", \"medium\", \"high\"), levels = c(\"low\", \"medium\", \"high\")) levels(treatment) #> [1] \"low\"    \"medium\" \"high\" sex = factor(c(\"male\", \"female\", \"female\", \"Male\", \"male\", \"female\", \"female\"))  barplot(table(sex)) sex[sex == \"Male\"] <- \"male\" barplot(table(sex)) levels(sex) #> [1] \"female\" \"male\"   \"Male\" sex <- droplevels(sex) barplot(table(sex))"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"back-to-the-data","dir":"Articles","previous_headings":"Scenario 1: ANCOVA","what":"Back to the data","title":"Lab 5: Multiple Regression","text":"need change diet factor, ’ll explicitly define level order: Let’s also visualize data:    finally, let’s quantify relationship length weight using linear regression:  can also add regression lines CIs plot using predict() function: Create new data frame containing sequence values predictor variable length Predict weight using values length Put predictions data together plotting  Note simple case, use built stat_smooth() ggplot2 plot regression line, though always work. predict() general method creating plotting regression lines fitted models.  ’s clear strong, positive relationship length weight. want quantify whether effect diet weight, clearly need control length analysis. aside, let’s see happens ignored length simply performed one-way ANOVA. Notice conclude difference weights function diet type even though later lab, see diet important factor. reach incorrect conclusion ?  Now ready perform ANCOVA. make interpretation simpler, center continuous predictor. interpret parameter estimates context data?  , can create predictions weight sequences lengths, every level diet:   can also look multiple comparisons. Note used lm() function instead aov(), able use TukeyHSD() . Instead can use glht() function multcomp package.","code":"dietdata$diet <- factor(dietdata$diet, levels = c(\"Control\", \"Low\", \"Med\", \"High\")) levels(dietdata$diet) #> [1] \"Control\" \"Low\"     \"Med\"     \"High\" library(ggplot2)  ggplot(dietdata, aes(x = length, y = weight)) +   geom_point() ggplot(dietdata, aes(x = diet, y = weight)) +   geom_boxplot() fm1 <- lm(weight ~ length, dietdata) summary(fm1) #>  #> Call: #> lm(formula = weight ~ length, data = dietdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -56.91 -12.58   0.63  10.28  61.69  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  213.174      8.069    26.4   <2e-16 *** #> length         2.594      0.337     7.7    2e-10 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 21.7 on 58 degrees of freedom #> Multiple R-squared:  0.505,  Adjusted R-squared:  0.497  #> F-statistic: 59.2 on 1 and 58 DF,  p-value: 1.99e-10 library(knitr) kable(broom::tidy(fm1), col.names = c(\"Term\", \"Estimate\", \"SE\", \"Statistic\", \"p-value\")) size <- dietdata$length nd <- data.frame(length = seq(min(size), max(size), length = 50)) E1 <- predict(fm1, newdata = nd, se.fit = TRUE, interval=\"confidence\") predictionData <- data.frame(E1$fit, nd)  ggplot() +   geom_point(data = dietdata, aes(x = length, y = weight)) +   geom_ribbon(data = predictionData, aes(x = length, ymin = lwr, ymax = upr),               color = \"black\", linetype = \"longdash\", fill = NA) +   geom_path(data = predictionData, aes(x = length, y = fit)) summary(aov(weight~ diet, data= dietdata)) #>             Df Sum Sq Mean Sq F value Pr(>F) #> diet         3   5459    1820    2.05   0.12 #> Residuals   56  49734     888 dietdata$length_centered <- dietdata$length - mean(dietdata$length) fm2 <- lm(weight~ length_centered + diet, data= dietdata) summary(fm2) #>  #> Call: #> lm(formula = weight ~ length_centered + diet, data = dietdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -38.25 -12.18  -2.53  12.15  49.23  #>  #> Coefficients: #>                 Estimate Std. Error t value Pr(>|t|)     #> (Intercept)      253.967      4.875   52.10  < 2e-16 *** #> length_centered    2.789      0.297    9.38  5.2e-13 *** #> dietLow           13.727      6.878    2.00  0.05091 .   #> dietMed           25.226      6.974    3.62  0.00065 *** #> dietHigh          30.784      6.833    4.51  3.5e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 18.6 on 55 degrees of freedom #> Multiple R-squared:  0.654,  Adjusted R-squared:  0.628  #> F-statistic: 25.9 on 4 and 55 DF,  p-value: 4.14e-12 lengthC <- dietdata$length_centered nd2 <- data.frame(diet=rep(c(\"Control\", \"Low\", \"Med\", \"High\"), each = 15),                   length_centered=rep(seq(min(lengthC), max(lengthC),length = 15), times = 4)) E2 <- predict(fm2, newdata=nd2, se.fit = TRUE, interval = \"confidence\") predData2 <- data.frame(E2$fit, nd2)  ggplot() +   geom_point(data = dietdata, aes(x = length_centered, y = weight, color = diet)) +   geom_line(data = predData2, aes(x = length_centered, y = fit, color = diet)) +   guides(colour= guide_legend(reverse= T)) +   scale_y_continuous(\"Weight\") +   scale_x_continuous(\"Length\") library(multcomp) summary(glht(fm2, linfct=mcp(diet=\"Tukey\"))) #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Tukey Contrasts #>  #>  #> Fit: lm(formula = weight ~ length_centered + diet, data = dietdata) #>  #> Linear Hypotheses: #>                     Estimate Std. Error t value Pr(>|t|)     #> Low - Control == 0     13.73       6.88    2.00   0.2020     #> Med - Control == 0     25.23       6.97    3.62   0.0035 **  #> High - Control == 0    30.78       6.83    4.51   <0.001 *** #> Med - Low == 0         11.50       6.83    1.68   0.3419     #> High - Low == 0        17.06       6.82    2.50   0.0709 .   #> High - Med == 0         5.56       6.87    0.81   0.8500     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"scenario-2-blocking","dir":"Articles","previous_headings":"","what":"Scenario 2: Blocking","title":"Lab 5: Multiple Regression","text":"Blocking used additional factor besides treatment interest may contributing variation response variable. blocking usually thought context ANOVA, can used linear modeling situations well. Blocks can regions, time periods, individual subjects, etc. blocking must occur design phase study. (Later course, discuss scenario interested second factor longer refer block.) can write additive Randomized Complete Block Design : $$\\large y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$ hypotheses related model?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"blocked-anova-by-hand","dir":"Articles","previous_headings":"Scenario 2: Blocking","what":"Blocked ANOVA by hand","title":"Lab 5: Multiple Regression","text":"“fit” RCBD model hand, ’ll use gypsy moth data saw lecture. dataset researcher counted number caterpillars plot treated either Bt Dimilin (left control). Additionally, treated plots located 4 geographic regions, region included block model. analyze moth data, need tell R treat Treatment Region variables factors: Right now, character integer objects, respectively. Note explicit order levels Treatment control came first.","code":"library(FANR6750) data(\"mothdata\")  head(mothdata) str(mothdata) #> tibble [12 × 3] (S3: tbl_df/tbl/data.frame) #>  $ Region   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 2 2 2 3 3 3 4 ... #>  $ Treatment: Factor w/ 3 levels \"Control\",\"Bt\",..: 1 2 3 1 2 3 1 2 3 1 ... #>  $ Larvae   : num [1:12] 25 16 14 10 3 2 15 10 16 32 ... mothdata$Treatment <- factor(mothdata$Treatment, levels = c(\"Control\", \"Bt\", \"Dimilin\")) levels(mothdata$Treatment) #> [1] \"Control\" \"Bt\"      \"Dimilin\"  mothdata$Region <- factor(mothdata$Region) levels(mothdata$Region) #> [1] \"1\" \"2\" \"3\" \"4\""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"means","dir":"Articles","previous_headings":"Scenario 2: Blocking > Blocked ANOVA by hand","what":"Means","title":"Lab 5: Multiple Regression","text":"Next, compute means:","code":"library(dplyr)  # Grand mean (grand.mean <- mean(mothdata$Larvae)) #> [1] 14.42  # Treatment means (treatment.means <- group_by(mothdata, Treatment) %>% summarise(mu = mean(Larvae))) #> # A tibble: 3 × 2 #>   Treatment    mu #>   <fct>     <dbl> #> 1 Control    20.5 #> 2 Bt         11.8 #> 3 Dimilin    11  # Block means (block.means <- group_by(mothdata, Region) %>% summarise(mu = mean(Larvae))) #> # A tibble: 4 × 2 #>   Region    mu #>   <fct>  <dbl> #> 1 1       18.3 #> 2 2        5   #> 3 3       13.7 #> 4 4       20.7"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"treatment-sums-of-squares","dir":"Articles","previous_headings":"Scenario 2: Blocking > Blocked ANOVA by hand","what":"Treatment sums of squares","title":"Lab 5: Multiple Regression","text":"formula treatment sums squares: $$\\Large b \\times \\sum_{=1}^(\\bar{y}_i - \\bar{y}_.)^2$$","code":"b <- nlevels(mothdata$Region) (SS.treat <- b * sum((treatment.means$mu - grand.mean)^2)) #> [1] 223.2"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"block-sums-of-squares","dir":"Articles","previous_headings":"Scenario 2: Blocking > Blocked ANOVA by hand","what":"Block sums of squares","title":"Lab 5: Multiple Regression","text":"formula block sums squares: $$\\Large \\times \\sum_{j=1}^b (\\bar{y}_j - \\bar{y}_.)^2$$","code":"a <- nlevels(mothdata$Treatment) (SS.block <- a * sum((block.means$mu - grand.mean)^2)) #> [1] 430.9"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"within-groups-sums-of-squares","dir":"Articles","previous_headings":"Scenario 2: Blocking > Blocked ANOVA by hand","what":"Within groups sums of squares","title":"Lab 5: Multiple Regression","text":"$$\\Large \\sum_{=1}^\\sum_{j=1}^b (y_{ij} - \\bar{y}_i - \\bar{y}_j + \\bar{y}_.)^2$$ NOTE: code work, treatment.means block.means must order original data.","code":"treatment.means.long <- rep(treatment.means$mu, times = b) block.means.long <- rep(block.means$mu, each = a) (SS.within <- sum((mothdata$Larvae - treatment.means.long - block.means.long + grand.mean)^2)) #> [1] 114.8"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"create-anova-table","dir":"Articles","previous_headings":"Scenario 2: Blocking > Blocked ANOVA by hand","what":"Create ANOVA table","title":"Lab 5: Multiple Regression","text":"Now ’re ready create ANOVA table. Start ’ve calculated far: Next, add mean squares: Now, F-values (NA error/within/residual row): Finally, p-values Quick reminder calculating p-values sure understand functions ! ’ll display table using kable(), options make look little nicer: RCBD ANOVA table calculated hand","code":"df.treat <- a - 1 df.block <- b - 1 df.within <- df.treat*df.block ANOVAtable <- data.frame(df = c(df.treat, df.block, df.within),                          SS = c(SS.treat, SS.block, SS.within)) rownames(ANOVAtable) <- c(\"Treatment\", \"Block\", \"Within\") ANOVAtable #>           df    SS #> Treatment  2 223.2 #> Block      3 430.9 #> Within     6 114.8 MSE <- ANOVAtable$SS / ANOVAtable$df ANOVAtable$MSE <- MSE ## makes a new column! F.stat <- c(MSE[1]/MSE[3], MSE[2]/MSE[3], NA) ANOVAtable$F.stat <- F.stat p <- c(1 - pf(F.stat[1], 2, 6), 1 - pf(F.stat[2], 3, 6), NA) ANOVAtable$p <- p qf(0.95, 2, 6) # 95% of the distribution is below this value of F #> [1] 5.143  1-pf(F.stat[1], 2, 6) # Proportion of the distribution beyond this F value #> [1] 0.03922 options(knitr.kable.NA = \"\") # leave NA cells empty knitr::kable(ANOVAtable, digits = 2, align = \"c\",              col.names = c(\"df\", \"SS\", \"MSE\", \"F\", \"p-value\"),              caption = \"RCBD ANOVA table calculated by hand\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"blocked-anova-using-aov","dir":"Articles","previous_headings":"Scenario 2: Blocking","what":"Blocked ANOVA using aov()","title":"Lab 5: Multiple Regression","text":"Look happens ignore blocking variable though:","code":"aov1 <- aov(Larvae ~ Treatment + Region, mothdata) summary(aov1) #>             Df Sum Sq Mean Sq F value Pr(>F)   #> Treatment    2    223   111.6    5.83  0.039 * #> Region       3    431   143.6    7.51  0.019 * #> Residuals    6    115    19.1                  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 aov2 <- aov(Larvae ~ Treatment, mothdata) summary(aov2) #>             Df Sum Sq Mean Sq F value Pr(>F) #> Treatment    2    223   111.6    1.84   0.21 #> Residuals    9    546    60.6"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"blocked-anova-using-lm","dir":"Articles","previous_headings":"Scenario 2: Blocking","what":"Blocked ANOVA using lm()","title":"Lab 5: Multiple Regression","text":"Can interpret parameter estimate output aov1 lm1? different information provided two ways constructing model? null hypotheses rejected? Failed rejected?","code":"lm1 <- lm(Larvae~ Treatment + Region, mothdata) summary(lm1) #>  #> Call: #> lm(formula = Larvae ~ Treatment + Region, data = mothdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -5.250 -1.021  0.167  0.604  5.750  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)         24.42       3.09    7.89  0.00022 *** #> TreatmentBt         -8.75       3.09   -2.83  0.03001 *   #> TreatmentDimilin    -9.50       3.09   -3.07  0.02191 *   #> Region2            -13.33       3.57   -3.73  0.00971 **  #> Region3             -4.67       3.57   -1.31  0.23924     #> Region4              2.33       3.57    0.65  0.53782     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 6 degrees of freedom #> Multiple R-squared:  0.851,  Adjusted R-squared:  0.726  #> F-statistic: 6.84 on 5 and 6 DF,  p-value: 0.0183"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab05_multiple_regression.html","id":"assignment-not-for-a-grade","dir":"Articles","previous_headings":"","what":"Assignment (not for a grade)","title":"Lab 5: Multiple Regression","text":"time ago, plantations Pinus caribaea established four locations Puerto Rico. part study, four spacing intervals used location determine effect spacing tree height. Twenty years plantations established, measurements (height ft) made study plots. Additionally, soil chemistry analyzed determine average pH nitrogen content (mg N/kg soil) tree. Note although spacing interval number, must treated factor since 4 spacing intervals (4) interest researcher. can access data using: Questions/tasks researcher like develop several models investigate variables affecting tree height plantations. done , asks submit report findings. Instructions report shown . Construct model using lm() function predicts tree height function spacing , researcher also interested seeing soil pH contributing differences tree height, include pH predictor well. may choose center continuous variable , remember affect way interpret results. Write hypotheses associated model Use equation editor show linear model use interpret term. State model conclusions context data. appear relationship tree height tree spacing. Determine tree spacings differ eachother regard effect tree height. Create publication quality plot displaying effect pH tree spacing tree height. figure descriptive caption aesthetics (color, line type, etc.) clearly defined. Construct ANOVA table hand shows relationship tree height spacing well categorical variable location. researcher particularly interested geographic location ’s sure exact mechanism, thinks may affect results. Create well formatted ANOVA table table header using available R functions. necessary include location model? changed overall conclusions effect spacing tree height included ? Construct model using lm() function predicts tree height function soil pH nitrogen content. Write hypotheses associated model Use equation editor show linear model use interpret term. Interpret parameter estimate model output Discussion: now, discussed might construct several types models used predict tree height data scenario. Later semester talk ‘model selection’ process choosing one model several somehow best represents research system. formal instruction topic yet (perhaps ) explain think model selection might done. kinds metrics might use decide model best represents data? (one right answer reasonably thought answers accepted.) may format report however like well-organized, relevant headers, plain text, elements described . always: sure output type set : output: html_document sure set echo = TRUE R chunks See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"library(FANR6750) data(\"pinedata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"lab-5","dir":"Articles","previous_headings":"","what":"Lab 5","title":"Lab 6: Interactions","text":"Multiple Regression ANCOVA Blocking Centering predictors","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 6: Interactions","text":"Interactions Definition Identifying interactions visually Types interactions Factorial Designs 2-way Factorial Designs using aov() Post-hoc analyses Visualizing 2-way Factorial Design 2-way Factorial Designs using lm()","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"scenario","dir":"Articles","previous_headings":"Interactions","what":"Scenario","title":"Lab 6: Interactions","text":"far talked models one categorical predictor (ANOVA), models one continuous predictor (simple linear regression), models categorical predictor accounting continuous predictor (ANCOVA), models two categorical predictors one interest (Blocked design). Now discuss scenarios multiple predictors interest researcher. Additionally, importantly lab, relationship /among predictors also interest. relationship mentioned referred interaction.  interaction model design terms refers phenomenon effect one predictor variable depends values one predictor variables.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"visualizing-an-interaction","dir":"Articles","previous_headings":"Interactions","what":"Visualizing an Interaction","title":"Lab 6: Interactions","text":"Figure 1: Relationship predictor variable (x) response variable (y) across two levels second predictor variable. Left figure represents interaction right figure represents presence interaction.  saw lecture, interactions can take many forms overall interpretation . interaction two continuous variables, continuous categorical variable, two categorical variables. look examples .","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"data-example-1","dir":"Articles","previous_headings":"Interactions","what":"Data Example 1","title":"Lab 6: Interactions","text":"dataset, two continuous predictor variables (moon phase percent cloud cover) used predict call rate Chuck-’s-widow. Although researchers interested variables alone, also interested determining values one predictor may alter effects . Load chuckdata dataset examine data  Notice several variables dataset recorded percentages. fact affect way interpret model output. defined now, interpretation slope means call rate change amount unit change cloud cover moon phase. Currently, one unit variables represents difference 0% 100%. intuitive us talk changes call rate percent change predictors. , can simply modify dataset. Now one unit change predictors represents one percent change.  Next, good idea plot relationship response variable predictor. Figure 2: Relationship call rate Chuck wills widow two predictor variables, cloud cover moon phase.  appear relationship call rate either predictor variables?  Let’s use linear model look relationships. First, need define linear model: yi=β0+β1x1i+β2x2i+β3x1ix2i+ϵi y_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + \\beta_3x_{1i}x_{2i} + \\epsilon_i interpret terms? ones represent main effects represents interaction?  Now ready specify model R1. interpret parameter estimates? main effects significant? interaction significant? Given results see , appropriate researcher consider main effects independently ?  Figure 3: Predicted call rate Chuck wills widow function moon phase. Solid lines represent fitted values dashed lines represent 95% confidence bands. Predictions shown two values cloud cover (0 = black, 100 = red).","code":"library(FANR6750) data('chuckdata')  str(chuckdata) #> 'data.frame':    60 obs. of  4 variables: #>  $ X        : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ call.rate: num  2.9 3.65 3.68 2.73 3.56 ... #>  $ cloud    : num  0.1185 0.9156 0.0941 0.8536 0.14 ... #>  $ moon     : num  0.241 0.56 0.56 0.855 0.24 ...  summary(chuckdata) #>        X          call.rate         cloud             moon        #>  Min.   : 1.0   Min.   :0.736   Min.   :0.0394   Min.   :0.0006   #>  1st Qu.:15.8   1st Qu.:2.840   1st Qu.:0.2242   1st Qu.:0.2200   #>  Median :30.5   Median :3.441   Median :0.4965   Median :0.4399   #>  Mean   :30.5   Mean   :3.361   Mean   :0.5171   Mean   :0.4464   #>  3rd Qu.:45.2   3rd Qu.:3.840   3rd Qu.:0.8123   3rd Qu.:0.7131   #>  Max.   :60.0   Max.   :5.594   Max.   :0.9697   Max.   :0.9930 chuckdata$cloud <- chuckdata$cloud*100 chuckdata$moon <- chuckdata$moon*100 # ```{r, fig.width=8, fig.height=4} R code chunk options for making this plot # install.packages('gridExtra') Check your package library to see if you already have this package library(gridExtra) library(ggplot2) grid.arrange(   ggplot(data= chuckdata, aes(x= cloud, y= call.rate)) +     geom_point(),      ggplot(data= chuckdata, aes(x= moon, y= call.rate)) +     geom_point(), ncol= 2) mod1 <- lm(call.rate~ moon*cloud, data= chuckdata) summary(mod1) #>  #> Call: #> lm(formula = call.rate ~ moon * cloud, data = chuckdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -2.365 -0.392  0.158  0.560  1.497  #>  #> Coefficients: #>              Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  2.582092   0.405007    6.38  3.7e-08 *** #> moon         0.027339   0.008023    3.41   0.0012 **  #> cloud        0.002281   0.006423    0.36   0.7238     #> moon:cloud  -0.000239   0.000123   -1.94   0.0573 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.833 on 56 degrees of freedom #> Multiple R-squared:  0.281,  Adjusted R-squared:  0.243  #> F-statistic: 7.31 on 3 and 56 DF,  p-value: 0.000322 nd1 <- data.frame(moon = seq(min(chuckdata$moon), max(chuckdata$moon), length = 100),                  cloud = rep(0,100)) nd2 <- data.frame(moon = seq(min(chuckdata$moon), max(chuckdata$moon), length = 100),                  cloud = rep(100,100))  E1 <- predict(mod1, newdata = nd1, se.fit = TRUE, interval=\"confidence\") predictionData1 <- data.frame(E1$fit, nd1)  E2 <- predict(mod1, newdata = nd2, se.fit = TRUE, interval=\"confidence\") predictionData2 <- data.frame(E2$fit, nd2)  colors <- c('0' = 'black', '100' = 'red') ggplot() +   geom_point(data = chuckdata, aes(x = moon, y = call.rate)) +   geom_ribbon(data = predictionData1, aes(x = moon, ymin = lwr, ymax = upr),               color = \"black\", linetype = \"longdash\", fill = NA) +   geom_path(data = predictionData1, aes(x = moon, y = fit, color= '0')) +   geom_ribbon(data= predictionData2, aes(x = moon, ymin = lwr, ymax = upr),               color = 'red', linetype = 'longdash', fill = NA) +   geom_path(data = predictionData2, aes(x = moon, y = fit, color= '100')) +    scale_color_manual(values= colors) +   labs(color= 'Cloud cover')"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"data-example-2","dir":"Articles","previous_headings":"Interactions","what":"Data example 2","title":"Lab 6: Interactions","text":"dataset contains information species microtus vole. Voles small rodents found throughout world generally feed grasses, grains, seeds. experiment, population voles introduced 24 enclosures. enclosure randomly assigned two treatments- food (three levels) predator presence (two levels). specified period time number surviving voles enclosure counted. researcher particularly interested understanding food availability affects vole population also taking account presence absence predators. dataset described can best approached particular kind ANOVA referred Factorial Design. factorial design commonly used : multiple categorical predictors interactions /among predictors interest replicates within combination treatment levels Load microtusdata dataset examine data: proceeding, need convert food predators factors: many replicates ?","code":"library(FANR6750) library(ggplot2) data(\"microtusdata\")  head(microtusdata) str(microtusdata) #> 'data.frame':    24 obs. of  3 variables: #>  $ voles    : int  10 12 8 14 18 20 21 24 20 18 ... #>  $ food     : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 2 2 2 2 3 3 ... #>  $ predators: chr  \"Present\" \"Present\" \"Present\" \"Present\" ... microtusdata$food <- factor(microtusdata$food) microtusdata$predators <- factor(microtusdata$predators) str(microtusdata) #> 'data.frame':    24 obs. of  3 variables: #>  $ voles    : int  10 12 8 14 18 20 21 24 20 18 ... #>  $ food     : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 2 2 2 2 3 3 ... #>  $ predators: Factor w/ 2 levels \"Absent\",\"Present\": 2 2 2 2 2 2 2 2 2 2 ... table(microtusdata[,c(\"predators\",\"food\")]) #>          food #> predators 0 1 2 #>   Absent  4 4 4 #>   Present 4 4 4"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"visualizing-the-data","dir":"Articles","previous_headings":"Interactions","what":"Visualizing the data","title":"Lab 6: Interactions","text":"Visualizing multiple factors can sometimes tricky. ’ll use fill aestethic make separate boxplots predation level, within food treatment: Figure 4: Number voles enclosure function food treatment levels. Colors represent predator treatment.  ’s clear figure 1) food supplementation influences vole abundance, 2) effects food treatment depend whether predators present absent.","code":"ggplot(microtusdata, aes(x = food, y = voles, fill = predators)) +   geom_boxplot() +   scale_x_discrete(\"Food treatment\") +   scale_y_continuous(\"Number of voles\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"defining-the-model","dir":"Articles","previous_headings":"Interactions","what":"Defining the model","title":"Lab 6: Interactions","text":"equation might use define linear model 2-way factorial design: yijk=μ+αi+βj+αβij+ϵijkwhereϵi∼N(0,σ) y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\epsilon_{ijk} \\ \\ \\ \\ \\ \\text{} \\ \\ \\ \\ \\ \\epsilon_i \\sim \\text{N}(0,\\sigma) Notice increased complexity subscripting. represent? αβij\\alpha\\beta_{ij} term represent? Can list relevant null alternative hypotheses?","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"factorial-design-using-aov","dir":"Articles","previous_headings":"Interactions","what":"Factorial design using aov()","title":"Lab 6: Interactions","text":"can analyze microtus data using aov() function ’re already familiar . , fit model without interaction (food + predators). results compare?","code":"aov1 <- aov(voles ~ food * predators, data = microtusdata) summary(aov1) #>                Df Sum Sq Mean Sq F value  Pr(>F)     #> food            2   1337     669    40.6 2.1e-07 *** #> predators       1    975     975    59.2 4.3e-07 *** #> food:predators  2    518     259    15.7 0.00011 *** #> Residuals      18    297      16                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"follow-ups","dir":"Articles","previous_headings":"Interactions","what":"Follow ups","title":"Lab 6: Interactions","text":"Often analysis, stop significant ANOVA result. example, maybe want explore effects one factor holding factor constant. case, can test whether food supplements influence vole abundance predators present using subset argument: predators absent: maybe want test levels different using old friend Tukey HSD test, factorial design return “main” effect differences interaction2:","code":"summary(aov(voles ~ food,              data = microtusdata,              subset = predators == \"Present\")) #>             Df Sum Sq Mean Sq F value Pr(>F)    #> food         2  193.5    96.7    14.8 0.0014 ** #> Residuals    9   58.7     6.5                   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 summary(aov(voles ~ food,              data = microtusdata,              subset = predators == \"Absent\")) #>             Df Sum Sq Mean Sq F value  Pr(>F)     #> food         2   1662     831    31.4 8.7e-05 *** #> Residuals    9    238      26                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 TukeyHSD(aov1) #>   Tukey multiple comparisons of means #>     95% family-wise confidence level #>  #> Fit: aov(formula = voles ~ food * predators, data = microtusdata) #>  #> $food #>       diff    lwr    upr  p adj #> 1-0 13.875  8.694 19.056 0.0000 #> 2-0 17.250 12.069 22.431 0.0000 #> 2-1  3.375 -1.806  8.556 0.2464 #>  #> $predators #>                  diff    lwr    upr p adj #> Present-Absent -12.75 -16.23 -9.267     0 #>  #> $`food:predators` #>                       diff      lwr     upr  p adj #> 1:Absent-0:Absent    18.00   8.8756  27.124 0.0001 #> 2:Absent-0:Absent    28.50  19.3756  37.624 0.0000 #> 0:Present-0:Absent   -2.50 -11.6244   6.624 0.9488 #> 1:Present-0:Absent    7.25  -1.8744  16.374 0.1684 #> 2:Present-0:Absent    3.50  -5.6244  12.624 0.8221 #> 2:Absent-1:Absent    10.50   1.3756  19.624 0.0189 #> 0:Present-1:Absent  -20.50 -29.6244 -11.376 0.0000 #> 1:Present-1:Absent  -10.75 -19.8744  -1.626 0.0158 #> 2:Present-1:Absent  -14.50 -23.6244  -5.376 0.0010 #> 0:Present-2:Absent  -31.00 -40.1244 -21.876 0.0000 #> 1:Present-2:Absent  -21.25 -30.3744 -12.126 0.0000 #> 2:Present-2:Absent  -25.00 -34.1244 -15.876 0.0000 #> 1:Present-0:Present   9.75   0.6256  18.874 0.0323 #> 2:Present-0:Present   6.00  -3.1244  15.124 0.3351 #> 2:Present-1:Present  -3.75 -12.8744   5.374 0.7781"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Interactions","what":"Visualizing the results","title":"Lab 6: Interactions","text":"Another follow often important visualizing results ANOVA model. case factorial design, generally involve making graph predicted response (associated uncertainty!) combination factors. create data form basis visualization, ’ll first use model.tables() function compute relevant means (.e., predicted values) standard errors: Next, ’ll extract group means confidence intervals? Notice model.tables() returns “Standard errors differences means”. Extending formulas ×BA \\times B factorial case, confidence interval difference two ×BA \\times B means : $$\\Large CI_{1−\\alpha} : \\bar{y}_{ij.} − \\bar{y}_{ij.′} \\pm t_{1−\\alpha/2,ab(n−1)}\\times \\sqrt{\\frac{2MSE}{n}}$$ n=4n = 4 vole example. set plot group means, however, need confidence interval mean. take ‘2’ SE expression: $$\\Large CI_{1−\\alpha} : \\bar{y}_{ij.} \\pm t_{1−\\alpha/2,ab(n−1)}\\times \\sqrt{\\frac{MSE}{n}}$$ can find MSE? Answer: output summary() aov1 object: Calculate width confidence intervals: Now can create data frame plot group means confidence intervals. Remember always check defined order dataframe make sure matches order group means displayed model.tables() function. Figure 5: Predicted number voles enclosure function food treatment level. Point estimates shown 95% confidence intervals. Colors represent predator treatment levels.","code":"(tab <- model.tables(aov1, type=\"means\", se = TRUE)) #> Tables of means #> Grand mean #>        #> 22.62  #>  #>  food  #> food #>     0     1     2  #> 12.25 26.12 29.50  #>  #>  predators  #> predators #>  Absent Present  #>   29.00   16.25  #>  #>  food:predators  #>     predators #> food Absent Present #>    0 13.50  11.00   #>    1 31.50  20.75   #>    2 42.00  17.00   #>  #> Standard errors for differences of means #>          food predators food:predators #>         2.030     1.658          2.871 #> replic.     8        12              4 (ybar_ij. <- tab$tables$\"food:predators\") #>     predators #> food Absent Present #>    0 13.50  11.00   #>    1 31.50  20.75   #>    2 42.00  17.00 summary(aov1) #>                Df Sum Sq Mean Sq F value  Pr(>F)     #> food            2   1337     669    40.6 2.1e-07 *** #> predators       1    975     975    59.2 4.3e-07 *** #> food:predators  2    518     259    15.7 0.00011 *** #> Residuals      18    297      16                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 MSE <- summary(aov1)[[1]]$`Mean Sq`[4] ## 16.5 ybar_ij.SE <- sqrt(MSE/4) CI.half <- qt(0.975, 18) * ybar_ij.SE (CI <- c(-CI.half, CI.half)) #> [1] -4.265  4.265 predicted.voles <- data.frame(Food = rep(c(0, 1, 2), 2),                               Predators = rep(c(\"Absent\", \"Present\"), each = 3),                               voles = c(ybar_ij.))  ## Add lower and upper confidence intervals predicted.voles <- dplyr::mutate(predicted.voles,                                   LCI = voles + CI[1],                                  UCI = voles + CI[2]) ggplot(predicted.voles, aes(x = Food, y = voles,                              color = Predators)) +   geom_path(aes(linetype = Predators)) +   geom_errorbar(aes(ymin = LCI, ymax = UCI), width = 0) +   geom_point() +   scale_y_continuous(\"Predicted number of voles\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"factorial-design-using-lm","dir":"Articles","previous_headings":"Interactions","what":"Factorial Design using lm()","title":"Lab 6: Interactions","text":"Although already completed analysis, lets also look built model using lm() function. lot going . Can interpret parameter estimates (especially interaction term)? conclude original set hypotheses related model?","code":"lm1 <- lm(voles~ food*predators, data= microtusdata) summary(lm1) #>  #> Call: #> lm(formula = voles ~ food * predators, data = microtusdata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #>  -9.50  -2.19   0.00   2.25   8.50  #>  #> Coefficients: #>                        Estimate Std. Error t value Pr(>|t|)     #> (Intercept)               13.50       2.03    6.65  3.1e-06 *** #> food1                     18.00       2.87    6.27  6.5e-06 *** #> food2                     28.50       2.87    9.93  1.0e-08 *** #> predatorsPresent          -2.50       2.87   -0.87    0.395     #> food1:predatorsPresent    -8.25       4.06   -2.03    0.057 .   #> food2:predatorsPresent   -22.50       4.06   -5.54  2.9e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.06 on 18 degrees of freedom #> Multiple R-squared:  0.905,  Adjusted R-squared:  0.879  #> F-statistic: 34.3 on 5 and 18 DF,  p-value: 1.34e-08"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab06_interactions.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 6: Interactions","text":"field fisheries, hard calcified structures otoliths fin spines frequently used age individual animals. Similar trees, structures lay annual growth rings can counted estimate age. use spines instead traditional aging structure fish, otoliths, advantage usually require killing fish. Otoliths, however, typically produce higher quality images since less susceptible damaged life fish. graduate student UGA conducting studies Lake Sturgeon (Acipenser fulvescens), large river dwelling fish Georgia found northwestern corner state. student plans use calcified structures fish better understand population age-distribution. examining growth annuli, however, student needs cut structures mount microscope slides. dataset , student processed 36 structures (spines otoliths) using 3 different mounting methods (, B, C) 3 different saw speeds (low, medium, high). mounted, time required (seconds) accurately age structure recorded fish. Questions/tasks Write equation linear model describe scenario. Define term (1 pt). Analyze dataset using ×B×CA \\times B \\times C factorial ANOVA implemented aov(). student interested 3-way interaction, set model include term (include main effects 2-way interactions variables) (1 pt). Interpret model output. main effects interactions significant? (2 pt) effect mounting method depend speed saw? , ? (1 pt) combination mounting method, saw speed, structure type resulted lowest average aging time? (0.5 pt) student decided going use fin spines future studies want use lethal method collecting otoliths. Given , combination mounting method saw speed result lowest average aging time? (0.5 pt) Put answers R Markdown report. report include: well-formatted ANOVA table (header); publication-quality plot estimated effect mounting method aging time types hard structures, including 95% confidence intervals. Rather making 3 plots (one saw speed), use predictions ‘high speed’ . figure also descriptive caption aesthetics (color, line type, etc.) clearly defined. may format report however like well-organized, relevant headers, plain text, elements described . always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"data(\"agingdata\") head(agingdata)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"lab-6","dir":"Articles","previous_headings":"","what":"Lab 6","title":"Lab 7: Evaluating assumptions of Linear Models","text":"Interactions experimental design Visualizing interactions Types interactions Factorial designs Using aov() Using lm() Follow-ups","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 7: Evaluating assumptions of Linear Models","text":"Review linear model assumptions OLS regression Gauss Markov Theory Evaluating assumptions (visual assessments tests) Linearity Normality Homoscedasticity Independence Multicollinearity Outliers influential observations","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"assumptions-of-linear-models","dir":"Articles","previous_headings":"","what":"Assumptions of linear models","title":"Lab 7: Evaluating assumptions of Linear Models","text":"using R help data analysis, keep mind , part, R happy anything tell . reality though, every linear model use semester come set assumptions. using model data analysis, need consider assumptions model decide whether reasonable make assumptions. , model may appropriate analysis; , need consider options. limited time course, part delve modeling options including nonparametric tests, GLMS, data transformation methods. Several courses campus, however, directly tackle approaches. linear models least 6 explicit assumptions. course, primarily consider first 4 assumptions, good know others (usually ) exist. go assumptions try find linear model assumptions show (subtle others). Linearity: assume relationship x y linear  Note: assumption mean need able fit straight line data. Often, actually used curved line (cases quadratic relationships). Rather, assumption means parameters enter model linearly. means one unit change predictor, ALWAYS constant change response. Let’s see examples: yi=β0+β1x1+ϵi y_i = \\beta_0 + \\beta_1x_1 + \\epsilon_i yi=β0+β1x1+β2x12+ϵi y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\epsilon_i yi=β0+β1x1+β2x2x3+ϵi y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2x_3 + \\epsilon_i yi=β0+x1β1+β2x12+ϵi y_i = \\beta_0 + x_1^{\\beta_1} + \\beta_2x_1^2 + \\epsilon_i examples linear ?  Normality: assume residuals normally distributed assumption important progress semester perform hypothesis testing assess significance predictor variables. normality residuals required, suggests model good job predicting response (.e. residuals close zero larger errors much less likely).  Homoscedasticity: assume residuals constant variance across levels values x (, refer heterscedasticity) Figure 1: Residuals linear model across range x (predictor variable). Heteroscedasticity demonstrated funnel shape.  might assumption violated? time series cross sectional studies  Independence: assume residuals independent one another independent predictors. assumption means predict size residual another residual (autocorrelation) value predictor variable (endogeneity). might assumption violated?  Unbiased estimator: assume residuals centered zero. residuals centered zero, estimate parameter good job approximating true value parameter. residuals centered zero, model either consistenly estimating parameter values (biased estimator). Figure 2: Histogram residuals linear model demonstrating biased estimator.  Note: using least squares regression, assume estimators unbiased. unbiased estimator always desirable though (even realistic [e.g. estimating λ2\\lambda^2 e−λe^{-\\lambda} poisson distribution]). general, bias variance just forms error model. One inherently worse . See depth discussion topic one founders modern probability theory. Remember total error (MSE) can calculated : MSE=Bias(β̂)2+Var(β̂) MSE = Bias(\\hat\\beta)^2 + Var(\\hat\\beta) might biased estimator advantageous?  Independent predictors: assume using multiple predictor variables, relatively independent one another. one explanatory variable able partially predicted using others (information provided variable somewhat redundant) refer multicollinearity. general, linear models able withstand great deal multicollinearity. methods, however, able deal situations predictor variable completely redundant. case, Gram matrix [X′XX'X] invertible (singular; .e. matrix algebra isn’t defined). issues might consider thinking whether multicollinearity affect model? Standard error estimates Stability parameters (inference) Issues model selection (effect penalty terms parsimony) Cost data collection Unreasonable impossible predictions (due impossible combinations predictor values)  examples multicollinearity might happen? Predicting nest survival using Fahrenheit Celsius Predicting presence birds using % land use categories Predicting movement fish using dissolved oxygen, salinity, conductivity","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"ordinary-least-squares-regression-and-the-gauss-markov-theory","dir":"Articles","previous_headings":"","what":"Ordinary Least Squares Regression and the Gauss Markov Theory","title":"Lab 7: Evaluating assumptions of Linear Models","text":"assumptions linear models can traced back method using (ordinary least squares regression) theory helps explain (Gauss Markov Theory). method OLS regression works minimizing sum squared residuals. Gauss Markov Theory says assumptions listed reasonably met, minimum can obtained straightforward way, unique, Best Linear Unbiased Estimator. Namely: β̂=(X′X)−1X′y \\hat\\beta = (X'X)^{-1}X'y formula may look somewhat daunting, remember information need form data. X matrix (variables used predictors) vector y (response variable). remains perform matrix algebra obtain vector β̂\\hat\\beta’s.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"evaluating-model-assumptions-in-r","dir":"Articles","previous_headings":"","what":"Evaluating model assumptions in R","title":"Lab 7: Evaluating assumptions of Linear Models","text":", seen many assumptions made using linear modeling tool data analysis. Now look might evaluate data see meet assumptions deviate enough need adjust methods.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"data-example","dir":"Articles","previous_headings":"Evaluating model assumptions in R","what":"Data example","title":"Lab 7: Evaluating assumptions of Linear Models","text":"Deer hunters often interested knowing variables affect deer movement rates. Presumably deer move certain conditions less others. Knowing affects change can help hunters know hunt likely profitable. dataset containing information 150 radio collared bucks. total movement (m) one day recorded deer across several tracts land Arkansas. information also collected includes day tracking conducted (either last day hunting season starts, day hunting season starts, first day hunting season ends), average temperature (F) day, average wind speed (mi/hr) day, patch size (categorical) deer resides. Let’s start loading deerdata dataset examining data. also examine structure summary dataset:  appears two variables read characters want factors. case, also makes sense manually define factor levels.  researchers set study already model mind. write conclusions though, come check model assumptions see reasonably met. surface, looks great. researchers included several predictor variables including quadratic term wind (perhaps expect happy medium wind conditions?) interaction term suggests patch size may matter certain seasons. Can interpret parameter estimates? variables ended significant model?","code":"library(FANR6750) data('deerdata') str(deerdata) #> 'data.frame':    150 obs. of  6 variables: #>  $ Deer    : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ Movement: int  6676 6290 5038 6849 7313 6725 7471 5845 9220 5138 ... #>  $ Season  : chr  \"Prehunt\" \"Prehunt\" \"Prehunt\" \"Prehunt\" ... #>  $ Temp    : int  47 56 68 54 51 47 58 64 50 52 ... #>  $ Wind    : int  12 14 12 14 20 20 13 16 17 21 ... #>  $ Patch   : chr  \"Medium\" \"Large\" \"Large\" \"Medium\" ...  summary(deerdata) #>       Deer          Movement       Season               Temp      #>  Min.   :  1.0   Min.   :1198   Length:150         Min.   :30.0   #>  1st Qu.: 38.2   1st Qu.:1636   Class :character   1st Qu.:55.0   #>  Median : 75.5   Median :3914   Mode  :character   Median :58.0   #>  Mean   : 75.5   Mean   :3952                      Mean   :58.6   #>  3rd Qu.:112.8   3rd Qu.:5806                      3rd Qu.:63.0   #>  Max.   :150.0   Max.   :9662                      Max.   :76.0   #>       Wind         Patch           #>  Min.   : 6.0   Length:150         #>  1st Qu.:13.2   Class :character   #>  Median :16.0   Mode  :character   #>  Mean   :15.9                      #>  3rd Qu.:19.0                      #>  Max.   :24.0 deerdata$Season <- factor(deerdata$Season, levels= c('Prehunt', 'Hunt', 'Posthunt')) deerdata$Patch <- factor(deerdata$Patch, levels= c('Small', 'Medium', 'Large')) options(scipen = 999) deerdata$Wind.sq <- deerdata$Wind^2 mod1 <- lm(Movement~ Temp + Wind + Wind.sq + Season + Patch + Season:Patch, data= deerdata) summary(mod1) #>  #> Call: #> lm(formula = Movement ~ Temp + Wind + Wind.sq + Season + Patch +  #>     Season:Patch, data = deerdata) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -2450.0  -531.7    63.2   566.9  2271.9  #>  #> Coefficients: #>                             Estimate Std. Error t value           Pr(>|t|)     #> (Intercept)                 9372.069   1316.724    7.12 0.0000000000547120 *** #> Temp                         -78.587     10.986   -7.15 0.0000000000453510 *** #> Wind                          43.743    128.385    0.34              0.734     #> Wind.sq                        0.354      4.038    0.09              0.930     #> SeasonHunt                 -1734.984    432.534   -4.01 0.0000985462680663 *** #> SeasonPosthunt             -3955.655    441.819   -8.95 0.0000000000000021 *** #> PatchMedium                  -87.401    415.696   -0.21              0.834     #> PatchLarge                   854.837    421.993    2.03              0.045 *   #> SeasonHunt:PatchMedium       510.248    502.542    1.02              0.312     #> SeasonPosthunt:PatchMedium     3.594    529.716    0.01              0.995     #> SeasonHunt:PatchLarge       1135.633    547.858    2.07              0.040 *   #> SeasonPosthunt:PatchLarge  -1007.949    524.112   -1.92              0.057 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 887 on 138 degrees of freedom #> Multiple R-squared:  0.851,  Adjusted R-squared:  0.839  #> F-statistic: 71.7 on 11 and 138 DF,  p-value: <0.0000000000000002"},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"residual-vs-fitted-plots","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions","what":"Residual vs fitted plots","title":"Lab 7: Evaluating assumptions of Linear Models","text":"’ll start creating plot model residuals function predicted values. can done either using base R plotting ggplot. Figure 3: Residuals across range predicted values Model 1 assumptions can examine plot? First, let’s consider linearity assumption. lecture, saw deviations linearity appear patterns residual plot. show consistently increasing decreasing values quadratic relationship. definitely appears something happening plot ordinary, smallest prediction values. One possibility sort upper bound dataset (perhaps particular factor level). Next, can consider issue heteroscedasticity. case, expect see funnel shape residuals across range predicted values (either fanning fanning ). might cause funnel? think happening ? Notice happens re-create plot change points color points represents season tracking event. Figure 4: Residuals across range predicted values (season) Model 1 seems happening ? can now see appears different relationship posthunt season. Additionally, appears much less variation season compared two seasons. see trend create boxplot movement function season. Figure 5: Residuals function tracking season Model 1","code":"ggplot(mod1, aes(x= .fitted, y= .resid)) +   geom_point() +   geom_hline(yintercept= 0) ggplot(mod1, aes(x= .fitted, y= .resid, color= deerdata$Season)) +   geom_point() +   geom_hline(yintercept= 0) ggplot(deerdata, aes(x= Season, y= Movement)) +   geom_boxplot()"},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"histograms","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions > Normality plots","what":"Histograms","title":"Lab 7: Evaluating assumptions of Linear Models","text":"Next, can check normality residuals coming model. can couple ways. One create histogram residuals. Figure 6: Histogram residuals Model 1. Vertical line represents mean residuals. histograms (degree plots made lab) notoriously subjective. Different researchers, depending kinds data accustomed working , may interpret plot differently. Thankfully, normality residuals strict assumption linear models distribution appears relatively normal. addition normality, plot can show us information bias estimator. might expect, since used unbiased estimator, mean residuals close zero ().","code":"ggplot(mod1, aes(x= .resid)) +   geom_histogram() +   geom_vline(xintercept= mean(mod1$residuals))"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"qq-plots","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions > Normality plots","what":"QQ plots","title":"Lab 7: Evaluating assumptions of Linear Models","text":"can also assess normality assumption using QQ plots. QQ plots essentially tell us whether two samples data come distribution. purposes, plot residuals (ascending order) vs theoretical order residuals truly came normal distribution. Ideally, points plot fall along center line. Deviations line can tell us great deal issues data. can find lots resources online help explain patterns might see QQ plots. One resource can found . Figure 7: Quantile-Quantile plot residuals Model 1.","code":"ggplot(mod1, aes(sample= .resid)) +   stat_qq() +   stat_qq_line() +    scale_y_continuous(\"Sample quantiles\") +   scale_x_continuous(\"Theoretical quantiles\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"independence","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions","what":"Independence","title":"Lab 7: Evaluating assumptions of Linear Models","text":"can assess independence residuals using autocorrelation function. Typically correlated residuals result either temporal (serial) autocorrelation spatial autocorrelation data points. Autocorrelated residuals manifest trends along x-axis ACF plot. might expect, spatial temporal trends dataset, see trends plot. Note: class delve time series modeling, courses campus specifically address topic. Figure 8: Autocorrelation function plot Model 1 residuals.","code":"acf(mod1$residuals, plot= T, main= '')"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"multicollinearity","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions","what":"Multicollinearity","title":"Lab 7: Evaluating assumptions of Linear Models","text":"can assess multicollinearity predictors (continuous ones!) creating covariance matrix R. numerous opinions different researchers far highly correlated predictors need consider throwing one model. two papers addressed issue extensive simulations. Figure 9: Correlation matrix predictors used Model 1.","code":"library(corrplot)  cor(deerdata[c(2,4,5)]) #>          Movement      Temp     Wind #> Movement   1.0000 -0.318801 0.134070 #> Temp      -0.3188  1.000000 0.008585 #> Wind       0.1341  0.008585 1.000000 corrplot(cor(deerdata[c(2,4,5)]), method= 'number', type = \"upper\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"getting-r-to-do-it-all","dir":"Articles","previous_headings":"Evaluating model assumptions in R > Evaluating model assumptions","what":"Getting R to do it all","title":"Lab 7: Evaluating assumptions of Linear Models","text":"made plots individually (makes easily edited), can also get R create plots .  can also formally test assumptions functions R.","code":"library(performance)  performance::check_model(mod1, check= c('linearity', 'homogeneity', 'qq', 'normality')) # Linearity library(lmtest) resettest(mod1) #>  #>  RESET test #>  #> data:  mod1 #> RESET = 9.6, df1 = 2, df2 = 136, p-value = 0.0001  # Normality shapiro.test(mod1$residuals) #>  #>  Shapiro-Wilk normality test #>  #> data:  mod1$residuals #> W = 1, p-value = 0.9  # Heteroscedasticity library(car) ncvTest(mod1, ~ Season) # Breusch-Pagan test #> Non-constant Variance Score Test  #> Variance formula: ~ Season  #> Chisquare = 20.85, Df = 2, p = 0.00003"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"outliers-and-influential-observations","dir":"Articles","previous_headings":"","what":"Outliers and influential observations","title":"Lab 7: Evaluating assumptions of Linear Models","text":"topic related discussion evaluating model assumptions issue outliers influential points. First let’s define terms:  outlier data point fit well model. points result large residuals (predicted value nowhere close actual observed value). influential observation data point whose removal dataset cause large change model fit. observation high leverage one extreme end x-axis (think fulcrum) potential pull model predictions toward (influential).  first glance, terms appear essentially defining concept. reality though, outlier may may influential model fit may may high leverage. influential point may may outlier. observation high leverage may may influential outlier. three, however, considered process model diagnostics. great plots demonstrating concepts Penn State College Science can found .  Several plots can made R help us identify outliers data points exerting high degrees leverage model. plotted studentized residuals (residuals divided sd) function leverage. Data points deviating far y-axis zero considered outliers leverage exceeding threshold (defined function data) considered high leverage. Figure 11: Studentized residuals Model 1 function leverage.  Now identified presence outliers dataset, ?  things consider first: Multiple outliers can mask effect . observation outlier one model may another model different variables included transformed. Outliers may reasonable/expected datasets others. residuals normally distributed, may common find extreme values. Individual outliers much less problem large enough dataset.  options? First, check data entry errors. discrepancy excel file paper datasheet? value code something don’t know ? Consider biological context. value impossible biologically? represent extreme potentially interesting phenomenon? Consider excluding observation see affects model. tricky subjective process. Always remember explain data management process including exclusion observations write methods.","code":"library(olsrr) ols_plot_resid_lev(mod1)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab07_linear_model_assumptions.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 7: Evaluating assumptions of Linear Models","text":"Freshwater mussels group aquatic organisms inhabit streams rivers throughout world. typically residing just surface stream substrate filter feed, mussels sometimes burrow substrate variety reasons. burrowing behavior can protect mussels environmental conditions, prevents mussels feeding time. Researchers UGA interested better understanding environmental conditions might cause mussels burrow. dataset contains information 90 juvenile mussels housed laboratory setting monitored assess burrowing behavior. Information related burrow depth (cm) collected well temperature (F) system flow (m3m^3/s) system. translate flow biologically relevant terms, researchers categorized flow represent severity drought flow represent. researchers interested burrow dynamics might vary across species, used three species mussel study design. made sure, however, individuals within particular species approximately size. task choose accept help researchers determine whether data reasonably meet linear modeling assumptions assumed model constructed. Create R Markdown file following: Create R chunk load data using: Formulate linear model R interpret results model. predictors appear significant? Explain results context biological system (1 pt). Let’s start thinking model assumptions considering outliers highly influential points. Create plot studentized residuals function leverage interpret results (0.5 pt). values appear outliers? , can identify TagID corresponds individual animal? (0.5 pt) presented problem. outlier exists dataset way knowing whether valid data point. data recorder mean type 9.9? enter ‘99’ represent missing value similar datasets might contain ‘-999’ impossible value? mussel actually burrow 99cm substrate? purposes analysis, remove outlier rerun model. Note whether changes significance predictors. (0.5 pt) Next, let’s consider Drought variable model. Although categorical variable formally assess multicollinearity, think variable redundant predictors? (0.5 pt) Create plot Depth function Flow color data points plot represent Drought condition (0.5 pt) Explain think researchers include Drought final model (0.5 pt) Remove Drought linear model analyze data . variables change significance? Explain think happened. seen result, opinion change utility including Drought model? (0.5 pt) following questions, leave Drought linear model model now appears Depth = Species + Temp + Flow. Assess linear model assumptions creating following series plots. plot, explain assumption assessed using plot conclusions (1.5 pt). Residual vs fitted plot Histogram residuals QQ plot residuals Correlation plot continuous predictors  things remember creating document: sure output type set : output: html_document sure set echo = TRUE R chunks code output visible knitted document Regularly knit document work check errors","code":"Depth = Species + Temp + Flow + Drought library(FANR6750) data('burrowdata')"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"lab-7","dir":"Articles","previous_headings":"","what":"Lab 7","title":"Lab 8: Model Selection","text":"Review linear model assumptions Evaluating model assumptions Visual assessments Formal tests Outliers influential points","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 8: Model Selection","text":"Model selection approaches Purpose –> Method Exploration Variable screening Stepwise Procedures Prediction","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"purpose-precedes-method","dir":"Articles","previous_headings":"Model Selection Approaches","what":"Purpose precedes method","title":"Lab 8: Model Selection","text":"Model selection formal process choosing model (possibly many) using defined performance criterion (several) somehow best describes system. seen lecture, many approaches available comes model selection. Importantly, however, note way go model selection directly informed purpose research study. Models used exploration used prediction. used prediction may may helpful context inference. , look several model selection approaches within context study purpose.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"exploration","dir":"Articles","previous_headings":"","what":"Exploration","title":"Lab 8: Model Selection","text":"biological system yet well understood, exploration often purpose many research studies. Exploration used help identify potential relationships variables can motivation development future hypotheses. important remember results exploratory study viewed demonstrating proof hypothesis. providing evidence potential hypothesis need future testing new data. Several model selection approaches commonly used exploratory studies. , talk two broad approaches, variable screening step-wise procedures.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"data-example","dir":"Articles","previous_headings":"Exploration","what":"Data example","title":"Lab 8: Model Selection","text":"Genetic modification allows production food crops increased usefulness people. modifications may directly related nutritional content crop, increasing digestibility, may broadly affect process crop production, decreasing time required harvest can occur. either case, knowledge relationships genetic variability phenotypic response necessary step modifying species’ genetic structure. Soybeans (Glycine max) species legume native East Asia used worldwide variety food products well feed livestock. dataset, researcher studying relationship several genetic markers flowering time soybean plants. row dataset, average flowering time 10 soybean plants particular variety recorded well parental type (H) 141 genetic markers.  Import data R examine dataset","code":"library(FANR6750) data('soybeandata')"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"variable-screening","dir":"Articles","previous_headings":"Exploration > Data example","what":"Variable screening","title":"Lab 8: Model Selection","text":"seen data set, clear many possible predictor variables like put one model. Determining include leave , however, can tedious process (especially many). begin process using variable screening. involve determining subset possible predictors strongest relationships response variable. Note: code used set analysis complex. worry much getting bogged details data management done. main principles demonstrated can seen simpler datasets well. , see first example loop course. loops allow us execute set statements iterative way can greatly reduce number code lines write. loop works iterating column genetic_matrix dataframe performing multiple tasks. First, loop conducts t-test using two levels (H) within particular genetic marker grouping variable flowering time response. loop 141 genetic markers. Next, loop stores t-test output, rounds values, assigns particular marker. , saved output loop dataframe. format data, however, convenient. information piled one column. , can take values spread matrix make easier view work . Now conducted t-tests, need way deciding genetic markers consider. case, wise somewhat liberal approach. don’t want miss potential relationships want retain markers t-test anywhere near significant. can filtering t-tests p-value < 0.1. dataframe, can see 36 genetic markers may somewhat related flowering time. can take subset original dataset, selecting columns correspond potentially relevant genetic markers. Note: reality, purpose analysis simply determine genetic markers may relationship soybean flowering time, done step. information now used genetic research goal minimizing flowering time. continue use dataset , however, demonstrate might construct parsimonious model estimating flowering time large number predictor variables.","code":"library(magicfor) library(MASS)  # Create a dataframe which contains only the genetic markers genetic_matrix <- soybeandata[,-c(1:3)]  genetic_matrix <- genetic_matrix %>% mutate_if(is.character,as.factor)  # Function which stores the results from a for loop magic_for(print, silent = TRUE)  for (i in 1:ncol(genetic_matrix)){   output1=t.test(soybeandata$Flower09 ~genetic_matrix[,i])   output2=round(c(output1$statistic,output1$stderr,output1$p.value),4)   output3=c(paste(\"marker\",i,sep=\"\"),output2)   print(output3) } # This line uses the magic_for() function to save the output of the for loop as a dataframe flower_output1=magic_result_as_dataframe() flower_output=as.data.frame(matrix(flower_output1[,2],nrow=141,byrow=TRUE))  # Creates more informative column names colnames(flower_output) <- c(\"marker\", \"t.stat\",\"Std.Error\",\"pval\")  flower_output$pval=as.numeric(as.character(flower_output$pval)) flower_marker1=filter(flower_output, flower_output$pval<0.10) head(flower_marker1) #>     marker  t.stat Std.Error   pval #> 1 marker12 -1.9443    3.3881 0.0599 #> 2 marker13 -2.4719    2.9354 0.0159 #> 3 marker14 -2.7228    2.9659 0.0081 #> 4 marker15 -2.4083    3.1559 0.0191 #> 5 marker16 -2.6763    2.9922 0.0093 #> 6 marker17 -1.7569    2.9337 0.0831 flower_data2 <- cbind(soybeandata[,c(1,2)], genetic_matrix[,which(colnames(genetic_matrix) %in%    flower_marker1$marker)])  flower_data2 <- flower_data2[-1]"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"step-wise-procedures","dir":"Articles","previous_headings":"Exploration > Data example","what":"Step-wise procedures","title":"Lab 8: Model Selection","text":"Now screened set predictor variables, can attempt construct parsimonious model. problem, however, still 36 potential predictors. decide include model? Three step-wise procedures exist (forward selection, backward elimination, stepwise regression) used resolve issue. see three action. Forward selection works starting simplest possible model (intercept model) adding one predictor variable time. computer iteratively create single variable model predictor choose predictor smallest p-value less alpha (significant). chosen first predictor, create set two variable models (always containing significant one possible predictors) select best two variable model. Larger larger models constructed either number possible predictors exhausted addition another predictor considered significant (p-value predictor greater alpha). Backward elimination works exactly opposite way. complex model specified predictors dropped one time (least significant predictor dropped first) remaining predictors model significant (p-value less alpha). Stepwise regression can thought processes . Working ends (intercept model full model) set predictors somewhere middle determined appropriate (multiple variations exactly can performed).  things consider: procedures performed (one variable added removed time) possible miss ‘optimal’ model. especially true correlated predictors inclusion removal one may affect meaningful include another. Stepwise procedures tend select smaller models might helpful prediction. Different procedures can arrive different ‘optimal’ models. case, metrics (adjusted R2R^2, root mean square error, predicted residuals sums squares, Mallow’s CP) considered.  can begin specifying intercept model full model. Next, tell R stepwise procedure perform. case, try three.  Notice differences results three approaches. predictors common across three models. AIC values differ slightly across models. tell us? Additionally, notice adjusted R2R^2 value proportion variability response explained predictors. slightly different across model, none particularly good. ?  model designed purpose exploration prediction. makes sense given many genetic markers total, handful going predict variation flowering time. model though, provide opportunity explore hypotheses future studies.","code":"# Fitting the models intercept_only <- lm(Flower09~ 1, data= flower_data2) full.model <- lm(Flower09~., data= flower_data2)   forward.model <- step(intercept_only, direction= 'forward', scope= formula(full.model))  backward.model <- step(full.model, direction= 'backward', scope= formula(full.model))  both.model <- step(intercept_only, direction= 'both', scope= formula(full.model)) summary(forward.model) #>  #> Call: #> lm(formula = Flower09 ~ marker125 + marker110 + marker139 + marker14 +  #>     marker66 + marker19 + marker92 + marker34 + marker47 + marker112 +  #>     marker117, data = flower_data2) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -22.561  -8.201  -1.903   5.712  39.848  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   65.178      2.596  25.106  < 2e-16 *** #> marker125H     7.617      2.295   3.318  0.00115 **  #> marker110H   -16.185      5.165  -3.133  0.00210 **  #> marker139H     9.909      2.815   3.520  0.00058 *** #> marker14H      6.173      2.396   2.577  0.01098 *   #> marker66H      6.281      2.162   2.905  0.00426 **  #> marker19H      6.047      2.246   2.693  0.00793 **  #> marker92H     -5.943      2.207  -2.693  0.00792 **  #> marker34H      4.931      2.549   1.935  0.05500 .   #> marker47H      4.226      2.183   1.936  0.05484 .   #> marker112H    10.321      5.403   1.910  0.05808 .   #> marker117H    -4.234      2.389  -1.772  0.07852 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 13.06 on 143 degrees of freedom #> Multiple R-squared:  0.4004, Adjusted R-squared:  0.3542  #> F-statistic:  8.68 on 11 and 143 DF,  p-value: 1.175e-11 summary(backward.model) #>  #> Call: #> lm(formula = Flower09 ~ marker15 + marker19 + marker34 + marker47 +  #>     marker66 + marker92 + marker110 + marker112 + marker116 +  #>     marker125 + marker139, data = flower_data2) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -22.618  -7.923  -2.788   6.597  40.008  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   65.309      2.599  25.126  < 2e-16 *** #> marker15H      6.219      2.469   2.519 0.012877 *   #> marker19H      5.426      2.294   2.365 0.019365 *   #> marker34H      5.363      2.558   2.097 0.037793 *   #> marker47H      4.456      2.180   2.044 0.042772 *   #> marker66H      6.306      2.177   2.896 0.004368 **  #> marker92H     -6.121      2.214  -2.764 0.006459 **  #> marker110H   -15.154      5.168  -2.932 0.003919 **  #> marker112H     9.298      5.448   1.707 0.090046 .   #> marker116H    -4.015      2.533  -1.585 0.115144     #> marker125H     7.894      2.304   3.426 0.000800 *** #> marker139H     9.550      2.832   3.372 0.000961 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 13.12 on 143 degrees of freedom #> Multiple R-squared:  0.3952, Adjusted R-squared:  0.3486  #> F-statistic: 8.493 on 11 and 143 DF,  p-value: 2.061e-11 summary(both.model) #>  #> Call: #> lm(formula = Flower09 ~ marker125 + marker110 + marker139 + marker14 +  #>     marker66 + marker19 + marker92 + marker34 + marker47 + marker112 +  #>     marker117, data = flower_data2) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -22.561  -8.201  -1.903   5.712  39.848  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   65.178      2.596  25.106  < 2e-16 *** #> marker125H     7.617      2.295   3.318  0.00115 **  #> marker110H   -16.185      5.165  -3.133  0.00210 **  #> marker139H     9.909      2.815   3.520  0.00058 *** #> marker14H      6.173      2.396   2.577  0.01098 *   #> marker66H      6.281      2.162   2.905  0.00426 **  #> marker19H      6.047      2.246   2.693  0.00793 **  #> marker92H     -5.943      2.207  -2.693  0.00792 **  #> marker34H      4.931      2.549   1.935  0.05500 .   #> marker47H      4.226      2.183   1.936  0.05484 .   #> marker112H    10.321      5.403   1.910  0.05808 .   #> marker117H    -4.234      2.389  -1.772  0.07852 .   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 13.06 on 143 degrees of freedom #> Multiple R-squared:  0.4004, Adjusted R-squared:  0.3542  #> F-statistic:  8.68 on 11 and 143 DF,  p-value: 1.175e-11  AIC(forward.model) #> [1] 1250.027 AIC(backward.model) #> [1] 1251.367 AIC(both.model) #> [1] 1250.027"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"prediction","dir":"Articles","previous_headings":"","what":"Prediction","title":"Lab 8: Model Selection","text":"Now see example can use model selection purpose prediction. , like select model performs well initial dataset, can also used create accurate predictions presented new data. One way can without two completely separate datasets using cross validation. example, use particular kind cross validation known K-fold. conceptual steps K-fold cross validation model selection follows: Randomly divide data set k number groups (preferably equal size) Fit model one groups Calculate metric MSE using observations k-th group used train model Repeat process k-number times using group Calculate overall MSE average calculated Repeat process separate model wish compare Compare metric estimated metric possible models select ‘best’ model","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"data-example-1","dir":"Articles","previous_headings":"Prediction","what":"Data example","title":"Lab 8: Model Selection","text":"Growth grasses ranches can affected many factors. example, researchers developed three models like use predict grass growth (Kg dry matter per hectare). One models contains abiotic factors (gallons water, ppm soil salinity, % soil nitrogen) another contains biotic factors (% pest cover number grazers plot) third contains .  Load data set examine data , specified three models Next, can tell R type cross validation plan use model. models appears accurately predict growth (shown R squared RMSE)? Take look resample values. different R squared k-number folds even though iteration using predictor variables? estimated R squared value cross validation models lower ran model summaries ?","code":"data('grasslanddata') mod1 <- lm(KgDMHA~ Water + Salinity + Nitrogen, data= grasslanddata) summary(mod1) #>  #> Call: #> lm(formula = KgDMHA ~ Water + Salinity + Nitrogen, data = grasslanddata) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -2991.99  -652.64    27.78   654.49  3105.87  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  4.373e+02  3.162e+02   1.383    0.167     #> Water        7.154e-03  3.805e-04  18.802   <2e-16 *** #> Salinity    -1.189e+02  9.003e+00 -13.207   <2e-16 *** #> Nitrogen     4.556e+02  3.969e+01  11.479   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 958.3 on 996 degrees of freedom #> Multiple R-squared:  0.395,  Adjusted R-squared:  0.3931  #> F-statistic: 216.7 on 3 and 996 DF,  p-value: < 2.2e-16  mod2 <- lm(KgDMHA~ Pests + Graze, data= grasslanddata) summary(mod2) #>  #> Call: #> lm(formula = KgDMHA ~ Pests + Graze, data = grasslanddata) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -2659.53  -571.16     4.26   601.87  2453.67  #>  #> Coefficients: #>              Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 10115.034    134.935   74.96   <2e-16 *** #> Pests         -40.570      1.964  -20.66   <2e-16 *** #> Graze         -57.924      2.349  -24.66   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 873 on 997 degrees of freedom #> Multiple R-squared:  0.4974, Adjusted R-squared:  0.4964  #> F-statistic: 493.4 on 2 and 997 DF,  p-value: < 2.2e-16  mod3 <- lm(KgDMHA~ Water + Salinity + Nitrogen + Pests + Graze, data= grasslanddata) summary(mod3) #>  #> Call: #> lm(formula = KgDMHA ~ Water + Salinity + Nitrogen + Pests + Graze,  #>     data = grasslanddata) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -829.03 -181.67    0.55  172.43  839.83  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  4.076e+03  9.471e+01   43.04   <2e-16 *** #> Water        7.921e-03  1.067e-04   74.27   <2e-16 *** #> Salinity    -1.161e+02  2.518e+00  -46.09   <2e-16 *** #> Nitrogen     5.087e+02  1.113e+01   45.72   <2e-16 *** #> Pests       -4.224e+01  6.037e-01  -69.96   <2e-16 *** #> Graze       -6.237e+01  7.238e-01  -86.17   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 268 on 994 degrees of freedom #> Multiple R-squared:  0.9528, Adjusted R-squared:  0.9525  #> F-statistic:  4010 on 5 and 994 DF,  p-value: < 2.2e-16 library(caret) library(randomForest) ctrl <- trainControl(method= 'cv', number= 10)  abiotic <- train(KgDMHA~ Water + Salinity + Nitrogen, data= grasslanddata, trControl= ctrl) #> note: only 2 unique complexity parameters in default grid. Truncating the grid to 2 .  print(abiotic) #> Random Forest  #>  #> 1000 samples #>    3 predictor #>  #> No pre-processing #> Resampling: Cross-Validated (10 fold)  #> Summary of sample sizes: 900, 900, 900, 900, 900, 900, ...  #> Resampling results across tuning parameters: #>  #>   mtry  RMSE      Rsquared   MAE      #>   2     1018.171  0.3251206  820.6387 #>   3     1022.410  0.3226034  824.3181 #>  #> RMSE was used to select the optimal model using the smallest value. #> The final value used for the model was mtry = 2.  abiotic$resample #>         RMSE  Rsquared      MAE Resample #> 1  1009.4179 0.2465991 812.2030   Fold01 #> 2   877.5759 0.4966374 697.4059   Fold02 #> 3  1002.9494 0.3123534 812.6133   Fold03 #> 4  1091.2507 0.2147421 861.1355   Fold04 #> 5  1073.5266 0.2407776 896.8341   Fold05 #> 6  1073.3304 0.3580076 873.4490   Fold06 #> 7  1054.7730 0.3499639 835.0267   Fold07 #> 8   929.4439 0.3425210 751.5689   Fold08 #> 9  1031.0531 0.3837676 822.6599   Fold09 #> 10 1038.3843 0.3058360 843.4903   Fold10  biotic <- train(KgDMHA~ Pests + Graze, data= grasslanddata, trControl= ctrl) #> note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .  print(biotic) #> Random Forest  #>  #> 1000 samples #>    2 predictor #>  #> No pre-processing #> Resampling: Cross-Validated (10 fold)  #> Summary of sample sizes: 900, 900, 900, 900, 900, 900, ...  #> Resampling results: #>  #>   RMSE      Rsquared   MAE      #>   974.6648  0.3958953  776.4952 #>  #> Tuning parameter 'mtry' was held constant at a value of 2  both <- train(KgDMHA~ Water + Salinity + Nitrogen + Pests + Graze, data= grasslanddata, trControl= ctrl)  print(both) #> Random Forest  #>  #> 1000 samples #>    5 predictor #>  #> No pre-processing #> Resampling: Cross-Validated (10 fold)  #> Summary of sample sizes: 900, 900, 900, 900, 900, 900, ...  #> Resampling results across tuning parameters: #>  #>   mtry  RMSE      Rsquared   MAE      #>   2     467.3569  0.8910970  365.6314 #>   3     463.9885  0.8832841  362.1499 #>   5     466.7486  0.8754120  364.6875 #>  #> RMSE was used to select the optimal model using the smallest value. #> The final value used for the model was mtry = 3."},{"path":"http://rushinglab.github.io/FANR6750/articles/lab08_model_selection.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 8: Model Selection","text":"Create R Markdown file following: Create R chunk load data using: Use data available fit 5 linear models predict jay abundance. model include least one interaction least one model quadratic term (1 pt). Compare AIC values model determine model appears best (1 pt). Construct AIC table hand using equations found lecture notes (2 pt). Use equation AIC = -2log-likelihood + 2k (log-likelihood can calculated using R function logLik()) Determine Akaike weight best performing model explain weight means comparison second best performing model Perform k-fold cross validation top two performing models (2 pt). Use 5 folds Explain think number folds affects process? think might choose use fewer folds?  things remember creating document: sure output type set : output: html_document sure set echo = TRUE R chunks code output visible knitted document Regularly knit document work check errors","code":"library(FANR6750) data('jaydata')"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"lab-8","dir":"Articles","previous_headings":"","what":"Lab 8","title":"Lab 9: Nested ANOVA","text":"Model selection Exploration Prediction","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 9: Nested ANOVA","text":"Nested ANOVA Specifying model Hypotheses Random effects lme() function","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"scenario","dir":"Articles","previous_headings":"","what":"Scenario","title":"Lab 9: Nested ANOVA","text":"Remember lecture nested designs useful situations subsample within experimental unit ’re interested treatment effects experimental (whole) unit level, subunit level. example: count larvae multiple subplots within plot weigh multiple chicks brood measure shrub growth multiple subplots within plot","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"the-additive-model","dir":"Articles","previous_headings":"Scenario","what":"The additive model","title":"Lab 9: Nested ANOVA","text":"$$\\Large y_{ijk} = \\mu + \\alpha_i + \\beta_{ij} + \\epsilon_{ijk}$$ Can define terms linear model? need many subscripts?  $$\\Large \\beta_{ij} \\sim normal(0, \\sigma^2_B)$$ always, $$\\Large \\epsilon_{ijk} \\sim normal(0, \\sigma^2)$$","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"hypotheses","dir":"Articles","previous_headings":"Scenario","what":"Hypotheses","title":"Lab 9: Nested ANOVA","text":"Treatment effects: $$\\Large H_0 : \\alpha_1 = · · · = \\alpha_a = 0$$ $$\\Large H_a : \\; least\\; one\\; inequality$$ Random variation among experimental units: $$\\Large H_0 : \\sigma^2_B = 0$$ $$\\Large H_a : \\sigma^2_B > 0$$ Note: referred variance term σB2\\sigma^2_B use reasonable subscript refer variance term random effect. Note: Recall nested ANOVA primarily used variation among experimental units interest. variation interest reason believe experimental units differ eachother, can simplify one-way ANOVA.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"example-data","dir":"Articles","previous_headings":"Scenario","what":"Example data","title":"Lab 9: Nested ANOVA","text":"lab, ’ll use another data set response gypsy moth larvae pesticide treatments: , ’ll need convert Plot Treatment factors: Note explicitly define levels Treatment control treated baseline level plotted first. don’t need Plot since default numeric order fine. see level replication within treatment plot, let’s cross-tabulate data. good idea check since unbalanced designs generally require additionl thought. Notice plots labeled! according j∈1,2,3j \\{1, 2, 3}, even though three plots within treatment (four replicates within plot). Rather, plot labeled independently, 1–9. important?","code":"library(FANR6750) data(\"mothdata2\") str(mothdata2) #> 'data.frame':    36 obs. of  3 variables: #>  $ larvae   : num  16 16 15.8 14.2 13.9 14.2 13.5 13.4 14 13.1 ... #>  $ Treatment: chr  \"Bt\" \"Bt\" \"Bt\" \"Bt\" ... #>  $ Plot     : int  1 1 1 1 2 2 2 2 3 3 ... mothdata2$Treatment <- factor(mothdata2$Treatment,                                levels = c(\"Control\", \"Bt\", \"Dimilin\"))  mothdata2$Plot <- factor(mothdata2$Plot) table(mothdata2$Treatment, mothdata2$Plot) #>           #>           1 2 3 4 5 6 7 8 9 #>   Control 0 0 0 4 4 4 0 0 0 #>   Bt      4 4 4 0 0 0 0 0 0 #>   Dimilin 0 0 0 0 0 0 4 4 4"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"analysis-using-aov","dir":"Articles","previous_headings":"Scenario","what":"Analysis using aov()","title":"Lab 9: Nested ANOVA","text":"Based models ’ve fit previously semester, might instinctively analyze data using following: , denominator degrees freedom incorrect. Instead, use: Can spot difference? calculations changed two analyses?  Note: next code chunks necessary. simply demonstrating exactly performing one way ANOVA averaging across subunits. happens analyze plot-level means? aggregate() function similar tapply() works entire data frames. get averages whole plot: use aov() analyze aggregated data, get F p :","code":"aov.wrong <- aov(larvae ~ Treatment + Plot, data = mothdata2) summary(aov.wrong) #>             Df Sum Sq Mean Sq F value Pr(>F)     #> Treatment    2  215.4   107.7  208.89 <2e-16 *** #> Plot         6   11.2     1.9    3.61 0.0093 **  #> Residuals   27   13.9     0.5                    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 aov.correct <- aov(larvae ~ Treatment + Error(Plot), data = mothdata2) summary(aov.correct) #>  #> Error: Plot #>           Df Sum Sq Mean Sq F value  Pr(>F)     #> Treatment  2  215.4   107.7    57.9 0.00012 *** #> Residuals  6   11.2     1.9                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Error: Within #>           Df Sum Sq Mean Sq F value Pr(>F) #> Residuals 27   13.9   0.516 plotData <- aggregate(larvae ~ Treatment + Plot,                        data = mothdata2, FUN = mean) plotData #>   Treatment Plot larvae #> 1        Bt    1  15.50 #> 2        Bt    2  13.75 #> 3        Bt    3  14.00 #> 4   Control    4  18.25 #> 5   Control    5  18.75 #> 6   Control    6  19.25 #> 7   Dimilin    7  12.50 #> 8   Dimilin    8  13.50 #> 9   Dimilin    9  13.00 aov.plot <- aov(larvae ~ Treatment, data = plotData) summary(aov.plot) #>             Df Sum Sq Mean Sq F value  Pr(>F)     #> Treatment    2   53.8   26.92    57.9 0.00012 *** #> Residuals    6    2.8    0.47                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"issues-with-analyzing-nested-data","dir":"Articles","previous_headings":"Scenario","what":"Issues with analyzing nested data","title":"Lab 9: Nested ANOVA","text":"using using aov() Error term: can’t use TukeyHSD() don’t get direct estimate σB2\\sigma^2_B Doesn’t handle unbalanced designs well , can use model.tables()  alternative use lme() function nlme package (designed mixed effects models) Possible get direct estimates σB2\\sigma^2_B variance parameters Handles complex models unbalanced designs Possible multiple comparisons contrasts using glht() function multcomp package … works random effects ANOVA tables aren’t complete aov()","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"using-the-lme-function","dir":"Articles","previous_headings":"Scenario","what":"Using the lme() function","title":"Lab 9: Nested ANOVA","text":"see lme() function works let’s fit moth data model using approach: syntax random effects can little confusing first see ?lme additional information. Now can view ANOVA table model using: ’ll notice simplified version ANOVA table. provides us one line relates treatment. can view variance parameter estimates: first row shows estimates σB2\\sigma^2_B σB\\sigma_B. redundant values since one can calculated . second row shows estimates σ2\\sigma^2 σ\\sigma.  can say amount random variation within whole units relative among whole units (accounting treatment effects)? appear scenario random effect plot necessary? Note reject null hypothesis related variance (σB2=0\\sigma^2_B = 0) F-statistic (MSBMSE\\frac{MS_B}{MS_E}) > Fa(b−1),ab(n−1),αF_{(b-1), ab(n-1),\\alpha}. Keep mind though get F-statistic, need use aov() function approach.  Next, can extract plot-level random effects. βij\\beta_{ij}’s: finally, multiple comparisons:","code":"library(nlme) library(multcomp)  lme1 <- lme(larvae ~ Treatment, random = ~1|Plot, data = mothdata2) anova(lme1, Terms = \"Treatment\") #> F-test for: Treatment  #>   numDF denDF F-value p-value #> 1     2     6   57.87   1e-04 VarCorr(lme1) #> Plot = pdLogChol(1)  #>             Variance StdDev #> (Intercept) 0.3364   0.580  #> Residual    0.5156   0.718 round(ranef(lme1), 2) #>   (Intercept) #> 1        0.78 #> 2       -0.48 #> 3       -0.30 #> 4       -0.36 #> 5        0.00 #> 6        0.36 #> 7       -0.36 #> 8        0.36 #> 9        0.00 tuk <- glht(lme1, linfct = mcp(Treatment=\"Tukey\")) summary(tuk) #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Tukey Contrasts #>  #>  #> Fit: lme.formula(fixed = larvae ~ Treatment, data = mothdata2, random = ~1 |  #>     Plot) #>  #> Linear Hypotheses: #>                        Estimate Std. Error z value Pr(>|z|)     #> Bt - Control == 0        -4.333      0.557   -7.78   <0.001 *** #> Dimilin - Control == 0   -5.750      0.557  -10.32   <0.001 *** #> Dimilin - Bt == 0        -1.417      0.557   -2.54     0.03 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method) confint(tuk) #>  #>   Simultaneous Confidence Intervals #>  #> Multiple Comparisons of Means: Tukey Contrasts #>  #>  #> Fit: lme.formula(fixed = larvae ~ Treatment, data = mothdata2, random = ~1 |  #>     Plot) #>  #> Quantile = 2.343 #> 95% family-wise confidence level #>   #>  #> Linear Hypotheses: #>                        Estimate lwr    upr    #> Bt - Control == 0      -4.333   -5.638 -3.029 #> Dimilin - Control == 0 -5.750   -7.055 -4.445 #> Dimilin - Bt == 0      -1.417   -2.721 -0.112"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab09_nested.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 9: Nested ANOVA","text":"determine salinity affects fish reproductive performance, researcher places one pregnant female tank one three salinity levels: low, medium, high, control tank. week birth, two offspring (fry) measured. data can accessed using: Perform appropriate analysis using aov() lme() dataset. Answer following questions: null alternative hypotheses? salinity affect fry growth? , salinity levels differ? random variation among within experimental units? think important include ‘adult’ random effect analysis simplified one-way ANOVA? Calculate F-statistic critical value determine whether reject null hypothesis related variance term. Include well formatted ANOVA table (header) aov() results. may format report however like well-organized, relevant headers, plain text, elements described . always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output Please upload html .Rmd files submit assignment See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"library(FANR6750) data(\"salinitydata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"lab","dir":"Articles","previous_headings":"","what":"Lab","title":"Lab 10: Split plot designs","text":"Nested ANOVA Random effects Using lme() aov()","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 10: Split plot designs","text":"Split Plot Design additive model Using aov() lme() Exploring interactions Multiple comparisons","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"scenario","dir":"Articles","previous_headings":"","what":"Scenario","title":"Lab 10: Split plot designs","text":"Remember lecture split-plot designs used apply treatments two levels experimental units: whole-units sub-units. can conceptualize two blocked designs, one nested inside . example Ag fields sprayed herbicides, fertilizers applied plots within fields Tenderizer applied roasts, cooking times applied cores cases, ’re interested treatment effects levels","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"the-additive-model","dir":"Articles","previous_headings":"","what":"The additive model","title":"Lab 10: Split plot designs","text":"can write split plot model : $$\\Large y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha \\beta_{ij} + \\gamma_k + \\delta_{ik} + \\epsilon_{ijk}$$ : =1,...,ai = 1,...,: whole unit treatment levels j=1,...,bj = 1,...,b: sub unit treatment levels k=1,...,ck = 1,...,c: number blocks  α\\alpha’s β\\beta’s fixed treatment effects αi\\alpha_i represents treatment applied whole unit βj\\beta_j represents treatment applied sub unit. Note interaction. want inferences apply whole units (e.g., roasts), δik\\delta_{ik} random. Specifically: $$\\Large \\delta_{ik} \\sim normal(0, \\sigma^2_R)$$ might view block (e.g. carcass) random : $$\\Large \\gamma_k \\sim normal(0, \\sigma^2_C)$$ always, $$\\Large \\epsilon_{ijk} \\sim normal(0, \\sigma^2)$$","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"data-example","dir":"Articles","previous_headings":"","what":"Data example","title":"Lab 10: Split plot designs","text":"classic dataset comes food science study. Six beef carcasses obtained random meat packaging plant. section carcass, three rolled roasts prepared nearly alike possible. roasts assigned random one three tenderizing treatments particular interest: control, vinegar marinade, papain marinade treatment, coring device used make four cores meat near center roast. cores left place three roasts carcass placed together oven allowed cook. 30 minutes, one cores taken random roast, another drawn randomly 36 minutes, third 42 minutes, final 48 minutes cooling, cores measured tenderness using Warner-Bratzler device. response variable WB score core. ’ve done examples, need convert predictor factor (case, converting factors necessary use mcp() function, later analysis):  mean treat time factor? interpret results model treated time continuous predictor vs factor?  Notice roast column enumerate tenderizer × carcass combinations (explains corresponding term additive model δik\\delta_{ik}):","code":"library(FANR6750) data(\"meatdata\") str(meatdata) #> 'data.frame':    72 obs. of  5 variables: #>  $ Wbscore   : num  8.25 7.5 4.25 3.5 7.25 6.25 3.5 3.5 6.5 4.5 ... #>  $ tenderizer: chr  \"C\" \"C\" \"C\" \"C\" ... #>  $ time      : int  30 36 42 48 30 36 42 48 30 36 ... #>  $ carcass   : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ roast     : int  1 1 1 1 2 2 2 2 3 3 ... summary(meatdata) #>     Wbscore      tenderizer             time         carcass        roast      #>  Min.   :2.00   Length:72          Min.   :30.0   Min.   :1.0   Min.   : 1.0   #>  1st Qu.:3.25   Class :character   1st Qu.:34.5   1st Qu.:2.0   1st Qu.: 5.0   #>  Median :4.25   Mode  :character   Median :39.0   Median :3.5   Median : 9.5   #>  Mean   :4.78                      Mean   :39.0   Mean   :3.5   Mean   : 9.5   #>  3rd Qu.:6.25                      3rd Qu.:43.5   3rd Qu.:5.0   3rd Qu.:14.0   #>  Max.   :8.25                      Max.   :48.0   Max.   :6.0   Max.   :18.0 meatdata$time <- factor(meatdata$time) meatdata$carcass <- factor(meatdata$carcass) meatdata$roast <- factor(meatdata$roast) meatdata$tenderizer <- factor(meatdata$tenderizer) head(meatdata, n=12) #>    Wbscore tenderizer time carcass roast #> 1     8.25          C   30       1     1 #> 2     7.50          C   36       1     1 #> 3     4.25          C   42       1     1 #> 4     3.50          C   48       1     1 #> 5     7.25          V   30       1     2 #> 6     6.25          V   36       1     2 #> 7     3.50          V   42       1     2 #> 8     3.50          V   48       1     2 #> 9     6.50          P   30       1     3 #> 10    4.50          P   36       1     3 #> 11    3.50          P   42       1     3 #> 12    2.50          P   48       1     3"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"using-aov","dir":"Articles","previous_headings":"Data example","what":"Using aov()","title":"Lab 10: Split plot designs","text":"one Error() term allowed aov(), can treat block effect fixed: Notice ANOVA table split whole unit error sub unit error. effects table tell us ? explain?","code":"aov.meat <- aov(Wbscore ~ tenderizer + time + tenderizer:time + carcass + Error(roast), data = meatdata) summary(aov.meat) #>  #> Error: roast #>            Df Sum Sq Mean Sq F value  Pr(>F)     #> tenderizer  2  20.72   10.36   190.0 1.1e-08 *** #> carcass     5   3.90    0.78    14.3 0.00028 *** #> Residuals  10   0.55    0.05                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Error: Within #>                 Df Sum Sq Mean Sq F value  Pr(>F)     #> time             3  170.1    56.7   656.6 < 2e-16 *** #> tenderizer:time  6    9.6     1.6    18.5 1.1e-10 *** #> Residuals       45    3.9     0.1                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"using-lme","dir":"Articles","previous_headings":"Data example","what":"Using lme()","title":"Lab 10: Split plot designs","text":"Using lme() function, able multiple random effects model. notation looks slightly different able essentially replicate results aov().  first row ANOVA table (Intercept) term, can ignore (hence [-1,] indexing operation):","code":"library(nlme) lme.meat <- lme(Wbscore ~ tenderizer * time,                 data = meatdata,                 correlation = corCompSymm(), # To make results same as aov()                 random = ~1|carcass/roast) anova(lme.meat)[-1,] #>                 numDF denDF F-value p-value #> tenderizer          2    10   190.0  <.0001 #> time                3    45   656.6  <.0001 #> tenderizer:time     6    45    18.5  <.0001"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"exploring-the-interaction","dir":"Articles","previous_headings":"Data example","what":"Exploring the interaction","title":"Lab 10: Split plot designs","text":"time effect significant level tenderizer? Similar labs ago, can create subsetted ANOVAs look effect one variable across multiple levels another. Yes .","code":"lme.meatP <- lme(Wbscore ~ time, data = meatdata,                  random = ~1|carcass/roast,                   correlation = corCompSymm(),                  subset = tenderizer==\"P\") anova(lme.meatP, Terms = \"time\") #> F-test for: time  #>   numDF denDF F-value p-value #> 1     3    15   126.7  <.0001 lme.meatV <- lme(Wbscore ~ time, data = meatdata,                  random = ~1|carcass/roast,                   correlation = corCompSymm(),                  subset = tenderizer==\"V\") anova(lme.meatV, Terms = \"time\") #> F-test for: time  #>   numDF denDF F-value p-value #> 1     3    15   274.7  <.0001 lme.meatC <- lme(Wbscore ~ time, data = meatdata,                   random = ~1|carcass/roast,                   correlation = corCompSymm(),                  subset = tenderizer==\"C\") anova(lme.meatC, Terms = \"time\") #> F-test for: time  #>   numDF denDF F-value p-value #> 1     3    15   305.7  <.0001"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"multiple-comparisons-using-glht-package-multcomp","dir":"Articles","previous_headings":"Data example","what":"Multiple comparisons using glht() (package multcomp)","title":"Lab 10: Split plot designs","text":"can also easily plot differences cooking times “P” tenderizer (2 levels tenderizer well)","code":"library(multcomp) mcP <- glht(lme.meatP, linfct = mcp(time=\"Tukey\")) summary(mcP) #>  #>   Simultaneous Tests for General Linear Hypotheses #>  #> Multiple Comparisons of Means: Tukey Contrasts #>  #>  #> Fit: lme.formula(fixed = Wbscore ~ time, data = meatdata, random = ~1 |  #>     carcass/roast, correlation = corCompSymm(), subset = tenderizer ==  #>     \"P\") #>  #> Linear Hypotheses: #>              Estimate Std. Error z value Pr(>|z|)     #> 36 - 30 == 0    -1.25       0.18   -6.94   <1e-07 *** #> 42 - 30 == 0    -2.33       0.18  -12.96   <1e-07 *** #> 48 - 30 == 0    -3.33       0.18  -18.52   <1e-07 *** #> 42 - 36 == 0    -1.08       0.18   -6.02   <1e-07 *** #> 48 - 36 == 0    -2.08       0.18  -11.57   <1e-07 *** #> 48 - 42 == 0    -1.00       0.18   -5.55   <1e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #> (Adjusted p values reported -- single-step method) plot(mcP)"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab10_split_plot.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 10: Split plot designs","text":"Nested--Crossed Design similar Split-Plot without block. study sweet potato yield measured response (=3a=3) types herbicide. herbicide applied 5 fields. field divided 4 plots plot treated one (b=4b=4) fertilizers. data can accessed using: Conduct appropriate ANOVA using aov() lme() functions (1 pt). Give appropriate additive model (slightly different one split-plot design, since blocking) define term. associated null alternative hypotheses? (1 pt) effect herbicide depend fertilizer? (1 pt) Use Tukey’s test determine fertilizers differ (1 pt) Put answers R Markdown report. report include: well-formatted ANOVA table (header) (1 pt); publication-quality plot (plots) estimated effect herbicide fertilizer yield. talked class estimating SE type study, can simply create plot point estimates without error bars. figure also descriptive caption aesthetics (color, line type, etc.) clearly defined (1 pt) may format report however like well-organized, relevant headers, plain text, elements described . always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output","code":"library(FANR6750) data(\"yielddata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"lab-10","dir":"Articles","previous_headings":"","what":"Lab 10","title":"Lab 11: Repeated measures","text":"Split Plot additive model Using aov() lme() Exploring interactions Multiple comparisons","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 11: Repeated measures","text":"Repeated Measures additive model Univariate approach Adjusted p-values MANOVA Multivariate approach Profile analysis Linear mixed effects","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Lab 11: Repeated measures","text":"Experiments use “repeated measures” following components: randomly assign ‘subject’ treatment record response treatment time  cases, need think several sources variation model: Treatment Time Treatment-time interaction Random variation among subjects Random variation within subjects","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"approaches","dir":"Articles","previous_headings":"","what":"Approaches","title":"Lab 11: Repeated measures","text":"saw lecture, several ways analyze repeated measures data.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"univariate-approach","dir":"Articles","previous_headings":"Approaches","what":"Univariate approach","title":"Lab 11: Repeated measures","text":"One way view repeated measures design similar split-plot design (nested crossed) know residuals correlated, need adjust p-values (Greenhouse-Geisser Huynh-Feldt methods) R, somewhat counter-intuitively, must conduct multivariate analysis variance (MANOVA) obtain adjusted p-values","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"multivariate-approach-intervals","dir":"Articles","previous_headings":"Approaches","what":"Multivariate approach (intervals)","title":"Lab 11: Repeated measures","text":"Another way look repeated measures data perform MANOVA differences allow us determine time intervals treatment effect (profile analysis)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"mixed-effects-model-with-ar-correlation-structure","dir":"Articles","previous_headings":"Approaches","what":"Mixed effects model with (AR) correlation structure","title":"Lab 11: Repeated measures","text":"can done using lme() Involves accounting serial autocorrelation directly model Preferred unbalanced designs complex models Multiple courses campus explore topics","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"univariate-approach-1","dir":"Articles","previous_headings":"","what":"Univariate Approach","title":"Lab 11: Repeated measures","text":"dataset contains information leaf growth 10 plants subjected different fertilizer treatments. Leaf growth plant measured weekly 5 weeks. begining analysis, want convert variables factors. always, helps visualize data: Figure 1: Leaf growth 10 plants subjected two fertilizer treatments across five weeks","code":"library(FANR6750) data(\"plantdata\") plantdata$plant <- factor(plantdata$plant) plantdata$fertilizer <- factor(plantdata$fertilizer) plantdata$week <- factor(plantdata$week)  str(plantdata) #> 'data.frame':    50 obs. of  4 variables: #>  $ plant     : Factor w/ 10 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 2 2 2 2 2 ... #>  $ fertilizer: Factor w/ 2 levels \"H\",\"L\": 2 2 2 2 2 2 2 2 2 2 ... #>  $ week      : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 1 2 3 4 5 ... #>  $ leaves    : int  4 5 6 8 10 3 4 6 6 9 ... library(ggplot2)  ggplot(plantdata, aes(x = week, y = leaves,                        group = plant,                        color = fertilizer)) +   geom_path() +   scale_x_discrete(\"Week\") +   scale_y_continuous(\"Number of leaves\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"additive-model","dir":"Articles","previous_headings":"Univariate Approach","what":"Additive model","title":"Lab 11: Repeated measures","text":"yijk=μ+αi+βj+αβij+δik+ϵijk y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\alpha\\beta_{ij} + \\delta_{ik} + \\epsilon_{ijk} : μ\\mu: Grand meanαi\\alpha_i: effect ith treatment levelβj\\beta_j: effect jth time levelαβij\\alpha\\beta_{ij}: interaction effect treatment time levelsδik\\delta_{ik}: effect kth subject receiving ith treatment levelϵijk\\epsilon_{ijk}: residual error  hypotheses related model? effects fixed random?  Now let’s fit univariate model:  Although output looks ok, remember need adjust p-values time interaction effects. ? R, requires reformatting data conducting multivariate analysis variance (MANOVA). Specifically, need covert data “long” format currently (measurement plant contained row) “wide” format (plant one row, separate columns weekly measurement). number ways convert long wide formats R ’ll use pivot_wider() function tidyr package1:","code":"aov1 <- aov(leaves ~ fertilizer * week + Error(plant),             data = plantdata) summary(aov1) #>  #> Error: plant #>            Df Sum Sq Mean Sq F value Pr(>F) #> fertilizer  1   16.8   16.82     2.6   0.15 #> Residuals   8   51.7    6.46                #>  #> Error: Within #>                 Df Sum Sq Mean Sq F value Pr(>F)     #> week             4  267.4    66.9  158.22 <2e-16 *** #> fertilizer:week  4    5.1     1.3    3.01  0.033 *   #> Residuals       32   13.5     0.4                    #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 plantdata_wide <- tidyr::pivot_wider(data = plantdata,                                       names_from = week,                                       names_prefix = \"week\",                                      values_from = leaves) head(plantdata_wide) #> # A tibble: 6 × 7 #>   plant fertilizer week1 week2 week3 week4 week5 #>   <fct> <fct>      <int> <int> <int> <int> <int> #> 1 1     L              4     5     6     8    10 #> 2 2     L              3     4     6     6     9 #> 3 3     L              6     7     9    10    12 #> 4 4     L              5     7     8    10    12 #> 5 5     L              5     6     7     8    10 #> 6 6     H              4     6     9     9    11"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"a-brief-aside-about-manova","dir":"Articles","previous_headings":"Univariate Approach","what":"A brief aside about MANOVA","title":"Lab 11: Repeated measures","text":"everything done point (everything later course), considered one response variable time model (univariate models). classes models exist, however, using one (multivariate models). Multivariate models involve estimating multiple response variables simultaneously.  Remember way back Lecture 2 discussed basic form linear models. said : yi∼N(μ,σ2) y_i \\sim N(\\mu, \\sigma^2) means response variable (yi)(y_i) normally distributed mean μ\\mu variance σ2\\sigma^2.  Now one response variable, need modify statement somewhat: Yij=Ni(μi,Σ) Y_{ij} = N_i(\\mu_i, \\Sigma) modified statement represent just one, several () response variables estimating (j) number observations. result, just one expected value, vector expected values (one response variable). Additionally, response variable variance term longer consider σ2\\sigma^2 instead consider matrix Σ\\Sigma contains variance covariances response variable. Now can run MANOVA: results contain adjusted p-values: things notice Remember adjusting p-values potentially change conclusions original values significant adjustment can make p-values larger. adjusted p-values used time treatment:time interaction. Notice R labels . R labeled ‘week’ term Intercept fertilizer:week interaction fertilizer.","code":"manova1 <- manova(cbind(week1, week2, week3, week4, week5) ~ fertilizer,                    data = plantdata_wide) anova(manova1, X = ~1, test = \"Spherical\") #> Analysis of Variance Table #>  #>  #> Contrasts orthogonal to #> ~1 #>  #> Greenhouse-Geisser epsilon: 0.5882 #> Huynh-Feldt epsilon:        0.8490 #>  #>             Df      F num Df den Df Pr(>F) G-G Pr H-F Pr #> (Intercept)  1 158.22      4     32 0.0000 0.0000 0.0000 #> fertilizer   1   3.01      4     32 0.0326 0.0666 0.0422 #> Residuals    8"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"profile-analysis","dir":"Articles","previous_headings":"","what":"Profile analysis","title":"Lab 11: Repeated measures","text":"profile analysis requires calculating differences growth time interval (.e., number leaves grown week): four responses shown even though five weeks study? responses tell us? mean first significant others ?  can also plot growth rates. First, calculate mean growth rate time interval: Next, calculate standard errors growth rates: Now, create data frame plot results: Figure 2: Growth rate (leaves/week) 10 plants subjected two fertilizer treatments. Points represent estimated values error bars represent standard errors.  kinds interpretations can make plot? mean error bars overlap time intervals (necessarily mean anything)? horizontal dotted line represent mean estimated values line?","code":"manova2 <- manova(cbind(week2 - week1, week3 - week2, week4 - week3, week5 - week4) ~ fertilizer, data = plantdata_wide) summary.aov(manova2) #>  Response 1 : #>             Df Sum Sq Mean Sq F value Pr(>F)    #> fertilizer   1    2.5     2.5    12.5 0.0077 ** #> Residuals    8    1.6     0.2                   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #>  Response 2 : #>             Df Sum Sq Mean Sq F value Pr(>F) #> fertilizer   1    1.6     1.6     3.2   0.11 #> Residuals    8    4.0     0.5                #>  #>  Response 3 : #>             Df Sum Sq Mean Sq F value Pr(>F) #> fertilizer   1    0.1     0.1    0.06   0.81 #> Residuals    8   12.8     1.6                #>  #>  Response 4 : #>             Df Sum Sq Mean Sq F value Pr(>F) #> fertilizer   1    0.1     0.1    0.09   0.77 #> Residuals    8    8.8     1.1 leavesMat <- plantdata_wide[,3:7] growthMat <- leavesMat[,2:5] - leavesMat[,1:4] colnames(growthMat) <- paste(\"interval\", 1:4, sep=\".\") (lowFertilizer <- colMeans(growthMat[1:5,])) #> interval.1 interval.2 interval.3 interval.4  #>        1.2        1.4        1.2        2.2  (highFertilizer <- colMeans(growthMat[6:10,])) #> interval.1 interval.2 interval.3 interval.4  #>        2.2        2.2        1.0        2.0 SE <- sqrt(diag(stats:::vcov.mlm(manova2))) SE <- SE[names(SE)==\":(Intercept)\"] # Only use \"intercept\" SEs unname(SE) ## Ignore the names #> [1] 0.2000 0.3162 0.5657 0.4690 growthDF  <- data.frame(interval = rep(1:4, 2),                         fertilizer = rep(c(\"Low\", \"High\"), each = 4),                         growth = c(lowFertilizer, highFertilizer),                          SE = rep(SE, 2))   ggplot(growthDF, aes(x = interval, y = growth, color = fertilizer)) +   geom_path() +   geom_point() +   geom_errorbar(aes(ymin = growth - SE, ymax = growth + SE), width = 0.1) +   geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey50\") +   scale_x_continuous(\"Time interval\") +   scale_y_continuous(\"Growth rate (leaves/week)\", limits  = c(-1, 4))"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab11_repeated_measures.html","id":"assignment-optional","dir":"Articles","previous_headings":"","what":"Assignment (optional)","title":"Lab 11: Repeated measures","text":"researcher wants assess effects crowding growth dark toadfish (Neophrynichthys latus). Fifteen fish tanks stocked three densities toadfish. Five tanks low density (1 fish), 5 tanks medium density (5 fish), 5 tanks high density (10 fish). tank, weight one “focal fish” recorded 6 consecutive weeks. data can loaded using: Conduct repeated measures ANOVA using aov(). Calculate adjusted p-values using Huynh-Feldt method. effect density growth change time? Conduct profile analysis. time intervals effect density growth rate significant? Put answers R Markdown report. report include: well-formatted ANOVA table (header); publication quality plot showing effects crowding toadfish weight across time intervals study. figure descriptive caption aesthetics (color, line type, etc.) clearly defined may format report however like well-organized, relevant headers, plain text, elements described . always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output Please upload html .Rmd files submit assignment See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"library(FANR6750) data(\"fishdata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"lab-11","dir":"Articles","previous_headings":"","what":"Lab 11","title":"Lab 12: Generalized linear models: Logistic Regression","text":"Repeated Measures additive model Univariate approach Multivariate approach","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 12: Generalized linear models: Logistic Regression","text":"Logistic Regression Link functions Using glm() Creating plots","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"generalized-linear-models","dir":"Articles","previous_headings":"","what":"Generalized Linear Models","title":"Lab 12: Generalized linear models: Logistic Regression","text":"Remember lecture generalized linear models (GLMs) useful response variable predictor variables underlying linear relationship. situation, instead modeling response function predictors can instead model function response (known link function) function predictors. method many benefits including: residuals don’t normally distributed response variable can binary, integer, strictly-positive, etc… variance assumed constant","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"logistic-regression","dir":"Articles","previous_headings":"","what":"Logistic regression","title":"Lab 12: Generalized linear models: Logistic Regression","text":"One situation generalized linear model useful attempting model binary response variable. case, values response can take coded 0 1 (takes value researcher easier interpretation). examples?  Presence/Absence Survived/Died Male/Female Diseased/Healthy Breeding/Nonbreeding Migration/Nonmigration Extinction/Nonextinction  Let’s take look data example demonstrates concept.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"example-orchid-data","dir":"Articles","previous_headings":"Logistic regression","what":"Example: Orchid data","title":"Lab 12: Generalized linear models: Logistic Regression","text":"First, lets visualize data. Figure 1: Observations orchid presence absence across range elevation.  interested estimating presence. start constructing model estimates presence function elevation. Now let’s add model predictions plot . Figure 2: Observations orchid presence absence across range elevation. Line represents model predictions simple linear regression (mod1)  issues see plot? look like reasonable way model response variable? case, presented problem. binary response variable (yiy_i) makes sense us predict probability (pip_i) rather response . derive equation estimates probability function predictor variables. referred logistic function: p(x)=11+e−β0+β1x1+...+βzxz p(x) = \\frac{1}{1 + e^{-\\beta_0 + \\beta_1x1 + ... + \\beta_zx_z}} Now ’ve run another problem. issue equation context linear modeling?  Instead, can use link function (case logit link) create linear relationship response predictors. Remember lecture logistic regression model: $$\\Large logit(p_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + · · · + \\beta_zx_{iz}$$ $$\\Large y_i \\sim binomial(N, p_i)$$ : NN number ‘trials’ (e.g. coin flips) pip_i probability success sample unit ii zz number predictors model Let’s start using glm() function model relationship orchid detections (0 = detected, 1 = detected), habitat, elevation: Notice use family = argument tell R logistic (.e., binomial) glm logit link function. parameter estimates model represent, including intercept? can say effects habitat elevation probability detecting orchid?  seen previously, can use predict() estimate occurrence probability different combinations elevation habitat. example, probability occurrence differ across elevations oak habitat: default, predictions link scale get confidence intervals probability scale, back transform using inverse-link (plogis()):  now plot predictions observations: Figure 3: Observations orchid presence absence across range elevation. Line represents model predictions logistic regression Oak habitat. Dashed lines represent 95% confidence bands.  occurrence probability habitat type particular elevation: Figure 4: Probability occurence orchids across multiple habitat types elevation= 250m. Error bars represent approximate 95% confidence intervals.","code":"library(FANR6750) data(\"orchiddata\") head(orchiddata) #>   presence abundance elevation habitat #> 1        0         0        58     Oak #> 2        1         7       191     Oak #> 3        0         0        43     Oak #> 4        1        11       374     Oak #> 5        1        11       337     Oak #> 6        1         1        64     Oak library(ggplot2)  ggplot() +   geom_point(data = orchiddata, aes(x = elevation, y = presence)) +   scale_y_continuous(\"Presence\") +   scale_x_continuous(\"Elevation (m)\") mod1 <- lm(presence~ elevation, data= orchiddata) summary(mod1) #>  #> Call: #> lm(formula = presence ~ elevation, data = orchiddata) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -0.7072 -0.3361  0.0176  0.3103  0.5196  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)    #> (Intercept) 0.394014   0.121553    3.24   0.0031 ** #> elevation   0.001464   0.000428    3.42   0.0019 ** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.384 on 28 degrees of freedom #> Multiple R-squared:  0.295,  Adjusted R-squared:  0.269  #> F-statistic: 11.7 on 1 and 28 DF,  p-value: 0.00194 model_pred <- predict(mod1)  ggplot() +   geom_point(data = orchiddata, aes(x = elevation, y = presence)) +   geom_line(aes(orchiddata$elevation, model_pred)) +    scale_y_continuous(\"Presence\") +   scale_x_continuous(\"Elevation (m)\") fm1 <- glm(presence ~ habitat + elevation, family = binomial(link = \"logit\"), data = orchiddata) summary(fm1) #>  #> Call: #> glm(formula = presence ~ habitat + elevation, family = binomial(link = \"logit\"),  #>     data = orchiddata) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)   #> (Intercept) -0.99598    1.21698   -0.82    0.413   #> habitatOak  -0.09678    1.36752   -0.07    0.944   #> habitatPine -0.33722    1.38157   -0.24    0.807   #> elevation    0.01366    0.00601    2.27    0.023 * #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 34.795  on 29  degrees of freedom #> Residual deviance: 23.132  on 26  degrees of freedom #> AIC: 31.13 #>  #> Number of Fisher Scoring iterations: 6 model.matrix(fm1) #>    (Intercept) habitatOak habitatPine elevation #> 1            1          1           0        58 #> 2            1          1           0       191 #> 3            1          1           0        43 #> 4            1          1           0       374 #> 5            1          1           0       337 #> 6            1          1           0        64 #> 7            1          1           0       195 #> 8            1          1           0       263 #> 9            1          1           0       181 #> 10           1          1           0        59 #> 11           1          0           0       489 #> 12           1          0           0       317 #> 13           1          0           0        12 #> 14           1          0           0       245 #> 15           1          0           0       474 #> 16           1          0           0        83 #> 17           1          0           0       467 #> 18           1          0           0       485 #> 19           1          0           0       335 #> 20           1          0           0        20 #> 21           1          0           1       430 #> 22           1          0           1       223 #> 23           1          0           1        68 #> 24           1          0           1       483 #> 25           1          0           1        78 #> 26           1          0           1       214 #> 27           1          0           1        64 #> 28           1          0           1        73 #> 29           1          0           1       162 #> 30           1          0           1       468 #> attr(,\"assign\") #> [1] 0 1 1 2 #> attr(,\"contrasts\") #> attr(,\"contrasts\")$habitat #> [1] \"contr.treatment\" predData.elev <- data.frame(elevation = seq(min(orchiddata$elevation), max(orchiddata$elevation), length = 50),habitat = \"Oak\") head(predData.elev) pred.link <- predict(fm1, newdata = predData.elev, se.fit = TRUE) predData.elev$p <- plogis(pred.link$fit) # back transform to probability scale predData.elev$lower <- plogis(pred.link$fit - 1.96 * pred.link$se.fit) predData.elev$upper <- plogis(pred.link$fit + 1.96 * pred.link$se.fit) ggplot() +   geom_point(data = orchiddata, aes(x = elevation, y = presence)) +   geom_path(data = predData.elev, aes(x = elevation, y = p)) +   geom_ribbon(data = predData.elev, aes(x = elevation, ymin = lower, ymax = upper),               fill = NA, color = \"black\", linetype = \"longdash\") +   scale_y_continuous(\"Probability of occurrence\") +   scale_x_continuous(\"Elevation (m)\") predData.hab <- data.frame(habitat = c(\"Oak\", \"Maple\", \"Pine\"), elevation = 250) pred.hab <- predict(fm1, newdata = predData.hab, se.fit = TRUE) predData.hab$p <- plogis(pred.hab$fit) # back transform to probability scale predData.hab$lower <- plogis(pred.hab$fit - pred.hab$se.fit) predData.hab$upper <- plogis(pred.hab$fit + pred.hab$se.fit)  ggplot() +   geom_col(data = predData.hab, aes(x = habitat, y = p), fill = \"grey60\") +   geom_errorbar(data = predData.hab, aes(x = habitat, ymin = lower, ymax = upper),                 width = 0.1) +   scale_y_continuous(\"Probability of occurrence\") +   scale_x_discrete(\"Habitat type\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 12: Generalized linear models: Logistic Regression","text":"Researchers want know latitude landscape type influence probability American Crows infected West Nile Virus. One hundred crows captured tested West Nile Virus urban rural landscapes spanning latitude gradient. data can loaded using: Create R markdown report following: Fit logistic regression model crow data assess effects latitude landscape type (1.5 pt) Include well-formatted summary table parameter estimates (1.5 pt) Provide clear interpretation parameter estimates text (1.5 pt) Plot relationship infection probability latitude, rural urban landscapes, graph. graph include (1.5 pt): observed data points (color coded landscape) legend confidence intervals may format report however like well-organized, relevant headers, plain text, elements described .  always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"library(FANR6750) data(\"crowdata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab12_glm_lr.html","id":"bonus-assignment","dir":"Articles","previous_headings":"","what":"Bonus Assignment","title":"Lab 12: Generalized linear models: Logistic Regression","text":"Georgia Department Natural Resources (GADNR) interested understanding relationship several demographic variables likelihood deer hunter reach annual harvest limit (10 2 bucks). data collected GADNR (can found harvestdata dataset) contains: Limit: Whether hunter harvested limit (1- ; 0- )Age: Age hunterFam_veh: number vehicles owned householdDrive_dist: average drive distance hunter’s home hunting spotSick12: number available sick days accumulated hunter past 12 monthsSick24: number available sick days accumulated hunter past 24 monthsRange_hrs: number hours spent hunter shooting range prior start seasonMuzzleloader: Whether hunter participated muzzleloader season (1- ; 0- )Archery: Whether hunter participated archery season (1- ; 0- )Utilized: percent available days hunting season hunter utilizedPast_max: Whether hunter harvested limit past season (1- ; 0- )COUNTY: hunter’s county residence tasked constructing parsimonious model can accurately predict whether hunter harvest limit. addition constructing model, GADNR like able tell model accuracy (.e. percentage time model correctly assign 0 1). several ways may approach research question, rough outline : Check missing values typos Consider variables considered numerical vs factors Check correlation among predictors consider removing necessary (justify reasons removed predictors) Consider using functions sample() setdiff() may use quadratic effects interaction terms required . point, can get predictions observations test dataset (using predict() function) already actual observations set Next can convert predictions (values probability) ones zeros factors. Consider using ifelse() function . Now can test see many values predictions dataframe match corresponding value test set. Include report least one figure shows relationship “Limit” one continuous predictor variables. Make sure include model predictions 95% confidence intervals can set variables either reference level (categorical) mean (continuous) Make sure figure displays multiple levels one categorical variable","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab13_glm_pr.html","id":"lab-12","dir":"Articles","previous_headings":"","what":"Lab 12","title":"Lab 13: Generalized linear models: Poisson Regression","text":"Logistic Regression Link functions Using glm() Creating plots","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab13_glm_pr.html","id":"todays-topics","dir":"Articles","previous_headings":"","what":"Today’s topics","title":"Lab 13: Generalized linear models: Poisson Regression","text":"Poisson Regression Link functions Creating plots Poisson vs Negative binomial","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/lab13_glm_pr.html","id":"poisson-regression","dir":"Articles","previous_headings":"","what":"Poisson regression","title":"Lab 13: Generalized linear models: Poisson Regression","text":"week see another situation using linear model appropriate approach. , interested using count data response variable estimating counts function predictors.  Let’s take look data example demonstrate concept. data modified Ver Hoef Boveng 2007 include 400 observed counts harbor seals aerial surveys conducted coastal Alaska. First let’s visualize data. Figure 1: Counts harbor seals sites across range tide heights (relative low tide).  interested estimating count harbor seals aerial surveys. Like last week, start constructing model estimates count function tide height. Now let’s add model predictions plot . Figure 2: Counts harbor seals sites across range tide heights (relative low tide). Line represents model predictions simple linear regression (mod1)  issues see plot? look like reasonable way model response variable? Similar last week, presented problem. integer count response variable using linear model results unreasonable predictions (since possible response take values integers zero). can resolve similar way last week. lab, data (0,1) needed create response variable take values zero one (inclusive) probability reasonable choice. case, response possible positive integers need response can take possible positive values. case, can model mean response (λ\\lambda) rather count observations . derive equation estimates λ\\lambda function predictor variables: λ=eβ0+β1x1+...+βzxz \\lambda = e^{\\beta_0 + \\beta_1x1 + ... + \\beta_zx_z} Now ’ve run another problem. issue equation context linear modeling?  Instead, can use link function (case log link) create linear relationship response predictors. Remember lecture model Poisson regression : $$\\Large log(\\lambda_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + · · · + \\beta_px_{ip}$$ $$\\Large y_i \\sim Poisson(\\lambda_i)$$ λi\\lambda_i expected value yiy_i Using harbor data, let’s model number harbor seals detected site function site substrate tide height relative low tide: Notice use family = argument tell R poisson glm log link function. parameter estimates model represent, including intercept? can say effects substrate tide height harbor seal counts?  seen previously, can use predict() estimate counts different combinations tide height substrate. example, predicted count change across tide heights ice: get confidence intervals (0,∞)(0, \\infty) scale, predict log (link) scale back transform using inverse-link (exp()): Figure 3: Counts harbor seals sites ice across range tide heights (relative low tide). Line represents model predictions poisson regression model dashed lines represent approximate 95% confidence bands.  Although completed analysis, still may appropriate way consider dataset. issues see either plot model output related concepts discussed lecture? fitted line appear pass data well?","code":"library(FANR6750) data(\"harbordata\") head(harbordata) #>   Survey Number substrate Reltolow      SITE #> 1      1    250      rock      0.0 Abbess I. #> 2      2    144      rock      3.2 Abbess I. #> 3      3      0      rock      4.9 Alava Bay #> 4      4     42      rock      3.6 Alava Bay #> 5      5     10      rock      0.0 Alava Bay #> 6      6      0      rock      1.6 Alava Bay library(ggplot2)  ggplot() +   geom_point(data = harbordata, aes(x = Reltolow, y = Number)) +   scale_y_continuous(\"Count\") +   scale_x_continuous(\"Tide relative to low (m)\") mod1 <- lm(Number~ Reltolow, data= harbordata) summary(mod1) #>  #> Call: #> lm(formula = Number ~ Reltolow, data = harbordata) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #>  -92.4  -63.3  -29.0   38.7  663.6  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)    92.39       6.05   15.26  < 2e-16 *** #> Reltolow      -11.52       2.17   -5.31  1.9e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 99.4 on 398 degrees of freedom #> Multiple R-squared:  0.0661, Adjusted R-squared:  0.0637  #> F-statistic: 28.2 on 1 and 398 DF,  p-value: 1.86e-07 model_pred <- predict(mod1)  ggplot() +   geom_point(data = harbordata, aes(x = Reltolow, y = Number)) +   geom_line(aes(harbordata$Reltolow, model_pred), color= 'red') +    scale_y_continuous(\"Count\") +   scale_x_continuous(\"Tide relative to low (m)\") fm2 <- glm(Number ~ substrate + Reltolow, family = poisson(link = \"log\"), data = harbordata) summary(fm2) #>  #> Call: #> glm(formula = Number ~ substrate + Reltolow, family = poisson(link = \"log\"),  #>     data = harbordata) #>  #> Coefficients: #>               Estimate Std. Error z value Pr(>|z|)     #> (Intercept)    5.42280    0.01343   403.7   <2e-16 *** #> substraterock -1.00615    0.01529   -65.8   <2e-16 *** #> substratesand -1.28421    0.03586   -35.8   <2e-16 *** #> Reltolow      -0.19100    0.00386   -49.5   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for poisson family taken to be 1) #>  #>     Null deviance: 42725  on 399  degrees of freedom #> Residual deviance: 34227  on 396  degrees of freedom #> AIC: 36116 #>  #> Number of Fisher Scoring iterations: 6 predData <- data.frame(Reltolow = seq(min(harbordata$Reltolow), max(harbordata$Reltolow), length = 10000), substrate= 'ice') pred.link <- predict(fm2, newdata = predData, se.fit = TRUE) predData$lambda <- exp(pred.link$fit) # exp is the inverse-link function predData$lower <- exp(pred.link$fit - 1.96 * pred.link$se.fit) predData$upper <- exp(pred.link$fit + 1.96 * pred.link$se.fit)  ggplot() +   geom_point(data = harbordata, aes(x = Reltolow, y = Number)) +   geom_path(data = predData, aes(x = Reltolow, y = lambda), color= 'red') +   geom_ribbon(data = predData, aes(x = Reltolow, ymin = lower, ymax = upper),               fill = NA, color = \"red\", linetype= 'dashed') +   scale_x_continuous(\"Tide relative to low (m)\") +   scale_y_continuous(\"Expected count\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab13_glm_pr.html","id":"negative-binomial-regression","dir":"Articles","previous_headings":"","what":"Negative Binomial Regression","title":"Lab 13: Generalized linear models: Poisson Regression","text":"dataset, assuming mean equal variance may good idea. Especially considering mean count 74.07 variance 1.0558^{4}. Instead, consider using methods. lecture, discussed use quasi-likelihood estimation (Quasi-Poisson). , talk briefly Negative Binomial (NB) distribution.1 sense, Poisson distribution can thought specific case NB distribution mean equal variance. relaxing assumption though, end two parameters instead one (mean ‘dispersion’). allows scenarios variance much larger mean (overdispersion). biological scenarios might see ?  , fit NB model dataset perform plotting steps see results differ. Figure 4: Counts harbor seals sites across range tide heights (relative low tide). Solid lines represent model predictions poisson regression model negative binomial regression model. Dashed lines represent approximate 95% confidence bands.  ways model predictions NB regression differ Poisson regression? Compare information provided model output model.","code":"library(MASS)  nb1 <- glm.nb(Number ~ substrate + Reltolow, data = harbordata) summary(nb1) #>  #> Call: #> glm.nb(formula = Number ~ substrate + Reltolow, data = harbordata,  #>     init.theta = 0.4661173743, link = log) #>  #> Coefficients: #>               Estimate Std. Error z value Pr(>|z|)     #> (Intercept)     5.4190     0.2933   18.48  < 2e-16 *** #> substraterock  -1.0372     0.3077   -3.37  0.00075 *** #> substratesand  -1.3827     0.4500   -3.07  0.00212 **  #> Reltolow       -0.1608     0.0327   -4.92  8.8e-07 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for Negative Binomial(0.4661) family taken to be 1) #>  #>     Null deviance: 532.29  on 399  degrees of freedom #> Residual deviance: 484.04  on 396  degrees of freedom #> AIC: 4009 #>  #> Number of Fisher Scoring iterations: 1 #>  #>  #>               Theta:  0.4661  #>           Std. Err.:  0.0319  #>  #>  2 x log-likelihood:  -3999.4500  predDatanb <- data.frame(Reltolow = seq(min(harbordata$Reltolow), max(harbordata$Reltolow), length = 10000), substrate= 'ice')  pred.linknb <- predict(nb1, newdata = predDatanb, se.fit = TRUE) predDatanb$lambda <- exp(pred.linknb$fit) # exp is the inverse-link function predDatanb$lower <- exp(pred.linknb$fit - 1.96 * pred.linknb$se.fit) predDatanb$upper <- exp(pred.linknb$fit + 1.96 * pred.linknb$se.fit)  ggplot() +   geom_point(data = harbordata, aes(x = Reltolow, y = Number)) +   geom_path(data = predData, aes(x = Reltolow, y = lambda, color= 'Poisson')) +   geom_ribbon(data = predData, aes(x = Reltolow, ymin = lower, ymax = upper, color= 'Poisson'),               fill = NA, linetype= 'dashed') +   geom_path(data = predDatanb, aes(x = Reltolow, y = lambda, color= 'Negative Binomial')) +   geom_ribbon(data = predDatanb, aes(x = Reltolow, ymin = lower, ymax = upper, color= 'Negative Binomial'),               fill = NA, linetype= 'dashed') +   scale_x_continuous(\"Tide relative to low (m)\") +   scale_y_continuous(\"Expected count\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/lab13_glm_pr.html","id":"assignment","dir":"Articles","previous_headings":"","what":"Assignment","title":"Lab 13: Generalized linear models: Poisson Regression","text":"Researchers interested understanding factors affect alligator clutch size (number eggs). 350 nests surveyed across 3 habitat types. surveys, clutch size recorded well length female alligator. data can loaded using: Create R markdown report following: Use regression methods discussed lab (Poisson NB) estimate clutch size function available predictors (2 pt) Include well-formatted summary table parameter estimates models (1 pt) Choose one models provide clear interpretation parameter estimates (1 pt). Use poisson regression model plot relationship number eggs female length, three habitat types, graph. graph include (1 pt): observed data points (color coded habitat) legend approximate 95% confidence intervals Create plot relationship number eggs female length (one three habitat types) using regression methods graph. graph include (1 pt): observed data points legend approximate 95% confidence intervals Interpret plot Question 5. regression method think appropriate dataset? Justify decision (2 pt) always: sure output type set : output: html_document sure include first last name author section sure set echo = TRUE R chunks can see code output See R Markdown reference sheet help creating R chunks, equations, tables, etc. See “Creating publication-quality graphics” reference sheet tips formatting figures","code":"library(FANR6750) data(\"alligatordata\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"common-workflows-and-their-problems","dir":"Articles","previous_headings":"","what":"Common workflows and their problems","title":"Improving your workflow through projects","text":"common workflow used many novice R users goes something like : Open base R maybe RStudio Create script save analysis_for_my_project.R Start script : Create bunch objects stuff : Realize need working dissertation instead side project open another script started earlier Get confused objects current environment created script rm(list = ls()) start scratch Rinse repeat","code":"rm(list = ls()) setwd(\"C:\\Users\\crushing\\path\\that\\only\\I\\have\")  library(package1) library(package2) x <- 1:10 y <- \"blah\""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"whats-wrong-with-this","dir":"Articles","previous_headings":"Common workflows and their problems","what":"What’s wrong with this?","title":"Improving your workflow through projects","text":"Remember open R (RStudio), create global workspace. objects create get stored. type ls() can see everything currently stored workspace. working different analyses global environment, easy accidentally create different objects name, write code depends code happens run previously one R session (example, loaded needed package one script didn’t add library(package) next script. happens try run code next time? often called “hidden dependency” can make life pretty miserable), many confusing behaviors. workflow also makes difficult share work. Remember R default looking /saving current working directory. control behavior, people taught use setwd(). problem path set using setwd() almost certainty specific computer used write code. use different computer later send code adviser help, able run without changing working directory. move files new place computer, code break. Got new computer? Oops, none code work anymore. running multiple analyses R session, ’ll also constantly change working directory, eventually lead confusion problems rerunning code. Manually setting working directory seems like minor annoyance constantly (example, instructor grade bunch homework assignments), minor annoyances add big headache. good news , ’s easy solution make life much easier.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"rstudio-projects","dir":"Articles","previous_headings":"","what":"RStudio Projects","title":"Improving your workflow through projects","text":"Experienced programmers learned long ago problems outlined . solve problems, typically use self-contained directory (.e., folder) project. directory contains data code needed create research output project. Notably, code reference data objects found another directory. helps ensure portability - can move entire project still able rerun code. RStudio makes easy create Projects (clarity, use “Project” referring specifically RStudio Projects “project” refer generic research projects, .e., chapter dissertation). highly recommend create new Project project working . several ways easiest way : Open RStudio Click File -> New Project click New Directory (already folder associated project, can click Existing Directory navigate folder) Click New Project choose name location Project (tip - don’t include spaces project name). now worry creating git repository using packrat Click Create Project bottom. RStudio create new directory name Project file called project_name.Rproj inside directory. new RStudio window open create new project. future, want work Project, just double click project_name.Rproj file open new instance RStudio.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"whats-so-special-about-rstudio-projects","dir":"Articles","previous_headings":"RStudio Projects","what":"What’s so special about RStudio Projects?","title":"Improving your workflow through projects","text":"Projects solve two big problems discussed earlier. First, project opens fresh instance R global environment projects self-contained. means can multiple projects open one behave independently. means rm(list = ls()) objects environment ones associated project (doesn’t totally resolve problem hidden dependencies ’ll talk ). Second, RStudio automatically set working directory root directory associate Project. means whatever folder .Rproj file automatically set working directory open project. can check typing getwd() console new project. setting avoids need use setwd() makes code much portable. can move send entire directory, double-click .Rproj file pick right left .","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"a-note-on-saving-your-workspace-to--rdata","dir":"Articles","previous_headings":"RStudio Projects","what":"A note on saving your workspace to .RData","title":"Improving your workflow through projects","text":"Another common mistake made many novice (novice) R users save current workspace .RData time quit R RStudio. surface, seems like really convenient shortcut. next time open R, objects created previously right waiting keep going! problem , , creates hidden dependencies. .RData file re-load packages using previous session. re-set options may set previous session (setwd(), stringsAsFactors = FALSE). objects needed , e.g., making figure may loaded others may . means even saved workspace, still end needing re-run chunks code get back left previously. chunks? knows. probably think ’ll remember won’t. better workflow start every R session empty workspace. means go session mindset need re-run code scratch every time. Although seems like pain, promise make life easier. way ensuring minimum, can reproduce analyses anytime need . RStudio, can enforce workflow going RStudio -> Tools -> Global options General tab setting Save workspace .Rdata exit Never. . Trust . re-running code time consuming? really need re-run code cleans raw data every time need reanalyze ? course . can always save objects (along scripts used create !) next time need , can just re-read R can everything downstream steps without problem. One benefits workflow treats raw data code real objects workflow. objects lose , big trouble (’re computer backed , right?). Cleaned data created raw data isn’t real. can just re-run code create objects (use R scripts instead manipulating raw data excel. See details). workspace isn’t real - just discussed, can () disappear time. Results figures real can re-create code whenever need . get mindset treating raw data code real objects, workflow automatically become organized reproducible. (Manuscripts also real save back !)","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"organizing-your-project-directories","dir":"Articles","previous_headings":"","what":"Organizing your project directories","title":"Improving your workflow through projects","text":"already saw, .Rproj file treats directory working directory project. means can read files location write files location without changing settings. example, can stick filled called data.csv directory following code work: natural instinct us inclined less-organized might put files associated project directory: nothing inherently wrong projects, end big directory hard navigate need find specific files. better idea put files intuitive subdirectories. can pick whatever structure makes sense , generally use something like: data-raw/ contains , guessed , raw data files R script reads raw data , cleans , saves clean data data/ data/ contains cleaned data files ready go straight analysis R/ contains scripts custom functions use data cleaning, analysis, making figures (’ll talk later) figs/ contains figure files doc/ contains files reports manuscripts related projects output/ includes output analysis, example results fitting model scripts/ contains R scripts used analyze data, make figures, whatever. put subdirectory related function script (data_cleaning_script.R) makes sense . can lump different scripts single script prefer. ’s .","code":"data <- read.csv(\"data.csv\") my_proj/ |─── data.csv |─── script_that_does_everything.R |─── test_script_that_does_something_else.R |─── Figure1.png |─── manuscript_v1.doc |─── manuscript_v2.doc . . . |─── manuscript_final_v2-4.doc my_proj/ |─── data-raw/       |─── data.csv       |─── data_cleaning_script.R |─── data/       |─── clean_data.rds |─── R/       |─── function1.R       |─── function2.R |─── figs/       |─── fig1.png       |─── fig2.png |─── doc/       |─── manuscript.Rmd |─── output/       |─── model_results.rds |─── scripts/       |─── analysis.R       |─── figures.R"},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"readingwriting-from-subdirectories","dir":"Articles","previous_headings":"Organizing your project directories","what":"Reading/writing from subdirectories","title":"Improving your workflow through projects","text":"RStudio treats project root directory working directory, following , example, data stored subdirectory: Instead, need use relative paths. Remember everything relative working directory: prefer, can many layers subdirectories want. important thing long move entire project directory, relative paths still work.","code":"data <- read.csv(\"data.csv\") data <- read.csv(\"data-raw/data.csv\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"a-suggested-structure-for-this-class","dir":"Articles","previous_headings":"Organizing your project directories","what":"A suggested structure for this class","title":"Improving your workflow through projects","text":"scripts/ contains R scripts created lab. homework/ contains sub-directories files associated homework. exams/ contains RMarkdown files exam (yes, exams completed RMarkdown).","code":"FANR6750/ |─── scripts/       |─── lab1.R       |─── lab2.R |─── homework/       |─── LastnameFirstname-homework1/             |─── LastnameFirstname-homework1.Rmd             |─── LastnameFirstname-homework1.html       |─── LastnameFirstname-homework2/             |─── LastnameFirstname-homework2.Rmd             |─── LastnameFirstname-homework2.html |─── exams/       |─── exam1.Rmd       |─── exam2.Rmd"},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"a-note-on-handling-raw-data","dir":"Articles","previous_headings":"Organizing your project directories","what":"A note on handling raw data","title":"Improving your workflow through projects","text":"Raw data sacred. purposely accidentally change raw data, everything comes downstream (analysis, results, reports) also change. maintain integrity work, get habit never making changes raw data files entered data. One great things using scripts manipulate data can leave raw data untouched paper trail every change made prepare data analysis. means adding new variables composites raw data (e.g., temp_c <- (temp_f -32)*(5/9)), removing outliers, joining different data sets together, whatever. done R ’ve read raw data rather Excel! Just important, don’t save new data objects raw data even directory. files go data/. means can read data-raw/ write . R , course, allow write data-raw/, just suggestion help maintain firewall keep raw data ’s raw form.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/projects_and_directories.html","id":"additional-resources","dir":"Articles","previous_headings":"","what":"Additional resources","title":"Improving your workflow through projects","text":"RStudio Project webpage Nice R Code Jenny Bryan’s 🔥 take setwd() rm(list=ls()) STAT545’s tutorial workspaces projects","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"logistics","dir":"Articles","previous_headings":"","what":"LOGISTICS","title":"Syllabus","text":"Lecture: Monday, Wednesday, Friday 11:30-12:20Location: 1-304 Lab: Monday 1:50 – 3:45, Tuesday 2:20 – 4:20, Wednesday 9:10 - 11:10Location: 4-419","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"instructor","dir":"Articles","previous_headings":"","what":"INSTRUCTOR","title":"Syllabus","text":"Dr. Clark Rushingclark.rushing@uga.eduOffice: Warnell 3-409Office hours: M 1:00-2:30","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"teaching-assistants","dir":"Articles","previous_headings":"","what":"TEACHING ASSISTANTS","title":"Syllabus","text":"Anna Willoughby (Monday lab)anna.willoughby@uga.eduOffice: TBDOffice hours: W 3:30-5pm Alan Bond (Tuesday lab)alan.bond1@uga.eduOffice: 1-102Office hours: F 10:00am-12:00pm Himasree Kolla (Wednesday lab)himasree.kolla@uga.eduOffice: TBDOffice hours: TBD","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"course-objectives","dir":"Articles","previous_headings":"","what":"COURSE OBJECTIVES","title":"Syllabus","text":"understand: (1) logical structure experiments, including manipulative observational experiments, (2) analysis experiments, focus linear models, (3) use models ecological studies (experimental observational).","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"PREREQUISITES","title":"Syllabus","text":"lab activities course rely heavily statistical programming language R. first weeks, lab session begin tutorial tools students expected experts prior course. However, quickly move activities require degree R proficiency highly recommend basic understanding programming R (e.g., importing/exporting & manipulating data objects, visualizing data) prior course. find struggling aspects using R, please seek individual help office hours. earlier can get speed, painless remainder semester .","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"course-format","dir":"Articles","previous_headings":"","what":"COURSE FORMAT","title":"Syllabus","text":"components scientific study need considered together rather separately; design analysis aspects completely blended course. Emphasis application, include instruction use R statistical software. Important points reinforced readings scientific literature discussions. Homework assignments exams intended keep material fresh students’ minds. syllabus subject change, order material correct.","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"textbooks","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Textbooks","title":"Syllabus","text":"Lecture material largely based : Fieberg, J. (2024). Statistics Ecologists: Frequentist Bayesian Treatment Modern Regression Models. open-source online textbook. Quinn, G.P. & Keough, M.J. 2002. Experimental Design Data Analysis Biologists. Cambridge University Press Fieberg (2024) textbook can accessed . Purchasing Quinn (2002) textbook required may helpful clarifying concepts. pdf book can downloaded . Additional readings come variety sources, including textbooks scientific journals, posted eLC.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"lab-materials","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Lab materials","title":"Syllabus","text":"Materials labs provided HTML R Markdown files course webpage. materials include step--step tutorials lab exercises well links additional online resources, problem sets, homework assignments.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"software","dir":"Articles","previous_headings":"COURSE RESOURCES","what":"Software","title":"Syllabus","text":"lab computers R RStudio installed students required install software computers. However, students free use laptops lab R RStudio installed make easier complete lab assignments outside class. students wishing use computers R RStudio installed running prior first lab. Detailed instructions installing R RStudio can found . plan use computer, sure recent versions software programs installed. greatly decrease chances running issues running code provide lab. Prior start semester, test R RStudio installed correctly following: Launch RStudio Put cursor window labeled Console. Type following code followed enter return: x <- 2 * 4. Next type x followed enter return. see value 8 print screen. yes, ’ve succeeded installing R RStudio.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"course-r-package","dir":"Articles","previous_headings":"COURSE RESOURCES > Software","what":"Course R package","title":"Syllabus","text":"Materials course distributed R package called FANR6750. main purpose package distribute code data used labs, though eventually additional materials may included, including lectures reference documents. can install current version FANR6750 : encounter problems previous steps, please contact prior first class.","code":"install.packages(\"devtools\") devtools::install_github(\"RushingLab/FANR6750\")"},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"course-policy-on-generative-ai","dir":"Articles","previous_headings":"","what":"COURSE POLICY ON GENERATIVE AI","title":"Syllabus","text":"Focus course conceptual basis modern experimental design statistical inference, primary objective providing students skills confidence design, conduct, report statistical analyses needed conduct research. many students, statistics class take graduate school. others, class beginning progression towards advanced statistical analyses. either case, goal provide firm foundation conducting evaluating scientific research. One core philosophies course one-size-fits-approach experimental design statistical inference. recipe book students can follow conducting research often multiple ways approach experimental design data analyses. Successful research therefore requires combining ecological knowledge thoughtful application statistical methods. complex systems within work, critical thinking, domain expertise, skepticism paramount. Developing skills graduate school. Computers integral modern statistical inference, allowing researchers perform complex data formatting, modeling, visualization tasks otherwise impractical impossible. resources (textbooks, message boards, blog posts, etc.), AI can help make tasks (e.g., coding) easier efficient used appropriately. reason, use AI homework assignments banned class (unless specifically noted instructions). However, students must document use AI providing prompts used AI responses every task AI used. Furthermore, increasing evidence AI may harm intellectual development used improperly. particular, risk AI replaces learning rather augmenting learning process. Developing research skills, including statistical analysis, takes practice. Failure, setbacks, frustration inevitable, critical learning. view, biggest risk generative AI can provide shortcut avoid short-term challenges associated learning new concepts skills. Although may seem convenient even helpful, long term harm using AI replace learning detrimental development researcher, scholar, citizen. reason, exams structured way AI used. homework questions also state AI used students expected follow instructions. Suspected cases unacknowledged use AI treated violations honor code.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"attendance","dir":"Articles","previous_headings":"","what":"ATTENDANCE","title":"Syllabus","text":"graduate students willingly signed course, presume eager learn material self-motivated enough put required effort. reason, set formal attendance policy. However, cover lot material course semester topic build concepts previous weeks. result, missing even lectures labs make difficult fully master learning outcomes described . know missing lectures labs, please contact advance can make sure get far behind material.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"a-note-on-fieldwork","dir":"Articles","previous_headings":"ATTENDANCE","what":"A note on fieldwork","title":"Syllabus","text":"realize many students field work obligations semester. need take course know advance field portion semester, please let know ASAP can discuss whether field work barrier taking course merely inconvenience. distinction mainly function long miss class. absences relatively , taking course may still option. best record lectures make available via eLC. Students still expected complete turn assignments miss. going miss many classes unable complete assignments ’re field, may better take class time field commitments smaller.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"grading","dir":"Articles","previous_headings":"","what":"GRADING","title":"Syllabus","text":"200 total points. Three exams, 40 points (20%) . Exams completed normal lecture periods using eLC/Lockdown Browser. Students must complete exams person scheduled exam period, unless otherwise approved instructor least 1 week exam. Remote students must arrange take exams proctor present exam arrangements must approved instructor prior exam. Students need laptop Lockdown Browser installed complete exams. Exams primarily focused concepts, coding. least 1 week prior exam, students given pool 8-10 potential questions. exam consist 2 questions list chosen instructor 2 questions chosen student. Exam dates schedule approximate subject change. remaining 80 points (40%) grade come lab homework assignments. Homework assignments build concepts skills cover lecture lab. Specific objectives tasks assignment, along necessary data, provided end lab activity. assignments meant ensure understand lecture lab concepts, provide additional practice implementing relevant statistical methods R, provide experience interpreting presenting model output. course semester, 8 assignments worth 10 points . assignment due class week assignment posted must prepared R Markdown file include text, code, model output, figures necessary fully document work conclusions (spend first several labs going preparation reports using R Markdown , , previous experience necessary). automatically receive 5 points turning completed lab assignment assigned due date. Late assignments receive 1-point deduction day late incomplete assignments points deducted proportion degree incompleteness (e.g., 1/3 assignment incomplete, 2-point deduction). remaining points based correctness answers. academic work must meet standards contained University’s academic honesty policy. students responsible informing standards performing academic work. penalties academic dishonesty severe, ignorance acceptable defense. know ahead time miss assignment exam, please let know soon possible can arrange make . let TA know ahead time, missed/late assignments lose 10% final grade per day late.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"pre--and-post-course-assessments","dir":"Articles","previous_headings":"","what":"PRE- AND POST-COURSE ASSESSMENTS","title":"Syllabus","text":"effort help track learning outcomes, students asked complete optional pre-course post-course assessments. assessments can accessed eLC first week (pre-assessment) last week (post-assessment) semester. assessments short (~20 questions) consist primarily multiple choice true/false questions cover range topics throughout semester, plus several self-assessment questions aimed gauging students’ experience statistical methods. Responses assessments count towards final grade used assess trends learning outcomes. incentivize participation, students receive 2 bonus points completing pre-assessment, 2 bonus points completing post-assessment, 1 additional bonus point completing (5 bonus points total).","code":""},{"path":[]},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"university-honor-code-academic-honesty","dir":"Articles","previous_headings":"","what":"UNIVERSITY HONOR CODE & ACADEMIC HONESTY","title":"Syllabus","text":"University Georgia student, agreed abide UGA academic honesty policy. UGA Student Honor code: academically honest academic work tolerate academic dishonesty others Culture Honesty, University’s policy procedures handling cases suspected dishonesty, can found https://honesty.uga.edu/ responsible informing university’s standards performing academic work. Lack knowledge academic honesty policy reasonable explanation violation. Please ask questions related course assignments academic honesty policy. form possible academic dishonesty reported UGA Office Vice President Instruction.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"accommodations-for-disabilities","dir":"Articles","previous_headings":"","what":"ACCOMMODATIONS FOR DISABILITIES","title":"Syllabus","text":"require disability-required accommodation, essential register Disability Resource Center (Clark Howell Hall; https://drc.uga.edu; 706-542-8719 [voice]; 706-542-8778 [TTY]) notify eligibility reasonable accommodations. can plan best coordinate accommodations. Please note accommodations provided retroactively.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"wellness-statement","dir":"Articles","previous_headings":"","what":"WELLNESS STATEMENT","title":"Syllabus","text":"Mental Health Wellness Resources: someone know needs assistance, encouraged contact Student Care Outreach Division Student Affairs 706-542-7774 visit https://sco.uga.edu/. help navigate difficult circumstances may facing connecting appropriate resources services. UGA several resources student seeking mental health services (https://www.uhs.uga.edu/bewelluga/bewelluga) crisis support (https://www.uhs.uga.edu/info/emergencies). need help managing stress anxiety, relationships, etc., please visit BeWellUGA (https://www.uhs.uga.edu/bewelluga/bewelluga) list FREE workshops, classes, mentoring, health coaching led licensed clinicians health educators University Health Center. Additional resources can accessed UGA App.","code":""},{"path":"http://rushinglab.github.io/FANR6750/articles/syllabus.html","id":"ferpa-notice","dir":"Articles","previous_headings":"","what":"FERPA NOTICE","title":"Syllabus","text":"Federal Family Educational Rights Privacy Act (FERPA) grants students certain information privacy rights. comply FERPA, communication refers individual students must secure medium (UGAMail eLC) person. Instructors allowed respond messages refer individual students student progress course non-UGA accounts, phone calls, types electronic media. details, please visit https://apps.reg.uga.edu/FERPA.","code":""},{"path":"http://rushinglab.github.io/FANR6750/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Clark Rushing. Maintainer.","code":""},{"path":"http://rushinglab.github.io/FANR6750/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"person, \"crushing@uga.edu\") e= (2025). FANR6750: Course Materials FANR 6750. R package version 2025.1.0, http://rushinglab.github.io/FANR6750.","code":"@Manual{,   title = {FANR6750: Course Materials for FANR 6750},   author = {{person} and email = \"crushing@uga.edu\")},   year = {2025},   note = {R package version 2025.1.0},   url = {http://rushinglab.github.io/FANR6750}, }"},{"path":"http://rushinglab.github.io/FANR6750/index.html","id":"welcome-to-fanr6750-experimental-methods-in-forestry-and-natural-resources-research","dir":"","previous_headings":"","what":"Course Materials for FANR 6750","title":"Course Materials for FANR 6750","text":"unofficial course website Fall 2025 offering FANR6750: Experimental Methods Forestry Natural Resources Research University Georgia. “official” course website (students enrolled course) eLC. goal website create central repository students access course materials - lecture slides, lab activities, code, data, etc. secondary goal make materials freely available students instructors may find useful. encounter issues suggestions, feel free contact clark.rushing [] uga [dot] edu. General course information can found clicking Syllabus Schedule links . Lecture slides lab activities can accessed using drop menus.","code":""},{"path":"http://rushinglab.github.io/FANR6750/index.html","id":"course-r-package","dir":"","previous_headings":"","what":"Course R package","title":"Course Materials for FANR 6750","text":"addition website, materials course distributed R package called FANR6750. main purpose package distribute code data used labs, though eventually additional materials may included, including lectures reference documents. can install current version FANR6750 :","code":"install.packages(\"devtools\") devtools::install_github(\"RushingLab/FANR6750\")"},{"path":"http://rushinglab.github.io/FANR6750/index.html","id":"use-of-material","dir":"","previous_headings":"","what":"Use of material","title":"Course Materials for FANR 6750","text":"Materials included course purposefully made available anyone finds useful. Users free use, adapt, distribute, display, communicate materials freely. find materials useful, please let know, especially using adapting materials teaching. Tracking use materials outside official course great validation effort, helps demonstrate “positive professional reputation teaching.”","code":""},{"path":"http://rushinglab.github.io/FANR6750/index.html","id":"previous-course-materials","dir":"","previous_headings":"","what":"Previous course materials","title":"Course Materials for FANR 6750","text":"websites previous versions course archived : Fall 2024 Fall 2023 Fall 2022","code":""}]
