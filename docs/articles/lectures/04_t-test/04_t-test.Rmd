---
title: "LECTURE 4: linear models part 1: categorical predictor w/ 2 levels"
subtitle: "FANR 6750 (Experimental design)"
author: "<br/><br/><br/>Fall 2023"
output:
  xaringan::moon_reader:
    css: ["default", "FANR6750.css", "FANR6750-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', warning=FALSE, message=FALSE, fig.retina = 2)
source(here::here("R/zzz.R"))
library(emo)
library(FANR6750)
# library(gganimate)
```

class: inverse

# outline

<br/>
#### 1) Categorical predictor with 1 level (linear model vs t-test formulations)

<br/>  
--

#### 2) Categorical predictor with 2 levels (linear model vs t-test formulations)

<br/> 
--

#### 3) Dummy variables and model matrices

<br/> 


---
# context, part 1

#### We are interested in whether a certain brand of scale provides accurate mass measurements

--

Hypothesis

> Measurement error, measured as the difference between the mass indicated by the scale and the true mass of an object, is 0 (on average)

--

Field procedure:

- randomly sample 10 scales of the same make and model

- weigh a test object (of known mass) on each scale and record the difference between the measured and true mass 


--

Data:

`-0.062, -0.38, 0.85, -0.58,  0.53,  0.09,  0.31,  0.77,  0.59,  -0.17`

---
# the linear model

$$\LARGE y_i = \beta_0 + \epsilon_i$$

$$\LARGE \epsilon_i \sim normal(0, \sigma)$$

--

- $\large \beta_0$ is the expected (or average) measurement error across all scales

- $\epsilon_i$ is residual error associated with each measurement

--

- This is most simple linear model we can construct

---
# the linear model

#### Because this is a linear model, we can fit it using the `lm` function:

```{r echo = TRUE}
y <- c(-0.062, -0.38, 0.85, -0.58,  0.53,  0.09,  0.31,  0.77,  0.59,  -0.17)

fit1 <- lm(y ~ 1)

broom::tidy(fit1)
```


--

- The `(Intercept)` estimate corresponds to $\beta_0$ and is simply the mean value of $y$

--

- The standard error is calculated simply as $\sigma \ sqrt{n}$
--

- We will discuss whether there is evidence that $\beta_0 \neq 0$ in the next lecture

---
# the linear model as a t-test

#### The model we just fit has a corresponding "test" that you may encounter: the *one-sample t-test*

.pull-left[
- One-sample t-tests are used to test whether the mean of a population differs from some value (in this case 0)

- there is a built-in function to fit t-tests

- the mean (and the SE, t-statistic, p-value, confidence interval) are exactly the same in both approaches
]

.pull-right[

```{r echo = TRUE}
t.test(y)
```
]

---
# the one-sample t-test by hand

#### Because this is such a simple model, we can calculate $\beta_0$ and the standard error by hand very easily

```{r echo = TRUE}
(beta0 <- mean(y))
(SE <- sd(y)/sqrt(10))
```

--

Again, we will discuss how to use this information to determine whether $\beta_0$ is or is not equal to 0 in the next lecture


---
class:inverse

# context, part 2

#### We are interested in determining how urban development influences the abundance of strawberry poison-dart frogs (*Oophaga pumilio*)

.left-column[
</br>
```{r}
knitr::include_graphics("fig/frog2.png")
```
]

.right-column[
Hypothesis

> Strawberry frog abundance will differ between islands with high urban development and island with low urban development

Field procedure:

- $\large n=10$ plots are established on one high-development island and one low-development island

- Within each plot, strawberry frogs are counted within 5 randomly-located quadrats 

]

---
# context, part 2

.pull-left[
</br>
```{r out.width="80%"}
knitr::include_graphics("fig/frog.png")
```

]

.pull-right[
#### Data:

`Low development: 16, 14, 18, 17, 29, 31, 14, 16, 22, 15`

`High development: 2, 11, 6, 8, 0, 3, 19, 1, 6, 5`
]

---
# the linear model

#### A "simple" example

$$\underbrace{\LARGE E[y_i] = 7 + 3.5 \times x_i}_{Deterministic}$$

$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma=0.25)}_{Stochastic}$$

```{r fig.height=3.5, fig.width=6}
x <- rnorm(100)
y <- 7 + 3.5 * x + rnorm(100, 0, 1)
mu <- 7 + 3.5 * x
resid <- y - mu
df <- data.frame(x = x, y = y, mu = mu, Residuals = resid)

ggplot(df, aes(x, y)) + 
  geom_abline(slope = 3.5, intercept = 7, color = "#446E9B", size = 1.5) +
  scale_y_continuous(expression(mu)) +
  geom_point(color = "#D47500", alpha = 0.75)
```


---
# the linear model

#### Same model, different $\Large x$

$$\underbrace{\LARGE E[y_i] = 7 + 3.5 \times x_i}_{Deterministic}$$

$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma=0.25)}_{Stochastic}$$


```{r fig.height=3.5, fig.width=4}
x <- rbinom(100, size = 1, prob = 0.5)
y <- 7 + 3.5 * x + rnorm(100, 0, 1)

df <- data.frame(x = x, y = y)

ggplot(df, aes(x, y)) + geom_boxplot(color = "white") +
  geom_segment(aes(x = -0.1, xend = 0.1, y = 7, yend = 7), color = "#446E9B", size = 1.5) +
  geom_segment(aes(x = 0.9, xend = 1.1, y = 10.5, yend = 10.5), color = "#446E9B", size = 1.5) +
  scale_y_continuous(expression(mu)) +
  scale_x_continuous(breaks = c(0, 1))
```


---
# the linear model

#### Same model, different $\Large x$

$$\underbrace{\LARGE E[y_i] = 7 + 3.5 \times x_i}_{Deterministic}$$

$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma=0.25)}_{Stochastic}$$

```{r fig.height=3.5, fig.width=4}
ggplot(df, aes(x, y, group = as.factor(x))) + geom_boxplot(color = "white", width = 0.25) +
  geom_segment(aes(x = -0.1, xend = 0.1, y = 7, yend = 7), color = "#446E9B", size = 1.5) +
  geom_segment(aes(x = 0.9, xend = 1.1, y = 10.5, yend = 10.5), color = "#446E9B", size = 1.5) +
  scale_y_continuous(expression(mu)) +
  scale_x_continuous(breaks = c(0, 1)) +
  geom_point(color = "#D47500", alpha = 0.25)
```

---
# the linear model

</br>

$$\LARGE E[y_i] = \beta_0 + \beta_1  x_i$$

--

#### When $\large x = 0$:

- $\large E[y] = \beta_0$

- What is the interpretation of $\large \beta_0$?

--

#### When $\large x = 1$:

- $\large E[y] = \beta_0 + \beta_1$

- What is the interpretation of $\large \beta_1$?

---
# fitting the model in `R`

```{r echo=TRUE}
data(frogs)

fit <- lm(Frogs ~ Development, data = frogs)

broom::tidy(fit)
```

--
To interpret this output, we need to know how `R` codes categorical predictors...

---
# categorical predictors in `R`

#### When you include a categorical predictor (factor or character object) in your model, `R` recodes it is a **dummy variable**

- For a predictor with two levels, one level gets coded as $\large 0$ and the other as $\large 1$

- For unordered factors or character objects, levels are determined by alphabetical order

- For ordered factors, levels are determined by factor levels

--

```{r echo = TRUE}
str(frogs)
```

---
# model matrix

#### We can always check how `R` coded a linear model using the `model.matrix()` function

- the model matrix (also called the *design matrix*) will always have one row for every observation and one column for every parameter in the model

```{r echo = TRUE}
### View rows 1, 2, 11, and 12 of the model matrix
model.matrix(fit)[c(1,2,11,12),]
```

--

To interpret this output (and see why it's useful), we need to review matrix algebra

---
# matrix multiplication

$$\begin{bmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i 
\end{bmatrix}\;\;
\mathbf \times \begin{bmatrix}
    x \\
    y \\
    z
\end{bmatrix}=\large \begin{bmatrix}
    a\times x + b\times y + c\times z\\
    d\times x + e\times y + f\times z\\
    g\times x + h\times y + i\times z
\end{bmatrix}$$

--

$$\begin{bmatrix}
    1 & x_1 \\
    1 & x_2 \\
    1 & x_3 
\end{bmatrix}\;\;
\mathbf \times \begin{bmatrix}
    \beta_0 \\
    \beta_1 
\end{bmatrix}=\large \begin{bmatrix}
    \beta_0 \times 1 + \beta_1 \times x_1\\
    \beta_0 \times 1 + \beta_1 \times x_2\\
    \beta_0 \times 1 + \beta_1 \times x_3
\end{bmatrix}$$

--

.pull-left[
```{r echo = TRUE}
### View rows 1, 2, 11, and 12 of the model matrix
model.matrix(fit)[c(1,2,11,12),]
```
]

.pull-right[
```{r echo = TRUE}
### View rows 1, 2, 11, and 12 of the data
frogs[c(1,2,11,12),]
```
]

---
# fitting the model in `R`

```{r echo=TRUE}
data(frogs)

fit2 <- lm(Frogs ~ Development, data = frogs)

broom::tidy(fit2)
```

--

- The `(Intercept)` estimate ( $\beta_0$ ) is the expected number of frogs in a low-development plot

--

- The `DevelopmentHigh` estimate ( $\beta_1$ ) is the *difference* between low-development and high-development plots

--

- **Question**: Are there fewer frogs in high-development plots?

---
# fitting the model by hand

In this case, we can also fit the model "by hand"

--

The parameter estimates are:

```{r echo = TRUE}
mu_high <- mean(frogs$Frogs[frogs$Development == "High"])
mu_low <- mean(frogs$Frogs[frogs$Development == "Low"])

(beta_0.hat <- mu_low)
(beta_1.hat <- mu_high - mu_low)
```

--

How do we calculate the standard errors?

---
# fitting the model by hand

How do we calculate the standard errors?

--

Because we have samples from two populations (low and high development), we have to use the *pooled* variance of both samples to calculate the correct standard errors:

$$\large s^2_p = \frac{(n_L − 1)s^2_L + (n_H − 1)s^2_H}{n_L + n_H − 2}$$

--

```{r}
var_low <- var(frogs$Frogs[frogs$Development == "Low"])
var_high <- var(frogs$Frogs[frogs$Development == "High"])

# Pooled variance
(s2.p <- (9*var_low + 9*var_high)/18)
```

---
# fitting the model by hand

How do we calculate the standard errors?

--

With the pooled variance calculated, we can calculate the standard errors. For the intercept:

$$\large SE_{\beta_0} = \sqrt{\frac{s^2_p}{n_H}}$$

```{r}
(SE_beta0 <- sqrt(s2.p/10))
```

--

For the slope:

$$\large SE_{\beta_1} = \sqrt{\frac{s^2_p}{n_L} + \frac{s^2_p}{n_H}}$$

```{r}
(SE_beta1 <- sqrt(s2.p/10 + s2.p/10))
```

--

What is the interpretation of these standard errors?


---
# the linear model as a t-test

#### As before, the model we just fit has a corresponding "test" that you may encounter: the *two-sample t-test*

.pull-left[
- Two-sample t-tests are used to test whether the means of two population are different

- there is a built-in function to fit t-tests

- the group means (and the SE, t-statistic, p-value, confidence interval) are exactly the same in both approaches
]

.pull-right[

```{r echo = TRUE}
t.test(Frogs ~ Development, data = frogs)
```
]

---
# other ways to fit the model

#### By default, the linear model approach provides the mean of the reference group and the difference in the group means

--
- This version of the linear model formulation is usually referred to as the *effects parameterization*

- We can easily calculate the mean of group 2 from the linear model as $\beta_0 + \beta_1$

--

- We can also recode the model in `R` to provide the group means (the *means parameterization*)

```{r echo = TRUE}
fit3 <- lm(Frogs ~ Development - 1, data = frogs)
broom::tidy(fit3)
```


---
# other ways to fit the model

#### Challenge question: What do you think the model matrix looks like for the means parameterization of the model?

--

```{r echo = TRUE}
model.matrix(fit3)[c(1,2,11,12),]
```

--

The key thing to recognize is that these alternative (t-test, effects parameterization, means parameterization) provide the *exact same information*, just presented in different ways

- In other words, they do not change your conclusions!


---
# looking ahead

<br/>

### **Next time**: Null hypothesis testing

<br/>

### **Reading**: [Fieberg chp. 1.10](https://statistics4ecologists-v1.netlify.app/linreg.html#hypothesis-tests-and-p-values)

